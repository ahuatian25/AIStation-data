{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验T3和T4：考虑时间问题，对AllTrain（AllTest为0）进行按时排序，有前20%进行第一次训练，预测第20%+1个样本的犯罪类型，后续再增加一个block（可能有几百或者几千个样本）进行训练，对第20%+block+1个样本进行预测，然后循环进行直至最后一个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras import layers,metrics\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils, multi_gpu_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.utils import shuffle as reset\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss,make_scorer\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "# import \n",
    "from matplotlib.pylab import plt\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler #https://imbalanced-learn.org/stable/generated/imblearn.over_sampling.RandomOverSampler.html?highlight=randomoversampler\n",
    "from frplayer import FilterResponseNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_DataFrame(data, test_size=0.2, considerTime=True, random_state=None):\n",
    "    # ConsiderTime-------trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "    if considerTime:\n",
    "        data=data.sort_values(by=\"Dates\", ascending=True)\n",
    "    else:\n",
    "        data=reset(data, random_state=random_state)\n",
    "    train=data[int(len(data)*test_size):].reset_index(drop=True)\n",
    "    test=data[:int(len(data)*test_size)].reset_index(drop=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    if '-' in x:\n",
    "        DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")#jjs\n",
    "    else:\n",
    "        DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")#zj    \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "def Dates2TDMY(x):\n",
    "    if '-' in x:\n",
    "        DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")#jjs\n",
    "    else:\n",
    "        DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")#zj  \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    #T_D_M_Y=str(time)+str(day)+str(month)+str(year)\n",
    "    T_D_M_Y=str(time)+str(day)+str(month)\n",
    "    return T_D_M_Y\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field2Vec(trainDF,testDF,fieldStr):\n",
    "    fields=sorted(trainDF[fieldStr].unique())\n",
    "    categories=sorted(trainDF[\"Category\"].unique())\n",
    "    C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "    F_C_counts=trainDF.groupby([fieldStr,\"Category\"]).size()\n",
    "    F_counts=trainDF.groupby([fieldStr]).size()\n",
    "    logodds={}\n",
    "    logoddsPF={}\n",
    "    MIN_CAT_COUNTS=2\n",
    "    default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "    for f in fields:\n",
    "        PA=F_counts[f]/float(len(trainDF))\n",
    "        logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "        logodds[f]=deepcopy(default_logodds)\n",
    "        for cat in F_C_counts[f].keys():\n",
    "            if (F_C_counts[f][cat]>MIN_CAT_COUNTS) and F_C_counts[f][cat]<F_counts[f]:\n",
    "                PA=F_C_counts[f][cat]/float(F_counts[f])\n",
    "                logodds[f][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "        logodds[f]=pd.Series(logodds[f])\n",
    "        logodds[f].index=range(len(categories))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    #fieldsTest=sorted(testDF[fieldStr].unique())\n",
    "    #N_count=0\n",
    "    #for f in fieldsTest:\n",
    "        #if f not in fields:\n",
    "            #logoddsPF[f]=-50.0  #np.log(0.)-np.log(1.)=-inf,便于计算，改为-99999.0\n",
    "            #logodds[f]=deepcopy(default_logodds)\n",
    "            #pa=1.0/float(len(categories))\n",
    "            #logodds[f][range(len(categories))]=np.log(pa)-np.log(1.0-pa)\n",
    "            #logodds[f]=pd.Series(logodds[f])\n",
    "            #logodds[f].index=range(len(categories))\n",
    "            #N_count=N_count+1\n",
    "    #print(fieldStr+' N_count: '+str(N_count))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    #引进代码原作者的新思想\n",
    "    if testDF.shape[0]>0: #如果testDF里有样本,......\n",
    "        print('There are some new:'+fieldStr)\n",
    "        new_fields=sorted(testDF[fieldStr].unique())\n",
    "        new_F_counts=testDF.groupby(fieldStr).size()\n",
    "        only_new=set(new_fields+fields)-set(fields)\n",
    "        only_old=set(new_fields+fields)-set(new_fields)\n",
    "        in_both=set(new_fields).intersection(fields)\n",
    "        print('# only_new_fieldds:'+str(len(only_new)))\n",
    "        for f in only_new:\n",
    "            PA=new_F_counts[f]/float(len(testDF)+len(trainDF))\n",
    "            logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "            logodds[f]=deepcopy(default_logodds)\n",
    "            logodds[f].index=range(len(categories))\n",
    "        for f in in_both:\n",
    "            PA=(F_counts[f]+new_F_counts[f])/float(len(testDF)+len(trainDF))\n",
    "            logoddsPF[f]=np.log(PA)-np.log(1.-PA)    \n",
    "    return logodds,logoddsPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(df,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y=False):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print(\"Creating address features\")###Creating address features###\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds_A[x])\n",
    "    address_features.columns=[\"logodds_A\"+str(x) for x in range(len(address_features.columns))]\n",
    "    if needT_D_M_Y:\n",
    "        print(\"Creating time T_D_M_Y features\")###Creating time T_D_M_Y features###\n",
    "        T_D_M_Y_features=cleanData[\"T_D_M_Y\"].apply(lambda xx: logodds_T[xx])\n",
    "        T_D_M_Y_features.columns=[\"logodds_T\"+str(xx) for xx in range(len(T_D_M_Y_features.columns))]\n",
    "\n",
    "    print(\"Parsing dates\")            ###Creating address features###\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "    #     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    #     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print(\"Creating one-hot variables\")\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPF_A\"]=cleanData[\"Address\"].apply(lambda x: logoddsPF_A[x])\n",
    "    if needT_D_M_Y:\n",
    "        cleanData[\"logoddsPF_T\"]=cleanData[\"T_D_M_Y\"].apply(lambda x: logoddsPF_T[x])\n",
    "    print(\"droping processed columns\")\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)    \n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    if needT_D_M_Y:\n",
    "        cleanData=cleanData.drop(\"T_D_M_Y\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    if needT_D_M_Y:\n",
    "        features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:]).join(T_D_M_Y_features.iloc[:,:])\n",
    "    else:\n",
    "        features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(keep='last')).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, Y, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(X) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, X.shape[-1]))\n",
    "        targets = np.zeros((len(rows),Y.shape[1]))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = X[indices]\n",
    "            targets[j] = Y[rows[j]+delay]\n",
    "        yield samples, targets\n",
    "    #Now here is the data generator that we will use. It yields a tuple (samples, targets) where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\n",
    "        # •data: The original array of floating point data, which we just normalized in the code snippet above.\n",
    "        # •lookback: How many timesteps back should our input data go.\n",
    "        # •delay: How many timesteps in the future should our target be.\n",
    "        # •min_index and max_index: Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another one for testing.\n",
    "        # •shuffle: Whether to shuffle our samples or draw them in chronological order.\n",
    "        # •batch_size: The number of samples per batch.\n",
    "        # •step: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of OrginalAllDF: (878049, 9)\n",
      "The shape of AllDF after del wrong X and Y values: (877982, 9)\n",
      "The shape of AllDF after drop_duplicates: (812529, 9)\n",
      "(689038, 2)\n",
      "Address_counts_allDF_trainDF_testDF: 23191_23191_0\n",
      "The # of AllDF, AllTrain, AllTest, is: 812529,812529,0\n",
      "-----------LOGOODS: Address-------------\n",
      "-----------LOGOODS: T_D_M_Y-------------\n",
      "-----------LOGOODS: parse_data of Alltrain  -------------\n",
      "Creating address features\n",
      "Creating time T_D_M_Y features\n",
      "Parsing dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n",
      "['X', 'Y', 'Time', 'Day', 'Month', 'Year', 'IsInterection', 'logoddsPF_A', 'logoddsPF_T', 'PD_BAYVIEW', 'PD_CENTRAL', 'PD_INGLESIDE', 'PD_MISSION', 'PD_NORTHERN', 'PD_PARK', 'PD_RICHMOND', 'PD_SOUTHERN', 'PD_TARAVAL', 'PD_TENDERLOIN', 'DAY_Friday', 'DAY_Monday', 'DAY_Saturday', 'DAY_Sunday', 'DAY_Thursday', 'DAY_Tuesday', 'DAY_Wednesday', 'logodds_A0', 'logodds_A1', 'logodds_A2', 'logodds_A3', 'logodds_A4', 'logodds_A5', 'logodds_A6', 'logodds_A7', 'logodds_A8', 'logodds_A9', 'logodds_A10', 'logodds_A11', 'logodds_A12', 'logodds_A13', 'logodds_A14', 'logodds_A15', 'logodds_A16', 'logodds_A17', 'logodds_A18', 'logodds_A19', 'logodds_A20', 'logodds_A21', 'logodds_A22', 'logodds_A23', 'logodds_A24', 'logodds_A25', 'logodds_A26', 'logodds_A27', 'logodds_A28', 'logodds_A29', 'logodds_A30', 'logodds_A31', 'logodds_A32', 'logodds_A33', 'logodds_A34', 'logodds_A35', 'logodds_A36', 'logodds_A37', 'logodds_A38', 'logodds_T0', 'logodds_T1', 'logodds_T2', 'logodds_T3', 'logodds_T4', 'logodds_T5', 'logodds_T6', 'logodds_T7', 'logodds_T8', 'logodds_T9', 'logodds_T10', 'logodds_T11', 'logodds_T12', 'logodds_T13', 'logodds_T14', 'logodds_T15', 'logodds_T16', 'logodds_T17', 'logodds_T18', 'logodds_T19', 'logodds_T20', 'logodds_T21', 'logodds_T22', 'logodds_T23', 'logodds_T24', 'logodds_T25', 'logodds_T26', 'logodds_T27', 'logodds_T28', 'logodds_T29', 'logodds_T30', 'logodds_T31', 'logodds_T32', 'logodds_T33', 'logodds_T34', 'logodds_T35', 'logodds_T36', 'logodds_T37', 'logodds_T38', 'IsDup', 'Awake', 'Summer', 'Fall', 'Winter', 'Spring']\n",
      "110\n",
      "------------Attention: we do not RandomOverSampler---------------\n",
      "------------ConsiderTime:  Sorting--------------\n"
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "ConsiderTime=True#False# True##trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "Rate_ALL=0.0 #0.0即不保留测试机\n",
    "needOverSampler=False\n",
    "needT_D_M_Y=True #False  使用_T_D_M_Y和周几\n",
    "allDF=pd.read_csv(\"./train_addrCorrect.csv\")\n",
    "print('The shape of OrginalAllDF: '+str(allDF.shape))\n",
    "\n",
    "xy_scaler=preprocessing.StandardScaler()\n",
    "xy_scaler.fit(allDF[[\"X\",\"Y\"]])\n",
    "allDF[[\"X\",\"Y\"]]=xy_scaler.transform(allDF[[\"X\",\"Y\"]])\n",
    "allDF=allDF[abs(allDF[\"Y\"])<100]\n",
    "allDF.index=range(len(allDF))\n",
    "print('The shape of AllDF after del wrong X and Y values: '+str(allDF.shape))\n",
    "\n",
    "def listCat(x):\n",
    "    return list(x)\n",
    "allDF.drop_duplicates(inplace=True,subset=['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'Category'])\n",
    "Train_duplicated=pd.pivot_table(allDF,index=['Dates','DayOfWeek','PdDistrict', 'Address', 'X', 'Y'], values='Category',aggfunc=[len,listCat])\n",
    "print('The shape of AllDF after drop_duplicates: '+str(allDF.shape))\n",
    "print(Train_duplicated.shape)\n",
    "\n",
    "trainDF,testDF=train_test_split_DataFrame(allDF, test_size=Rate_ALL, considerTime=ConsiderTime, random_state=None)\n",
    "print('Address_counts_allDF_trainDF_testDF: ' + str(len(allDF[\"Address\"].unique())) + '_'+ str(len(trainDF[\"Address\"].unique())) + '_' + str(len(testDF[\"Address\"].unique())))\n",
    "\n",
    "N_AllSample=allDF.shape[0]\n",
    "N_AllTrain=trainDF.shape[0]\n",
    "N_AllTest=testDF.shape[0]\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "print('The # of AllDF, AllTrain, AllTest, is: '+str(N_AllSample)+','+str(N_AllTrain)+','+str(N_AllTest))\n",
    "#################Now proceed as before#################\n",
    "print('-----------LOGOODS: Address-------------')\n",
    "logodds_A,logoddsPF_A=field2Vec(trainDF,testDF,\"Address\")\n",
    "if needT_D_M_Y:\n",
    "    trainDF[\"T_D_M_Y\"]=trainDF[\"Dates\"].apply(Dates2TDMY)\n",
    "    trainDF[\"T_D_M_Y\"]=trainDF[\"T_D_M_Y\"]+trainDF[\"DayOfWeek\"]\n",
    "    if Rate_ALL>0:\n",
    "        testDF[[\"X\",\"Y\"]]=xy_scaler.transform(testDF[[\"X\",\"Y\"]])\n",
    "        testDF[\"T_D_M_Y\"]=testDF[\"Dates\"].apply(Dates2TDMY)\n",
    "        testDF[\"T_D_M_Y\"]=testDF[\"T_D_M_Y\"]+testDF[\"DayOfWeek\"]\n",
    "    print('-----------LOGOODS: T_D_M_Y-------------')\n",
    "    logodds_T,logoddsPF_T=field2Vec(trainDF,testDF,\"T_D_M_Y\")    \n",
    "else:\n",
    "    logodds_T=None\n",
    "    logoddsPF_T=None\n",
    "    \n",
    "print('-----------LOGOODS: parse_data of Alltrain  -------------')\n",
    "features, labels=parse_data(trainDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y) \n",
    "if Rate_ALL>0:\n",
    "    print('-----------LOGOODS: parse_data of Alltest  -------------')\n",
    "    features_test, labels_test=parse_data(testDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y)###########和训练集使用同样的时间和地点Logoodds值#####\n",
    "    x_test=features_test.values\n",
    "    y_test=labels_test.values\n",
    "    y_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_test)), num_classes=N_CLASS)\n",
    "\n",
    "print(features.columns.tolist())\n",
    "print(len(features.columns))\n",
    "\n",
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)\n",
    "if Rate_ALL>0:\n",
    "    features_test[collist]=scaler.transform(features_test)###########和训练集使用同样的scaler值#####\n",
    "######################################################\n",
    "#############################先进行过采样，然后再根据时间来排序##################################\n",
    "if needOverSampler:\n",
    "    print('------------RandomOverSampler--------------')\n",
    "    ros = RandomOverSampler()\n",
    "    featuresArrayOverSampler, labelsArrayOverSampler = ros.fit_resample(features.values,labels.values)#####过采样#####\n",
    "    N_AllTrain_OverSampler=int(featuresArrayOverSampler.shape[0])\n",
    "    print('Shape of OverSampler of AllTrain: '+str(featuresArrayOverSampler.shape))\n",
    "else:\n",
    "    featuresArrayOverSampler=features.values\n",
    "    labelsArrayOverSampler=labels.values\n",
    "    N_AllTrain_OverSampler=int(featuresArrayOverSampler.shape[0])\n",
    "    print('------------Attention: we do not RandomOverSampler---------------')\n",
    "if ConsiderTime:\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "    print('------------ConsiderTime:  Sorting--------------')\n",
    "    time_temp=featuresArrayOverSampler[:,2]+np.dot(featuresArrayOverSampler[:,3],100)+np.dot(featuresArrayOverSampler[:,4],10000)+np.dot(featuresArrayOverSampler[:,5],1000000)\n",
    "    features_label_time=np.column_stack((featuresArrayOverSampler,labelsArrayOverSampler))\n",
    "    features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "    features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "    labelsArrayOverSampler=features_label_time[:,-2]\n",
    "    featuresArrayOverSampler=features_label_time[:,0:featuresArrayOverSampler.shape[1]]\n",
    "    del features_label_time\n",
    "    #############################先进行过采样，然后再根据时间来排序----结束############################\n",
    "if Rate_ALL>0:\n",
    "    print('------------RandomOverSampler for AllTest--------------')\n",
    "    ros = RandomOverSampler()\n",
    "    featuresArray_test, labelsArray_test = ros.fit_resample(features_test.values,labels_test.values)#####过采样#####\n",
    "    N_AllTest_OverSampler=int(featuresArray_test.shape[0])\n",
    "    labelsArray_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArray_test)), num_classes=N_CLASS)\n",
    "    print('Shape of OverSampler of AllTest: '+str(featuresArray_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验T3：只对新的Block进行Transfer Learning，不是和旧的TrainSet混在一起重新学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n",
      "-----------------Build DNN model and start the 1st training!!---------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(162506, 110)\n",
      "(1, 110)\n",
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "Train on 162506 samples, validate on 1 samples\n",
      "Epoch 1/7\n",
      "162506/162506 [==============================] - 54s 331us/step - loss: 2.3316 - accuracy: 0.3059 - top_k_categorical_accuracy: 0.7182 - val_loss: 1.5102 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/7\n",
      "162506/162506 [==============================] - 53s 324us/step - loss: 2.1884 - accuracy: 0.3249 - top_k_categorical_accuracy: 0.7512 - val_loss: 1.5410 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/7\n",
      "162506/162506 [==============================] - 53s 324us/step - loss: 2.1700 - accuracy: 0.3279 - top_k_categorical_accuracy: 0.7559 - val_loss: 1.7787 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "162506/162506 [==============================] - 52s 322us/step - loss: 2.1579 - accuracy: 0.3305 - top_k_categorical_accuracy: 0.7581 - val_loss: 1.4706 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/7\n",
      "162506/162506 [==============================] - 53s 327us/step - loss: 2.1486 - accuracy: 0.3337 - top_k_categorical_accuracy: 0.7594 - val_loss: 1.6141 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "162506/162506 [==============================] - 52s 321us/step - loss: 2.1390 - accuracy: 0.3351 - top_k_categorical_accuracy: 0.7625 - val_loss: 2.0004 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "162506/162506 [==============================] - 53s 326us/step - loss: 2.1311 - accuracy: 0.3366 - top_k_categorical_accuracy: 0.7650 - val_loss: 1.8443 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "[1.844341516494751, 1.0, 1.0]\n",
      "-----------------Start the loop training!!---------------------\n",
      "i_s=0 , [[3.44887209 0.         0.        ]]\n",
      "i_s=1 , [[3.7294209 0.        0.       ]]\n",
      "i_s=2 , [[1.09252036 1.         1.        ]]\n",
      "i_s=3 , [[3.47775626 0.         0.        ]]\n",
      "i_s=4 , [[1.53093195 0.         1.        ]]\n",
      "i_s=5 , [[4.34751749 0.         0.        ]]\n",
      "i_s=6 , [[4.36426067 0.         0.        ]]\n",
      "i_s=7 , [[5.0230875 0.        0.       ]]\n",
      "i_s=8 , [[2.08662891 0.         1.        ]]\n",
      "i_s=9 , [[0.35256371 1.         1.        ]]\n",
      "i_s=10 , [[2.81344128 0.         1.        ]]\n",
      "i_s=11 , [[1.00621343 1.         1.        ]]\n",
      "i_s=12 , [[0.64506018 1.         1.        ]]\n",
      "i_s=13 , [[0.80344367 1.         1.        ]]\n",
      "i_s=14 , [[0.09162918 1.         1.        ]]\n",
      "i_s=15 , [[1.80062437 1.         1.        ]]\n",
      "i_s=16 , [[1.03233099 1.         1.        ]]\n",
      "i_s=17 , [[1.65533578 0.         1.        ]]\n",
      "i_s=18 , [[2.34784675 0.         1.        ]]\n",
      "i_s=19 , [[3.47762704 0.         0.        ]]\n",
      "i_s=20 , [[4.31271553 0.         0.        ]]\n",
      "i_s=21 , [[0.46926472 1.         1.        ]]\n",
      "i_s=22 , [[1.49401343 1.         1.        ]]\n",
      "i_s=23 , [[1.92783582 0.         1.        ]]\n",
      "i_s=24 , [[2.47645092 0.         1.        ]]\n",
      "i_s=25 , [[1.50585318 0.         1.        ]]\n",
      "i_s=26 , [[2.01366353 0.         1.        ]]\n",
      "i_s=27 , [[0.24701372 1.         1.        ]]\n",
      "i_s=28 , [[3.7098074 0.        0.       ]]\n",
      "i_s=29 , [[2.5504837 0.        1.       ]]\n",
      "i_s=30 , [[4.33434391 0.         0.        ]]\n",
      "i_s=31 , [[0.37350899 1.         1.        ]]\n",
      "i_s=32 , [[2.13811135 0.         1.        ]]\n",
      "i_s=33 , [[1.22791088 1.         1.        ]]\n",
      "i_s=34 , [[3.80220652 0.         0.        ]]\n",
      "i_s=35 , [[1.89736128 0.         1.        ]]\n",
      "i_s=36 , [[3.5222044 0.        1.       ]]\n",
      "i_s=37 , [[2.60593176 0.         1.        ]]\n",
      "i_s=38 , [[0.90922314 1.         1.        ]]\n",
      "i_s=39 , [[3.73659277 0.         0.        ]]\n",
      "i_s=40 , [[2.43721676 0.         1.        ]]\n",
      "i_s=41 , [[2.25615859 0.         1.        ]]\n",
      "i_s=42 , [[4.7282052 0.        0.       ]]\n",
      "i_s=43 , [[2.42047787 0.         1.        ]]\n",
      "i_s=44 , [[0.69658107 1.         1.        ]]\n",
      "i_s=45 , [[5.2079525 0.        0.       ]]\n",
      "i_s=46 , [[1.75575697 0.         1.        ]]\n",
      "i_s=47 , [[1.92637312 0.         1.        ]]\n",
      "i_s=48 , [[1.61494362 0.         1.        ]]\n",
      "i_s=49 , [[1.78173375 1.         1.        ]]\n",
      "i_s=50 , [[3.01040769 0.         0.        ]]\n",
      "i_s=51 , [[0.73221308 1.         1.        ]]\n",
      "i_s=52 , [[3.21221185 0.         0.        ]]\n",
      "i_s=53 , [[3.55852556 0.         1.        ]]\n",
      "i_s=54 , [[0.61811012 1.         1.        ]]\n",
      "i_s=55 , [[2.19543362 0.         1.        ]]\n",
      "i_s=56 , [[2.10641456 0.         1.        ]]\n",
      "i_s=57 , [[1.89187717 0.         1.        ]]\n",
      "i_s=58 , [[1.39395511 1.         1.        ]]\n",
      "i_s=59 , [[3.40354896 0.         0.        ]]\n",
      "i_s=60 , [[3.31902409 0.         0.        ]]\n",
      "i_s=61 , [[1.6694361 1.        1.       ]]\n",
      "i_s=62 , [[0.59239954 1.         1.        ]]\n",
      "i_s=63 , [[1.05441344 1.         1.        ]]\n",
      "i_s=64 , [[1.11691368 1.         1.        ]]\n",
      "i_s=65 , [[3.39733648 0.         0.        ]]\n",
      "i_s=66 , [[1.39542913 0.         1.        ]]\n",
      "i_s=67 , [[2.08617067 0.         1.        ]]\n",
      "i_s=68 , [[1.62830091 0.         1.        ]]\n",
      "i_s=69 , [[2.88041306 0.         0.        ]]\n",
      "i_s=70 , [[2.42200851 0.         1.        ]]\n",
      "i_s=71 , [[2.59199858 0.         1.        ]]\n",
      "i_s=72 , [[0.7881124 1.        1.       ]]\n",
      "i_s=73 , [[1.24767196 1.         1.        ]]\n",
      "i_s=74 , [[1.67220211 1.         1.        ]]\n",
      "i_s=75 , [[1.62074149 0.         1.        ]]\n",
      "i_s=76 , [[0.59356034 1.         1.        ]]\n",
      "i_s=77 , [[3.0841856 0.        0.       ]]\n",
      "i_s=78 , [[3.19885588 0.         0.        ]]\n",
      "i_s=79 , [[3.4564321 0.        0.       ]]\n",
      "i_s=80 , [[0.38157955 1.         1.        ]]\n",
      "i_s=81 , [[0.70080215 1.         1.        ]]\n",
      "i_s=82 , [[2.09603262 0.         1.        ]]\n",
      "i_s=83 , [[2.38644123 0.         1.        ]]\n",
      "i_s=84 , [[3.31561375 0.         0.        ]]\n",
      "i_s=85 , [[0.23140988 1.         1.        ]]\n",
      "i_s=86 , [[3.9945612 0.        0.       ]]\n",
      "i_s=87 , [[2.14806008 0.         1.        ]]\n",
      "i_s=88 , [[2.49499989 0.         1.        ]]\n",
      "i_s=89 , [[3.09557772 0.         0.        ]]\n",
      "i_s=90 , [[1.9183358 0.        1.       ]]\n",
      "i_s=91 , [[1.29334939 1.         1.        ]]\n",
      "i_s=92 , [[2.04761887 0.         1.        ]]\n",
      "i_s=93 , [[1.43617439 0.         1.        ]]\n",
      "i_s=94 , [[1.27730894 1.         1.        ]]\n",
      "i_s=95 , [[2.95222259 0.         1.        ]]\n",
      "i_s=96 , [[1.66614771 0.         1.        ]]\n",
      "i_s=97 , [[2.67462325 0.         1.        ]]\n",
      "i_s=98 , [[1.04525256 1.         1.        ]]\n",
      "i_s=99 , [[0.72972745 1.         1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.15031152, 0.34      , 0.74      ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=7\n",
    "N_EPOCHS=3\n",
    "N_HN_1=128\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=100\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('-----------------Build DNN model and start the 1st training!!---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "# y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "print(str(x_train.shape))\n",
    "\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print(str(x_val_i.shape))\n",
    "\n",
    "# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score0))\n",
    "print('-----------------Start the loop training!!---------------------')\n",
    "Scores_all=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):\n",
    "    x_train=featuresArrayOverSampler[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    x_val_i=featuresArrayOverSampler[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=0,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    loss_i, acc_i, top5acc_i=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "    Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])\n",
    "    \n",
    "np.mean(Scores_all, axis=0)    \n",
    "# y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "# \n",
    "# model.save('jjs_model_0124V3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验T4：对新的Block和旧的TrainSet混在一起重进行Transfer　Learning学习，这样速度非常慢，分成100个Block大概需要10个小时以上【由于操作失误，重新运行了此程序段，但是又考虑到耗时太长，就终止了，实验结果可以参考word记录】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n",
      "-----------------Build DNN model and start the 1st training!!---------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(162506, 110)\n",
      "(1, 110)\n",
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "Train on 162506 samples, validate on 1 samples\n",
      "Epoch 1/7\n",
      " 29184/162506 [====>.........................] - ETA: 46s - loss: 2.7513 - accuracy: 0.2656 - top_k_categorical_accuracy: 0.6205"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7ebc97721b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------DNN Training Go! Go! Go!!!!-----------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_BATCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;31m# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mscore0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_val_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=7\n",
    "N_EPOCHS=3\n",
    "N_HN_1=128\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=100\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('-----------------Build DNN model and start the 1st training!!---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "# y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "print(str(x_train.shape))\n",
    "\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print(str(x_val_i.shape))\n",
    "\n",
    "# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score0))\n",
    "print('-----------------Start the loop training!!---------------------')\n",
    "Scores_all=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):\n",
    "    #model不需要重建\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    x_val_i=featuresArrayOverSampler[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=0,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    loss_i, acc_i, top5acc_i=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "    Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])\n",
    "    \n",
    "np.mean(Scores_all, axis=0)    \n",
    "# y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "# \n",
    "# model.save('jjs_model_0124V3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5：考虑到时间太长的问题，我们分成500个block，并且只对新的Block进行Transfer Learning，而不是和前面的所有样本混在一起训练，T6,1000个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n",
      "-----------------Build DNN model and start the 1st training!!---------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(162506, 110)\n",
      "(1, 110)\n",
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "Train on 162506 samples, validate on 1 samples\n",
      "Epoch 1/7\n",
      "162506/162506 [==============================] - 53s 329us/step - loss: 2.3323 - accuracy: 0.3053 - top_k_categorical_accuracy: 0.7178 - val_loss: 1.4861 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/7\n",
      "162506/162506 [==============================] - 53s 326us/step - loss: 2.1888 - accuracy: 0.3244 - top_k_categorical_accuracy: 0.7510 - val_loss: 1.9140 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/7\n",
      "162506/162506 [==============================] - 53s 326us/step - loss: 2.1705 - accuracy: 0.3291 - top_k_categorical_accuracy: 0.7560 - val_loss: 1.8580 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "162506/162506 [==============================] - 54s 332us/step - loss: 2.1583 - accuracy: 0.3308 - top_k_categorical_accuracy: 0.7588 - val_loss: 1.7888 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/7\n",
      "162506/162506 [==============================] - 52s 322us/step - loss: 2.1481 - accuracy: 0.3333 - top_k_categorical_accuracy: 0.7615 - val_loss: 1.8041 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "162506/162506 [==============================] - 52s 319us/step - loss: 2.1403 - accuracy: 0.3348 - top_k_categorical_accuracy: 0.7631 - val_loss: 1.8603 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "162506/162506 [==============================] - 52s 318us/step - loss: 2.1322 - accuracy: 0.3364 - top_k_categorical_accuracy: 0.7648 - val_loss: 1.5676 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[1.567645788192749, 1.0, 1.0]\n",
      "-----------------Start the loop training!!---------------------\n",
      "i_s=0 , [[1.8059833 0.        1.       ]]\n",
      "i_s=1 , [[4.2384119 0.        0.       ]]\n",
      "i_s=2 , [[2.13630438 0.         1.        ]]\n",
      "i_s=3 , [[1.88719523 0.         1.        ]]\n",
      "i_s=4 , [[0.77006292 1.         1.        ]]\n",
      "i_s=5 , [[1.76273441 0.         1.        ]]\n",
      "i_s=6 , [[0.05018072 1.         1.        ]]\n",
      "i_s=7 , [[5.76540375 0.         0.        ]]\n",
      "i_s=8 , [[5.31202221 0.         0.        ]]\n",
      "i_s=9 , [[2.84532261 0.         1.        ]]\n",
      "i_s=10 , [[1.80513334 0.         1.        ]]\n",
      "i_s=11 , [[1.87359035 0.         1.        ]]\n",
      "i_s=12 , [[3.59047961 0.         0.        ]]\n",
      "i_s=13 , [[2.01592469 0.         1.        ]]\n",
      "i_s=14 , [[2.05188799 0.         1.        ]]\n",
      "i_s=15 , [[1.46593618 1.         1.        ]]\n",
      "i_s=16 , [[2.7720871 0.        0.       ]]\n",
      "i_s=17 , [[2.25397158 0.         1.        ]]\n",
      "i_s=18 , [[1.94093466 0.         1.        ]]\n",
      "i_s=19 , [[3.23683453 0.         0.        ]]\n",
      "i_s=20 , [[2.42197943 0.         1.        ]]\n",
      "i_s=21 , [[1.43364835 1.         1.        ]]\n",
      "i_s=22 , [[2.12297916 0.         1.        ]]\n",
      "i_s=23 , [[2.71742916 0.         1.        ]]\n",
      "i_s=24 , [[1.02896452 1.         1.        ]]\n",
      "i_s=25 , [[0.84261185 1.         1.        ]]\n",
      "i_s=26 , [[2.32112026 0.         1.        ]]\n",
      "i_s=27 , [[1.60297906 0.         1.        ]]\n",
      "i_s=28 , [[1.3743813 1.        1.       ]]\n",
      "i_s=29 , [[1.00296795 1.         1.        ]]\n",
      "i_s=30 , [[1.30928218 1.         1.        ]]\n",
      "i_s=31 , [[1.85374928 0.         1.        ]]\n",
      "i_s=32 , [[3.1767478 0.        0.       ]]\n",
      "i_s=33 , [[1.46202397 0.         1.        ]]\n",
      "i_s=34 , [[0.91944933 1.         1.        ]]\n",
      "i_s=35 , [[4.9703455 0.        0.       ]]\n",
      "i_s=36 , [[6.02827311 0.         0.        ]]\n",
      "i_s=37 , [[1.23409867 0.         1.        ]]\n",
      "i_s=38 , [[0.49851924 1.         1.        ]]\n",
      "i_s=39 , [[3.31272244 0.         0.        ]]\n",
      "i_s=40 , [[0.11339063 1.         1.        ]]\n",
      "i_s=41 , [[2.84158564 0.         1.        ]]\n",
      "i_s=42 , [[7.68979359 0.         0.        ]]\n",
      "i_s=43 , [[3.00899887 0.         0.        ]]\n",
      "i_s=44 , [[4.94305992 0.         0.        ]]\n",
      "i_s=45 , [[3.24218464 0.         0.        ]]\n",
      "i_s=46 , [[1.57128608 0.         1.        ]]\n",
      "i_s=47 , [[3.93519592 0.         1.        ]]\n",
      "i_s=48 , [[4.11857176 0.         0.        ]]\n",
      "i_s=49 , [[1.58011317 1.         1.        ]]\n",
      "i_s=50 , [[1.17482829 0.         1.        ]]\n",
      "i_s=51 , [[4.07235098 0.         0.        ]]\n",
      "i_s=52 , [[0.33999419 1.         1.        ]]\n",
      "i_s=53 , [[1.14004719 1.         1.        ]]\n",
      "i_s=54 , [[1.57055914 0.         1.        ]]\n",
      "i_s=55 , [[1.32749104 1.         1.        ]]\n",
      "i_s=56 , [[1.62827361 0.         1.        ]]\n",
      "i_s=57 , [[1.86480021 0.         1.        ]]\n",
      "i_s=58 , [[1.61414742 0.         1.        ]]\n",
      "i_s=59 , [[4.45178556 0.         0.        ]]\n",
      "i_s=60 , [[1.73489213 0.         1.        ]]\n",
      "i_s=61 , [[0.88518053 1.         1.        ]]\n",
      "i_s=62 , [[0.87881237 1.         1.        ]]\n",
      "i_s=63 , [[0.38859376 1.         1.        ]]\n",
      "i_s=64 , [[4.75713682 0.         0.        ]]\n",
      "i_s=65 , [[1.62316394 0.         1.        ]]\n",
      "i_s=66 , [[1.74948645 0.         1.        ]]\n",
      "i_s=67 , [[5.62043095 0.         0.        ]]\n",
      "i_s=68 , [[4.6731658 0.        0.       ]]\n",
      "i_s=69 , [[4.04084778 0.         0.        ]]\n",
      "i_s=70 , [[1.45879626 0.         1.        ]]\n",
      "i_s=71 , [[2.29660797 0.         1.        ]]\n",
      "i_s=72 , [[3.47660756 0.         0.        ]]\n",
      "i_s=73 , [[1.24602175 1.         1.        ]]\n",
      "i_s=74 , [[0.89787263 1.         1.        ]]\n",
      "i_s=75 , [[2.71038771 0.         1.        ]]\n",
      "i_s=76 , [[1.33336699 0.         1.        ]]\n",
      "i_s=77 , [[4.7853756 0.        0.       ]]\n",
      "i_s=78 , [[3.15783596 0.         0.        ]]\n",
      "i_s=79 , [[5.27932072 0.         0.        ]]\n",
      "i_s=80 , [[5.51955986 0.         0.        ]]\n",
      "i_s=81 , [[6.09179115 0.         0.        ]]\n",
      "i_s=82 , [[3.24714303 0.         0.        ]]\n",
      "i_s=83 , [[7.49436903 0.         0.        ]]\n",
      "i_s=84 , [[1.90941417 0.         1.        ]]\n",
      "i_s=85 , [[7.5820179 0.        0.       ]]\n",
      "i_s=86 , [[1.70000625 1.         1.        ]]\n",
      "i_s=87 , [[0.79815501 1.         1.        ]]\n",
      "i_s=88 , [[2.32072592 0.         1.        ]]\n",
      "i_s=89 , [[1.82431817 0.         1.        ]]\n",
      "i_s=90 , [[0.66767859 1.         1.        ]]\n",
      "i_s=91 , [[3.29631114 0.         0.        ]]\n",
      "i_s=92 , [[3.46068263 0.         0.        ]]\n",
      "i_s=93 , [[3.00491834 0.         0.        ]]\n",
      "i_s=94 , [[0.57225102 1.         1.        ]]\n",
      "i_s=95 , [[2.31417656 0.         1.        ]]\n",
      "i_s=96 , [[0.60350776 1.         1.        ]]\n",
      "i_s=97 , [[1.97370577 0.         1.        ]]\n",
      "i_s=98 , [[0.5301249 1.        1.       ]]\n",
      "i_s=99 , [[0.38862431 1.         1.        ]]\n",
      "i_s=100 , [[1.64447021 0.         1.        ]]\n",
      "i_s=101 , [[1.82506478 0.         1.        ]]\n",
      "i_s=102 , [[0.47503322 1.         1.        ]]\n",
      "i_s=103 , [[2.36427736 0.         1.        ]]\n",
      "i_s=104 , [[2.05008221 0.         1.        ]]\n",
      "i_s=105 , [[1.91217971 0.         1.        ]]\n",
      "i_s=106 , [[3.16759825 0.         0.        ]]\n",
      "i_s=107 , [[4.12296057 0.         0.        ]]\n",
      "i_s=108 , [[2.60342526 0.         1.        ]]\n",
      "i_s=109 , [[2.28932905 0.         1.        ]]\n",
      "i_s=110 , [[3.12750196 0.         0.        ]]\n",
      "i_s=111 , [[2.39580774 0.         0.        ]]\n",
      "i_s=112 , [[0.27059913 1.         1.        ]]\n",
      "i_s=113 , [[2.74118996 0.         1.        ]]\n",
      "i_s=114 , [[1.47388947 0.         1.        ]]\n",
      "i_s=115 , [[3.33689404 0.         0.        ]]\n",
      "i_s=116 , [[2.69435406 0.         1.        ]]\n",
      "i_s=117 , [[0.20305344 1.         1.        ]]\n",
      "i_s=118 , [[0.6387825 1.        1.       ]]\n",
      "i_s=119 , [[1.28672564 1.         1.        ]]\n",
      "i_s=120 , [[1.25084722 0.         1.        ]]\n",
      "i_s=121 , [[2.58508706 0.         1.        ]]\n",
      "i_s=122 , [[1.62043762 0.         1.        ]]\n",
      "i_s=123 , [[2.34470797 0.         1.        ]]\n",
      "i_s=124 , [[2.11416769 0.         1.        ]]\n",
      "i_s=125 , [[0.28315836 1.         1.        ]]\n",
      "i_s=126 , [[2.00156498 0.         1.        ]]\n",
      "i_s=127 , [[0.95048308 1.         1.        ]]\n",
      "i_s=128 , [[1.82770514 0.         1.        ]]\n",
      "i_s=129 , [[0.49492991 1.         1.        ]]\n",
      "i_s=130 , [[0.80855721 1.         1.        ]]\n",
      "i_s=131 , [[2.09316778 0.         1.        ]]\n",
      "i_s=132 , [[1.65463912 0.         1.        ]]\n",
      "i_s=133 , [[2.45494366 0.         1.        ]]\n",
      "i_s=134 , [[3.89790344 0.         0.        ]]\n",
      "i_s=135 , [[1.2064631 1.        1.       ]]\n",
      "i_s=136 , [[1.87924898 0.         1.        ]]\n",
      "i_s=137 , [[2.35712957 0.         1.        ]]\n",
      "i_s=138 , [[0.32224423 1.         1.        ]]\n",
      "i_s=139 , [[0.9646076 1.        1.       ]]\n",
      "i_s=140 , [[8.60892487 0.         0.        ]]\n",
      "i_s=141 , [[2.4412446 0.        1.       ]]\n",
      "i_s=142 , [[1.66654801 0.         1.        ]]\n",
      "i_s=143 , [[0.81325817 1.         1.        ]]\n",
      "i_s=144 , [[0.23541178 1.         1.        ]]\n",
      "i_s=145 , [[0.6669476 1.        1.       ]]\n",
      "i_s=146 , [[1.73571134 0.         1.        ]]\n",
      "i_s=147 , [[2.91661501 0.         0.        ]]\n",
      "i_s=148 , [[2.40052795 0.         1.        ]]\n",
      "i_s=149 , [[0.07052463 1.         1.        ]]\n",
      "i_s=150 , [[2.55439973 0.         1.        ]]\n",
      "i_s=151 , [[1.37005913 1.         1.        ]]\n",
      "i_s=152 , [[1.97056901 0.         1.        ]]\n",
      "i_s=153 , [[1.43657053 0.         1.        ]]\n",
      "i_s=154 , [[2.71306658 0.         1.        ]]\n",
      "i_s=155 , [[2.99125457 0.         0.        ]]\n",
      "i_s=156 , [[1.74453449 0.         1.        ]]\n",
      "i_s=157 , [[1.93787837 0.         1.        ]]\n",
      "i_s=158 , [[2.94882083 0.         1.        ]]\n",
      "i_s=159 , [[1.79313195 1.         1.        ]]\n",
      "i_s=160 , [[0.83581221 1.         1.        ]]\n",
      "i_s=161 , [[1.80988669 1.         1.        ]]\n",
      "i_s=162 , [[4.02127361 0.         0.        ]]\n",
      "i_s=163 , [[1.91115749 0.         1.        ]]\n",
      "i_s=164 , [[1.91796851 0.         1.        ]]\n",
      "i_s=165 , [[1.96603715 0.         1.        ]]\n",
      "i_s=166 , [[3.57553673 0.         0.        ]]\n",
      "i_s=167 , [[1.03530002 0.         1.        ]]\n",
      "i_s=168 , [[1.19595063 1.         1.        ]]\n",
      "i_s=169 , [[1.18694091 1.         1.        ]]\n",
      "i_s=170 , [[3.33285379 0.         0.        ]]\n",
      "i_s=171 , [[2.34704423 0.         1.        ]]\n",
      "i_s=172 , [[1.67999327 1.         1.        ]]\n",
      "i_s=173 , [[1.62518811 0.         1.        ]]\n",
      "i_s=174 , [[2.34452224 0.         1.        ]]\n",
      "i_s=175 , [[2.93446469 0.         1.        ]]\n",
      "i_s=176 , [[2.18379402 0.         1.        ]]\n",
      "i_s=177 , [[1.35417461 1.         1.        ]]\n",
      "i_s=178 , [[2.13090372 0.         1.        ]]\n",
      "i_s=179 , [[1.45823705 1.         1.        ]]\n",
      "i_s=180 , [[0.94845635 1.         1.        ]]\n",
      "i_s=181 , [[2.9658308 0.        1.       ]]\n",
      "i_s=182 , [[1.23745501 1.         1.        ]]\n",
      "i_s=183 , [[1.5019691 0.        1.       ]]\n",
      "i_s=184 , [[3.10232258 0.         0.        ]]\n",
      "i_s=185 , [[2.33705997 0.         1.        ]]\n",
      "i_s=186 , [[1.88523602 0.         1.        ]]\n",
      "i_s=187 , [[1.90329504 0.         1.        ]]\n",
      "i_s=188 , [[5.76220131 0.         0.        ]]\n",
      "i_s=189 , [[2.32236862 0.         1.        ]]\n",
      "i_s=190 , [[0.71231043 1.         1.        ]]\n",
      "i_s=191 , [[2.42314291 0.         1.        ]]\n",
      "i_s=192 , [[1.92931652 0.         1.        ]]\n",
      "i_s=193 , [[0.71203578 1.         1.        ]]\n",
      "i_s=194 , [[3.12159562 0.         0.        ]]\n",
      "i_s=195 , [[0.47056651 1.         1.        ]]\n",
      "i_s=196 , [[3.35263824 0.         0.        ]]\n",
      "i_s=197 , [[3.42709064 0.         0.        ]]\n",
      "i_s=198 , [[0.77352053 1.         1.        ]]\n",
      "i_s=199 , [[3.36887836 0.         0.        ]]\n",
      "i_s=200 , [[2.94384623 0.         0.        ]]\n",
      "i_s=201 , [[2.8899889 0.        1.       ]]\n",
      "i_s=202 , [[2.24276018 0.         1.        ]]\n",
      "i_s=203 , [[1.40120316 1.         1.        ]]\n",
      "i_s=204 , [[1.49672842 1.         1.        ]]\n",
      "i_s=205 , [[3.18846321 0.         0.        ]]\n",
      "i_s=206 , [[1.04423392 1.         1.        ]]\n",
      "i_s=207 , [[2.6024251 0.        1.       ]]\n",
      "i_s=208 , [[2.26955223 0.         1.        ]]\n",
      "i_s=209 , [[5.15035725 0.         0.        ]]\n",
      "i_s=210 , [[3.69228363 0.         1.        ]]\n",
      "i_s=211 , [[2.1743083 0.        1.       ]]\n",
      "i_s=212 , [[1.3380928 1.        1.       ]]\n",
      "i_s=213 , [[3.19377947 0.         0.        ]]\n",
      "i_s=214 , [[1.92666054 1.         1.        ]]\n",
      "i_s=215 , [[3.28464413 0.         0.        ]]\n",
      "i_s=216 , [[1.02463472 1.         1.        ]]\n",
      "i_s=217 , [[0.98335284 1.         1.        ]]\n",
      "i_s=218 , [[0.50326049 1.         1.        ]]\n",
      "i_s=219 , [[0.34138298 1.         1.        ]]\n",
      "i_s=220 , [[2.77374077 0.         0.        ]]\n",
      "i_s=221 , [[1.96957207 0.         1.        ]]\n",
      "i_s=222 , [[1.79095042 0.         1.        ]]\n",
      "i_s=223 , [[1.64250326 0.         1.        ]]\n",
      "i_s=224 , [[3.99100375 0.         0.        ]]\n",
      "i_s=225 , [[0.29386067 1.         1.        ]]\n",
      "i_s=226 , [[0.64237249 1.         1.        ]]\n",
      "i_s=227 , [[2.69645381 0.         1.        ]]\n",
      "i_s=228 , [[2.47352552 0.         1.        ]]\n",
      "i_s=229 , [[2.02300835 0.         1.        ]]\n",
      "i_s=230 , [[2.86497045 0.         1.        ]]\n",
      "i_s=231 , [[2.10990524 0.         1.        ]]\n",
      "i_s=232 , [[1.49576306 0.         1.        ]]\n",
      "i_s=233 , [[2.4677949 0.        1.       ]]\n",
      "i_s=234 , [[2.64677739 0.         1.        ]]\n",
      "i_s=235 , [[2.39166379 0.         1.        ]]\n",
      "i_s=236 , [[1.51918054 0.         1.        ]]\n",
      "i_s=237 , [[1.51999545 0.         1.        ]]\n",
      "i_s=238 , [[1.35891676 1.         1.        ]]\n",
      "i_s=239 , [[1.74553132 0.         1.        ]]\n",
      "i_s=240 , [[2.18751049 0.         1.        ]]\n",
      "i_s=241 , [[4.90302324 0.         0.        ]]\n",
      "i_s=242 , [[0.85348278 1.         1.        ]]\n",
      "i_s=243 , [[3.17685366 0.         1.        ]]\n",
      "i_s=244 , [[0.65530413 1.         1.        ]]\n",
      "i_s=245 , [[1.08742881 1.         1.        ]]\n",
      "i_s=246 , [[1.83454418 0.         1.        ]]\n",
      "i_s=247 , [[1.98052013 0.         1.        ]]\n",
      "i_s=248 , [[1.87617481 0.         1.        ]]\n",
      "i_s=249 , [[2.37941599 0.         1.        ]]\n",
      "i_s=250 , [[2.56389189 0.         1.        ]]\n",
      "i_s=251 , [[3.70877624 0.         0.        ]]\n",
      "i_s=252 , [[1.52895701 0.         1.        ]]\n",
      "i_s=253 , [[1.18670988 1.         1.        ]]\n",
      "i_s=254 , [[2.06546879 0.         1.        ]]\n",
      "i_s=255 , [[2.88404441 0.         0.        ]]\n",
      "i_s=256 , [[0.43022233 1.         1.        ]]\n",
      "i_s=257 , [[1.80239713 0.         1.        ]]\n",
      "i_s=258 , [[2.31224871 0.         1.        ]]\n",
      "i_s=259 , [[1.18552744 1.         1.        ]]\n",
      "i_s=260 , [[1.97447252 0.         1.        ]]\n",
      "i_s=261 , [[2.478374 0.       1.      ]]\n",
      "i_s=262 , [[1.33694351 1.         1.        ]]\n",
      "i_s=263 , [[1.35660732 1.         1.        ]]\n",
      "i_s=264 , [[2.2116046 0.        1.       ]]\n",
      "i_s=265 , [[0.53896666 1.         1.        ]]\n",
      "i_s=266 , [[0.87395608 1.         1.        ]]\n",
      "i_s=267 , [[0.26485801 1.         1.        ]]\n",
      "i_s=268 , [[1.5133239 1.        1.       ]]\n",
      "i_s=269 , [[2.1427784 0.        1.       ]]\n",
      "i_s=270 , [[3.4086864 0.        0.       ]]\n",
      "i_s=271 , [[2.79536295 0.         1.        ]]\n",
      "i_s=272 , [[3.99087858 0.         0.        ]]\n",
      "i_s=273 , [[1.62021136 0.         1.        ]]\n",
      "i_s=274 , [[1.27507436 0.         1.        ]]\n",
      "i_s=275 , [[3.10838652 0.         0.        ]]\n",
      "i_s=276 , [[0.50597668 1.         1.        ]]\n",
      "i_s=277 , [[1.4780364 0.        1.       ]]\n",
      "i_s=278 , [[1.79206228 0.         1.        ]]\n",
      "i_s=279 , [[0.25176892 1.         1.        ]]\n",
      "i_s=280 , [[3.53037357 0.         0.        ]]\n",
      "i_s=281 , [[1.11206961 1.         1.        ]]\n",
      "i_s=282 , [[0.53729284 1.         1.        ]]\n",
      "i_s=283 , [[0.89703804 1.         1.        ]]\n",
      "i_s=284 , [[0.24483088 1.         1.        ]]\n",
      "i_s=285 , [[2.45292425 0.         1.        ]]\n",
      "i_s=286 , [[0.66441411 1.         1.        ]]\n",
      "i_s=287 , [[2.7058301 0.        1.       ]]\n",
      "i_s=288 , [[1.32751691 1.         1.        ]]\n",
      "i_s=289 , [[3.69989872 0.         0.        ]]\n",
      "i_s=290 , [[1.02830911 1.         1.        ]]\n",
      "i_s=291 , [[1.55287445 1.         1.        ]]\n",
      "i_s=292 , [[1.55706835 0.         1.        ]]\n",
      "i_s=293 , [[1.48024309 0.         1.        ]]\n",
      "i_s=294 , [[3.94450426 0.         0.        ]]\n",
      "i_s=295 , [[0.33808458 1.         1.        ]]\n",
      "i_s=296 , [[2.65309381 0.         1.        ]]\n",
      "i_s=297 , [[1.17438936 1.         1.        ]]\n",
      "i_s=298 , [[1.12560153 1.         1.        ]]\n",
      "i_s=299 , [[2.61197948 0.         1.        ]]\n",
      "i_s=300 , [[1.80886567 0.         1.        ]]\n",
      "i_s=301 , [[1.75956798 0.         1.        ]]\n",
      "i_s=302 , [[1.80582416 0.         1.        ]]\n",
      "i_s=303 , [[5.04069424 0.         0.        ]]\n",
      "i_s=304 , [[4.42020178 0.         0.        ]]\n",
      "i_s=305 , [[3.17987275 0.         0.        ]]\n",
      "i_s=306 , [[0.93680191 1.         1.        ]]\n",
      "i_s=307 , [[1.75531936 0.         1.        ]]\n",
      "i_s=308 , [[3.04218102 0.         0.        ]]\n",
      "i_s=309 , [[3.62000608 0.         0.        ]]\n",
      "i_s=310 , [[5.94025612 0.         0.        ]]\n",
      "i_s=311 , [[3.17431951 0.         0.        ]]\n",
      "i_s=312 , [[2.30004191 0.         1.        ]]\n",
      "i_s=313 , [[2.51857758 0.         1.        ]]\n",
      "i_s=314 , [[1.66531169 0.         1.        ]]\n",
      "i_s=315 , [[1.26024115 1.         1.        ]]\n",
      "i_s=316 , [[1.77615964 0.         1.        ]]\n",
      "i_s=317 , [[1.24819672 1.         1.        ]]\n",
      "i_s=318 , [[2.58838129 0.         1.        ]]\n",
      "i_s=319 , [[0.53318757 1.         1.        ]]\n",
      "i_s=320 , [[2.54101205 0.         1.        ]]\n",
      "i_s=321 , [[2.74183083 0.         1.        ]]\n",
      "i_s=322 , [[1.63205528 1.         1.        ]]\n",
      "i_s=323 , [[1.50718355 1.         1.        ]]\n",
      "i_s=324 , [[2.814188 0.       0.      ]]\n",
      "i_s=325 , [[3.89767933 0.         0.        ]]\n",
      "i_s=326 , [[1.30862319 1.         1.        ]]\n",
      "i_s=327 , [[0.92578411 1.         1.        ]]\n",
      "i_s=328 , [[1.79790807 0.         1.        ]]\n",
      "i_s=329 , [[2.27902365 0.         1.        ]]\n",
      "i_s=330 , [[0.97710401 1.         1.        ]]\n",
      "i_s=331 , [[3.67347336 0.         0.        ]]\n",
      "i_s=332 , [[3.23552895 0.         1.        ]]\n",
      "i_s=333 , [[2.0532515 0.        1.       ]]\n",
      "i_s=334 , [[1.82433462 0.         1.        ]]\n",
      "i_s=335 , [[1.71368551 0.         1.        ]]\n",
      "i_s=336 , [[3.78780437 0.         0.        ]]\n",
      "i_s=337 , [[1.25625622 1.         1.        ]]\n",
      "i_s=338 , [[0.9599905 1.        1.       ]]\n",
      "i_s=339 , [[1.23667252 1.         1.        ]]\n",
      "i_s=340 , [[1.00358677 1.         1.        ]]\n",
      "i_s=341 , [[2.45850134 0.         1.        ]]\n",
      "i_s=342 , [[5.70331383 0.         0.        ]]\n",
      "i_s=343 , [[1.01561201 1.         1.        ]]\n",
      "i_s=344 , [[1.77354741 0.         1.        ]]\n",
      "i_s=345 , [[1.21449316 1.         1.        ]]\n",
      "i_s=346 , [[2.7659297 0.        1.       ]]\n",
      "i_s=347 , [[2.15704012 0.         1.        ]]\n",
      "i_s=348 , [[1.77663481 0.         1.        ]]\n",
      "i_s=349 , [[3.70567751 0.         0.        ]]\n",
      "i_s=350 , [[3.66964722 0.         0.        ]]\n",
      "i_s=351 , [[2.54731083 0.         0.        ]]\n",
      "i_s=352 , [[0.61884201 1.         1.        ]]\n",
      "i_s=353 , [[2.64625454 0.         0.        ]]\n",
      "i_s=354 , [[1.0249567 1.        1.       ]]\n",
      "i_s=355 , [[4.71035576 0.         0.        ]]\n",
      "i_s=356 , [[0.55126595 1.         1.        ]]\n",
      "i_s=357 , [[2.66670847 0.         0.        ]]\n",
      "i_s=358 , [[1.57189012 0.         1.        ]]\n",
      "i_s=359 , [[1.84502649 0.         1.        ]]\n",
      "i_s=360 , [[2.05887413 0.         1.        ]]\n",
      "i_s=361 , [[2.01129866 0.         1.        ]]\n",
      "i_s=362 , [[1.81037283 0.         1.        ]]\n",
      "i_s=363 , [[4.54231834 0.         0.        ]]\n",
      "i_s=364 , [[0.77923816 1.         1.        ]]\n",
      "i_s=365 , [[4.20253754 0.         0.        ]]\n",
      "i_s=366 , [[1.94619465 0.         1.        ]]\n",
      "i_s=367 , [[1.1087029 1.        1.       ]]\n",
      "i_s=368 , [[3.17252755 0.         0.        ]]\n",
      "i_s=369 , [[3.52703857 0.         1.        ]]\n",
      "i_s=370 , [[0.28577375 1.         1.        ]]\n",
      "i_s=371 , [[2.92718029 0.         0.        ]]\n",
      "i_s=372 , [[4.57109213 0.         0.        ]]\n",
      "i_s=373 , [[1.93306768 0.         1.        ]]\n",
      "i_s=374 , [[0.95758748 1.         1.        ]]\n",
      "i_s=375 , [[4.43864822 0.         0.        ]]\n",
      "i_s=376 , [[5.74217367 0.         0.        ]]\n",
      "i_s=377 , [[1.13590395 1.         1.        ]]\n",
      "i_s=378 , [[3.83814502 0.         0.        ]]\n",
      "i_s=379 , [[2.46470213 0.         1.        ]]\n",
      "i_s=380 , [[3.71827316 0.         0.        ]]\n",
      "i_s=381 , [[2.81087685 0.         0.        ]]\n",
      "i_s=382 , [[2.59752321 0.         0.        ]]\n",
      "i_s=383 , [[0.67816848 1.         1.        ]]\n",
      "i_s=384 , [[1.44163132 1.         1.        ]]\n",
      "i_s=385 , [[3.72445536 0.         0.        ]]\n",
      "i_s=386 , [[0.55455589 1.         1.        ]]\n",
      "i_s=387 , [[0.31908226 1.         1.        ]]\n",
      "i_s=388 , [[1.29166746 1.         1.        ]]\n",
      "i_s=389 , [[1.08699453 1.         1.        ]]\n",
      "i_s=390 , [[2.55697298 0.         1.        ]]\n",
      "i_s=391 , [[1.63613355 0.         1.        ]]\n",
      "i_s=392 , [[1.26422977 1.         1.        ]]\n",
      "i_s=393 , [[0.22256756 1.         1.        ]]\n",
      "i_s=394 , [[1.28837216 1.         1.        ]]\n",
      "i_s=395 , [[1.17290533 0.         1.        ]]\n",
      "i_s=396 , [[0.03219503 1.         1.        ]]\n",
      "i_s=397 , [[1.81939077 0.         1.        ]]\n",
      "i_s=398 , [[0.78403634 1.         1.        ]]\n",
      "i_s=399 , [[3.04624009 0.         0.        ]]\n",
      "i_s=400 , [[4.59443045 0.         0.        ]]\n",
      "i_s=401 , [[3.13618183 0.         0.        ]]\n",
      "i_s=402 , [[1.50453568 1.         1.        ]]\n",
      "i_s=403 , [[3.23402119 0.         0.        ]]\n",
      "i_s=404 , [[0.93923998 1.         1.        ]]\n",
      "i_s=405 , [[2.71623421 0.         0.        ]]\n",
      "i_s=406 , [[1.99771547 0.         1.        ]]\n",
      "i_s=407 , [[1.62950015 1.         1.        ]]\n",
      "i_s=408 , [[1.7824533 0.        1.       ]]\n",
      "i_s=409 , [[2.5979743 0.        1.       ]]\n",
      "i_s=410 , [[2.2417531 0.        1.       ]]\n",
      "i_s=411 , [[3.81300473 0.         0.        ]]\n",
      "i_s=412 , [[0.4156048 1.        1.       ]]\n",
      "i_s=413 , [[7.12755966 0.         0.        ]]\n",
      "i_s=414 , [[1.91005099 0.         1.        ]]\n",
      "i_s=415 , [[2.33973694 0.         1.        ]]\n",
      "i_s=416 , [[2.37103534 0.         1.        ]]\n",
      "i_s=417 , [[1.07467556 1.         1.        ]]\n",
      "i_s=418 , [[3.81498098 0.         0.        ]]\n",
      "i_s=419 , [[1.96151924 0.         1.        ]]\n",
      "i_s=420 , [[1.0613476 1.        1.       ]]\n",
      "i_s=421 , [[2.48395491 0.         0.        ]]\n",
      "i_s=422 , [[0.91316056 1.         1.        ]]\n",
      "i_s=423 , [[2.66017628 0.         1.        ]]\n",
      "i_s=424 , [[0.7406283 1.        1.       ]]\n",
      "i_s=425 , [[1.00436115 1.         1.        ]]\n",
      "i_s=426 , [[0.02980758 1.         1.        ]]\n",
      "i_s=427 , [[1.33656275 1.         1.        ]]\n",
      "i_s=428 , [[1.80866981 0.         1.        ]]\n",
      "i_s=429 , [[5.2450223 0.        0.       ]]\n",
      "i_s=430 , [[0.31629741 1.         1.        ]]\n",
      "i_s=431 , [[1.14052284 1.         1.        ]]\n",
      "i_s=432 , [[1.92892361 0.         1.        ]]\n",
      "i_s=433 , [[1.76196909 1.         1.        ]]\n",
      "i_s=434 , [[2.4134388 0.        1.       ]]\n",
      "i_s=435 , [[2.71707296 0.         1.        ]]\n",
      "i_s=436 , [[1.77947104 0.         1.        ]]\n",
      "i_s=437 , [[4.233634 0.       0.      ]]\n",
      "i_s=438 , [[3.09842587 0.         1.        ]]\n",
      "i_s=439 , [[1.77729881 1.         1.        ]]\n",
      "i_s=440 , [[2.74041581 0.         1.        ]]\n",
      "i_s=441 , [[0.87342155 1.         1.        ]]\n",
      "i_s=442 , [[2.20934725 0.         1.        ]]\n",
      "i_s=443 , [[3.57183266 0.         1.        ]]\n",
      "i_s=444 , [[0.95532608 0.         1.        ]]\n",
      "i_s=445 , [[2.73660183 0.         1.        ]]\n",
      "i_s=446 , [[0.95034039 1.         1.        ]]\n",
      "i_s=447 , [[2.53886938 0.         1.        ]]\n",
      "i_s=448 , [[1.47722542 0.         1.        ]]\n",
      "i_s=449 , [[0.7790345 1.        1.       ]]\n",
      "i_s=450 , [[2.59236717 0.         1.        ]]\n",
      "i_s=451 , [[0.2344625 1.        1.       ]]\n",
      "i_s=452 , [[1.2089299 1.        1.       ]]\n",
      "i_s=453 , [[0.26888093 1.         1.        ]]\n",
      "i_s=454 , [[0.95122242 1.         1.        ]]\n",
      "i_s=455 , [[1.84020686 0.         1.        ]]\n",
      "i_s=456 , [[1.38219512 0.         1.        ]]\n",
      "i_s=457 , [[2.28385305 0.         1.        ]]\n",
      "i_s=458 , [[1.53848207 1.         1.        ]]\n",
      "i_s=459 , [[6.14112186 0.         0.        ]]\n",
      "i_s=460 , [[3.350389 0.       0.      ]]\n",
      "i_s=461 , [[1.0900588 1.        1.       ]]\n",
      "i_s=462 , [[2.35656857 0.         1.        ]]\n",
      "i_s=463 , [[5.06042767 0.         0.        ]]\n",
      "i_s=464 , [[2.1646204 0.        1.       ]]\n",
      "i_s=465 , [[3.64783716 0.         0.        ]]\n",
      "i_s=466 , [[2.60395384 0.         1.        ]]\n",
      "i_s=467 , [[2.10678911 0.         1.        ]]\n",
      "i_s=468 , [[1.4606638 1.        1.       ]]\n",
      "i_s=469 , [[1.83238602 0.         1.        ]]\n",
      "i_s=470 , [[2.9031806 0.        0.       ]]\n",
      "i_s=471 , [[1.72722292 0.         1.        ]]\n",
      "i_s=472 , [[1.70736432 0.         1.        ]]\n",
      "i_s=473 , [[1.90384877 0.         1.        ]]\n",
      "i_s=474 , [[0.96331573 1.         1.        ]]\n",
      "i_s=475 , [[2.11643219 0.         1.        ]]\n",
      "i_s=476 , [[2.41801596 0.         1.        ]]\n",
      "i_s=477 , [[3.88601089 0.         0.        ]]\n",
      "i_s=478 , [[0.7218188 1.        1.       ]]\n",
      "i_s=479 , [[2.53130436 0.         1.        ]]\n",
      "i_s=480 , [[1.14043367 1.         1.        ]]\n",
      "i_s=481 , [[3.4309504 0.        0.       ]]\n",
      "i_s=482 , [[1.97809815 0.         1.        ]]\n",
      "i_s=483 , [[2.90766954 0.         1.        ]]\n",
      "i_s=484 , [[2.79460955 0.         1.        ]]\n",
      "i_s=485 , [[2.40718079 0.         1.        ]]\n",
      "i_s=486 , [[3.57805753 0.         0.        ]]\n",
      "i_s=487 , [[1.94234037 0.         1.        ]]\n",
      "i_s=488 , [[0.75128031 1.         1.        ]]\n",
      "i_s=489 , [[1.35302794 1.         1.        ]]\n",
      "i_s=490 , [[1.82829547 0.         1.        ]]\n",
      "i_s=491 , [[2.73660779 0.         1.        ]]\n",
      "i_s=492 , [[2.39249516 0.         1.        ]]\n",
      "i_s=493 , [[1.80837297 0.         1.        ]]\n",
      "i_s=494 , [[3.88611078 0.         0.        ]]\n",
      "i_s=495 , [[1.84002352 0.         1.        ]]\n",
      "i_s=496 , [[1.19176972 1.         1.        ]]\n",
      "i_s=497 , [[1.66917205 0.         1.        ]]\n",
      "i_s=498 , [[2.61855507 0.         1.        ]]\n",
      "i_s=499 , [[1.72862554 1.         1.        ]]\n",
      "i_s=500 , [[3.02256632 0.         0.        ]]\n",
      "i_s=501 , [[0.98024422 1.         1.        ]]\n",
      "i_s=502 , [[1.10689461 1.         1.        ]]\n",
      "i_s=503 , [[1.8410666 0.        1.       ]]\n",
      "i_s=504 , [[3.57751966 0.         0.        ]]\n",
      "i_s=505 , [[0.45530373 1.         1.        ]]\n",
      "i_s=506 , [[1.51293719 1.         1.        ]]\n",
      "i_s=507 , [[1.14210296 1.         1.        ]]\n",
      "i_s=508 , [[1.15668809 0.         1.        ]]\n",
      "i_s=509 , [[3.05950856 0.         0.        ]]\n",
      "i_s=510 , [[1.11722231 1.         1.        ]]\n",
      "i_s=511 , [[0.0291917 1.        1.       ]]\n",
      "i_s=512 , [[1.00384307 1.         1.        ]]\n",
      "i_s=513 , [[0.98788023 1.         1.        ]]\n",
      "i_s=514 , [[3.24070048 0.         0.        ]]\n",
      "i_s=515 , [[0.82405216 1.         1.        ]]\n",
      "i_s=516 , [[1.13928914 1.         1.        ]]\n",
      "i_s=517 , [[1.22604954 1.         1.        ]]\n",
      "i_s=518 , [[1.51275158 0.         1.        ]]\n",
      "i_s=519 , [[0.63902563 1.         1.        ]]\n",
      "i_s=520 , [[4.37182283 0.         0.        ]]\n",
      "i_s=521 , [[1.97279418 0.         1.        ]]\n",
      "i_s=522 , [[1.42214513 0.         1.        ]]\n",
      "i_s=523 , [[4.08484125 0.         0.        ]]\n",
      "i_s=524 , [[3.14128494 0.         0.        ]]\n",
      "i_s=525 , [[1.62230968 0.         1.        ]]\n",
      "i_s=526 , [[2.14320755 0.         1.        ]]\n",
      "i_s=527 , [[3.64883828 0.         1.        ]]\n",
      "i_s=528 , [[0.56609273 1.         1.        ]]\n",
      "i_s=529 , [[3.15989304 0.         0.        ]]\n",
      "i_s=530 , [[3.98567104 0.         0.        ]]\n",
      "i_s=531 , [[3.22368622 0.         0.        ]]\n",
      "i_s=532 , [[3.72051668 0.         0.        ]]\n",
      "i_s=533 , [[1.92468011 0.         1.        ]]\n",
      "i_s=534 , [[3.19113684 0.         1.        ]]\n",
      "i_s=535 , [[4.10566616 0.         0.        ]]\n",
      "i_s=536 , [[3.40007782 0.         1.        ]]\n",
      "i_s=537 , [[1.71532857 0.         1.        ]]\n",
      "i_s=538 , [[3.94472885 0.         0.        ]]\n",
      "i_s=539 , [[3.57875204 0.         1.        ]]\n",
      "i_s=540 , [[1.38649559 0.         1.        ]]\n",
      "i_s=541 , [[0.49628881 1.         1.        ]]\n",
      "i_s=542 , [[2.98791265 0.         0.        ]]\n",
      "i_s=543 , [[0.5610761 1.        1.       ]]\n",
      "i_s=544 , [[1.74463892 1.         1.        ]]\n",
      "i_s=545 , [[0.33408988 1.         1.        ]]\n",
      "i_s=546 , [[2.39785028 0.         1.        ]]\n",
      "i_s=547 , [[0.73723781 1.         1.        ]]\n",
      "i_s=548 , [[1.47457027 1.         1.        ]]\n",
      "i_s=549 , [[0.88057774 1.         1.        ]]\n",
      "i_s=550 , [[0.45090026 1.         1.        ]]\n",
      "i_s=551 , [[1.55071092 0.         1.        ]]\n",
      "i_s=552 , [[2.0356009 0.        1.       ]]\n",
      "i_s=553 , [[1.62943375 0.         1.        ]]\n",
      "i_s=554 , [[2.35075617 0.         1.        ]]\n",
      "i_s=555 , [[0.97725713 0.         1.        ]]\n",
      "i_s=556 , [[1.22426379 1.         1.        ]]\n",
      "i_s=557 , [[3.30046368 0.         0.        ]]\n",
      "i_s=558 , [[0.15348227 1.         1.        ]]\n",
      "i_s=559 , [[2.11755228 0.         1.        ]]\n",
      "i_s=560 , [[2.67179418 0.         0.        ]]\n",
      "i_s=561 , [[1.38595057 1.         1.        ]]\n",
      "i_s=562 , [[2.94308043 0.         1.        ]]\n",
      "i_s=563 , [[2.07393789 0.         1.        ]]\n",
      "i_s=564 , [[1.66566539 0.         1.        ]]\n",
      "i_s=565 , [[2.63415384 0.         0.        ]]\n",
      "i_s=566 , [[0.27823821 1.         1.        ]]\n",
      "i_s=567 , [[0.44770485 1.         1.        ]]\n",
      "i_s=568 , [[1.9217397 0.        1.       ]]\n",
      "i_s=569 , [[1.81152534 0.         1.        ]]\n",
      "i_s=570 , [[1.12699246 1.         1.        ]]\n",
      "i_s=571 , [[2.17374468 0.         1.        ]]\n",
      "i_s=572 , [[3.29609108 0.         0.        ]]\n",
      "i_s=573 , [[1.48775911 1.         1.        ]]\n",
      "i_s=574 , [[1.75735784 1.         1.        ]]\n",
      "i_s=575 , [[2.24790573 0.         1.        ]]\n",
      "i_s=576 , [[3.62400484 0.         0.        ]]\n",
      "i_s=577 , [[1.94856417 0.         1.        ]]\n",
      "i_s=578 , [[0.25331208 1.         1.        ]]\n",
      "i_s=579 , [[1.91647172 0.         1.        ]]\n",
      "i_s=580 , [[1.72159612 0.         1.        ]]\n",
      "i_s=581 , [[2.24817753 0.         1.        ]]\n",
      "i_s=582 , [[3.61095405 0.         0.        ]]\n",
      "i_s=583 , [[5.44043732 0.         0.        ]]\n",
      "i_s=584 , [[2.59036636 0.         1.        ]]\n",
      "i_s=585 , [[2.70103788 0.         1.        ]]\n",
      "i_s=586 , [[0.91834027 1.         1.        ]]\n",
      "i_s=587 , [[2.26873565 0.         1.        ]]\n",
      "i_s=588 , [[1.23272586 1.         1.        ]]\n",
      "i_s=589 , [[1.24977982 1.         1.        ]]\n",
      "i_s=590 , [[2.65973091 0.         1.        ]]\n",
      "i_s=591 , [[0.62089312 1.         1.        ]]\n",
      "i_s=592 , [[2.51552176 0.         1.        ]]\n",
      "i_s=593 , [[2.03402138 0.         1.        ]]\n",
      "i_s=594 , [[3.75083447 0.         0.        ]]\n",
      "i_s=595 , [[0.18250825 1.         1.        ]]\n",
      "i_s=596 , [[0.76862663 1.         1.        ]]\n",
      "i_s=597 , [[1.42133713 1.         1.        ]]\n",
      "i_s=598 , [[1.24102199 1.         1.        ]]\n",
      "i_s=599 , [[3.86917496 0.         0.        ]]\n",
      "i_s=600 , [[1.17342842 1.         1.        ]]\n",
      "i_s=601 , [[3.70329285 0.         0.        ]]\n",
      "i_s=602 , [[7.06895542 0.         0.        ]]\n",
      "i_s=603 , [[0.44759941 1.         1.        ]]\n",
      "i_s=604 , [[4.14219856 0.         0.        ]]\n",
      "i_s=605 , [[2.05337477 0.         1.        ]]\n",
      "i_s=606 , [[0.63817477 1.         1.        ]]\n",
      "i_s=607 , [[2.50605178 0.         1.        ]]\n",
      "i_s=608 , [[1.89719439 1.         1.        ]]\n",
      "i_s=609 , [[3.44212484 0.         0.        ]]\n",
      "i_s=610 , [[3.45069265 0.         0.        ]]\n",
      "i_s=611 , [[2.61286569 0.         1.        ]]\n",
      "i_s=612 , [[1.70144534 0.         1.        ]]\n",
      "i_s=613 , [[0.97918236 1.         1.        ]]\n",
      "i_s=614 , [[0.53316629 1.         1.        ]]\n",
      "i_s=615 , [[2.22936296 0.         1.        ]]\n",
      "i_s=616 , [[1.80318093 0.         1.        ]]\n",
      "i_s=617 , [[0.80565906 1.         1.        ]]\n",
      "i_s=618 , [[1.15248334 1.         1.        ]]\n",
      "i_s=619 , [[1.75117707 0.         1.        ]]\n",
      "i_s=620 , [[0.9508146 1.        1.       ]]\n",
      "i_s=621 , [[2.1050036 0.        1.       ]]\n",
      "i_s=622 , [[2.59091043 0.         1.        ]]\n",
      "i_s=623 , [[2.7369318 0.        1.       ]]\n",
      "i_s=624 , [[2.82445359 0.         0.        ]]\n",
      "i_s=625 , [[2.33578873 0.         1.        ]]\n",
      "i_s=626 , [[1.81560075 0.         1.        ]]\n",
      "i_s=627 , [[3.261446 0.       0.      ]]\n",
      "i_s=628 , [[0.46280271 1.         1.        ]]\n",
      "i_s=629 , [[0.49093589 1.         1.        ]]\n",
      "i_s=630 , [[1.97212434 0.         1.        ]]\n",
      "i_s=631 , [[3.47510672 0.         0.        ]]\n",
      "i_s=632 , [[2.71740246 0.         1.        ]]\n",
      "i_s=633 , [[2.48031664 0.         1.        ]]\n",
      "i_s=634 , [[1.82314086 0.         1.        ]]\n",
      "i_s=635 , [[2.03681469 0.         1.        ]]\n",
      "i_s=636 , [[7.68684483 0.         0.        ]]\n",
      "i_s=637 , [[3.97340822 0.         0.        ]]\n",
      "i_s=638 , [[2.02792645 0.         1.        ]]\n",
      "i_s=639 , [[0.90462929 1.         1.        ]]\n",
      "i_s=640 , [[1.03986382 1.         1.        ]]\n",
      "i_s=641 , [[2.88953733 0.         0.        ]]\n",
      "i_s=642 , [[3.57441783 0.         0.        ]]\n",
      "i_s=643 , [[2.34538674 0.         1.        ]]\n",
      "i_s=644 , [[2.49066806 0.         1.        ]]\n",
      "i_s=645 , [[1.40518308 1.         1.        ]]\n",
      "i_s=646 , [[4.31208944 0.         0.        ]]\n",
      "i_s=647 , [[2.14667392 0.         1.        ]]\n",
      "i_s=648 , [[2.46774626 0.         1.        ]]\n",
      "i_s=649 , [[1.22240114 1.         1.        ]]\n",
      "i_s=650 , [[4.6775713 0.        0.       ]]\n",
      "i_s=651 , [[1.81282616 0.         1.        ]]\n",
      "i_s=652 , [[1.3335793 0.        1.       ]]\n",
      "i_s=653 , [[2.09739923 0.         1.        ]]\n",
      "i_s=654 , [[1.64700818 0.         1.        ]]\n",
      "i_s=655 , [[2.16140461 0.         1.        ]]\n",
      "i_s=656 , [[1.39437163 0.         1.        ]]\n",
      "i_s=657 , [[1.39446473 0.         1.        ]]\n",
      "i_s=658 , [[3.54069471 0.         0.        ]]\n",
      "i_s=659 , [[2.90084481 0.         0.        ]]\n",
      "i_s=660 , [[1.41774487 0.         1.        ]]\n",
      "i_s=661 , [[0.62287199 1.         1.        ]]\n",
      "i_s=662 , [[1.66682601 0.         1.        ]]\n",
      "i_s=663 , [[2.99253321 0.         0.        ]]\n",
      "i_s=664 , [[1.32561839 1.         1.        ]]\n",
      "i_s=665 , [[1.85865533 0.         1.        ]]\n",
      "i_s=666 , [[2.24704456 0.         1.        ]]\n",
      "i_s=667 , [[0.32121876 1.         1.        ]]\n",
      "i_s=668 , [[1.31924927 1.         1.        ]]\n",
      "i_s=669 , [[1.23275197 0.         1.        ]]\n",
      "i_s=670 , [[2.77857685 0.         1.        ]]\n",
      "i_s=671 , [[2.21553135 0.         1.        ]]\n",
      "i_s=672 , [[1.96074009 1.         1.        ]]\n",
      "i_s=673 , [[0.69470572 1.         1.        ]]\n",
      "i_s=674 , [[1.20196283 1.         1.        ]]\n",
      "i_s=675 , [[3.7179935 0.        0.       ]]\n",
      "i_s=676 , [[1.97014642 0.         1.        ]]\n",
      "i_s=677 , [[2.52045083 0.         1.        ]]\n",
      "i_s=678 , [[1.17016101 1.         1.        ]]\n",
      "i_s=679 , [[2.14841485 0.         1.        ]]\n",
      "i_s=680 , [[3.16471815 0.         0.        ]]\n",
      "i_s=681 , [[5.89966488 0.         0.        ]]\n",
      "i_s=682 , [[1.96304953 0.         1.        ]]\n",
      "i_s=683 , [[2.62667274 0.         1.        ]]\n",
      "i_s=684 , [[1.13437247 1.         1.        ]]\n",
      "i_s=685 , [[0.47711331 1.         1.        ]]\n",
      "i_s=686 , [[0.4435105 1.        1.       ]]\n",
      "i_s=687 , [[2.09246874 0.         1.        ]]\n",
      "i_s=688 , [[0.42960501 1.         1.        ]]\n",
      "i_s=689 , [[1.61621809 0.         1.        ]]\n",
      "i_s=690 , [[1.25684226 1.         1.        ]]\n",
      "i_s=691 , [[0.63612199 1.         1.        ]]\n",
      "i_s=692 , [[1.51647592 0.         1.        ]]\n",
      "i_s=693 , [[0.89127421 1.         1.        ]]\n",
      "i_s=694 , [[6.75500441 0.         0.        ]]\n",
      "i_s=695 , [[2.62906742 0.         1.        ]]\n",
      "i_s=696 , [[2.54574442 0.         1.        ]]\n",
      "i_s=697 , [[0.95787215 1.         1.        ]]\n",
      "i_s=698 , [[3.42616749 0.         0.        ]]\n",
      "i_s=699 , [[3.23232818 0.         0.        ]]\n",
      "i_s=700 , [[2.83386397 0.         1.        ]]\n",
      "i_s=701 , [[1.5298692 1.        1.       ]]\n",
      "i_s=702 , [[4.84586573 0.         0.        ]]\n",
      "i_s=703 , [[0.63048542 1.         1.        ]]\n",
      "i_s=704 , [[2.18962765 0.         1.        ]]\n",
      "i_s=705 , [[1.78299224 0.         1.        ]]\n",
      "i_s=706 , [[1.74622595 0.         1.        ]]\n",
      "i_s=707 , [[3.19607759 0.         0.        ]]\n",
      "i_s=708 , [[1.33293092 1.         1.        ]]\n",
      "i_s=709 , [[2.40478659 0.         1.        ]]\n",
      "i_s=710 , [[1.82094789 0.         1.        ]]\n",
      "i_s=711 , [[3.05447435 0.         1.        ]]\n",
      "i_s=712 , [[4.36374712 0.         0.        ]]\n",
      "i_s=713 , [[0.18362033 1.         1.        ]]\n",
      "i_s=714 , [[0.46098149 1.         1.        ]]\n",
      "i_s=715 , [[3.05368614 0.         0.        ]]\n",
      "i_s=716 , [[1.67292571 0.         1.        ]]\n",
      "i_s=717 , [[1.23004651 0.         1.        ]]\n",
      "i_s=718 , [[2.64939666 0.         1.        ]]\n",
      "i_s=719 , [[3.22911072 0.         1.        ]]\n",
      "i_s=720 , [[5.61298466 0.         0.        ]]\n",
      "i_s=721 , [[3.22081757 0.         0.        ]]\n",
      "i_s=722 , [[4.50298405 0.         1.        ]]\n",
      "i_s=723 , [[1.09075284 1.         1.        ]]\n",
      "i_s=724 , [[4.31899452 0.         0.        ]]\n",
      "i_s=725 , [[0.89872098 1.         1.        ]]\n",
      "i_s=726 , [[1.75626659 0.         1.        ]]\n",
      "i_s=727 , [[0.59653783 1.         1.        ]]\n",
      "i_s=728 , [[0.79774398 1.         1.        ]]\n",
      "i_s=729 , [[1.42004955 1.         1.        ]]\n",
      "i_s=730 , [[0.47338235 1.         1.        ]]\n",
      "i_s=731 , [[4.88692284 0.         0.        ]]\n",
      "i_s=732 , [[2.66068482 0.         1.        ]]\n",
      "i_s=733 , [[1.65976226 1.         1.        ]]\n",
      "i_s=734 , [[1.44689918 1.         1.        ]]\n",
      "i_s=735 , [[0.99203569 0.         1.        ]]\n",
      "i_s=736 , [[1.24860513 1.         1.        ]]\n",
      "i_s=737 , [[4.01741409 0.         0.        ]]\n",
      "i_s=738 , [[0.35756779 1.         1.        ]]\n",
      "i_s=739 , [[1.42686343 1.         1.        ]]\n",
      "i_s=740 , [[1.18008327 1.         1.        ]]\n",
      "i_s=741 , [[2.05271268 0.         1.        ]]\n",
      "i_s=742 , [[2.16881466 0.         1.        ]]\n",
      "i_s=743 , [[0.90161896 1.         1.        ]]\n",
      "i_s=744 , [[2.37051821 0.         1.        ]]\n",
      "i_s=745 , [[1.1100266 1.        1.       ]]\n",
      "i_s=746 , [[2.1057601 0.        1.       ]]\n",
      "i_s=747 , [[2.99786997 0.         0.        ]]\n",
      "i_s=748 , [[1.8550936 0.        1.       ]]\n",
      "i_s=749 , [[1.9259361 0.        1.       ]]\n",
      "i_s=750 , [[3.48396015 0.         0.        ]]\n",
      "i_s=751 , [[4.6782155 0.        0.       ]]\n",
      "i_s=752 , [[3.55692649 0.         0.        ]]\n",
      "i_s=753 , [[0.62691343 1.         1.        ]]\n",
      "i_s=754 , [[0.2743755 1.        1.       ]]\n",
      "i_s=755 , [[3.97140622 0.         0.        ]]\n",
      "i_s=756 , [[1.3427217 1.        1.       ]]\n",
      "i_s=757 , [[0.38493362 1.         1.        ]]\n",
      "i_s=758 , [[2.18669558 0.         1.        ]]\n",
      "i_s=759 , [[1.67802238 0.         1.        ]]\n",
      "i_s=760 , [[0.96206999 1.         1.        ]]\n",
      "i_s=761 , [[0.89541978 1.         1.        ]]\n",
      "i_s=762 , [[4.28645086 0.         0.        ]]\n",
      "i_s=763 , [[1.75046539 0.         1.        ]]\n",
      "i_s=764 , [[0.44132456 1.         1.        ]]\n",
      "i_s=765 , [[0.75845098 1.         1.        ]]\n",
      "i_s=766 , [[0.59140551 1.         1.        ]]\n",
      "i_s=767 , [[0.72515553 1.         1.        ]]\n",
      "i_s=768 , [[3.34602714 0.         0.        ]]\n",
      "i_s=769 , [[0.51619053 1.         1.        ]]\n",
      "i_s=770 , [[2.51643038 0.         0.        ]]\n",
      "i_s=771 , [[2.27160025 0.         1.        ]]\n",
      "i_s=772 , [[1.98571503 0.         1.        ]]\n",
      "i_s=773 , [[3.34722996 0.         0.        ]]\n",
      "i_s=774 , [[1.85004163 0.         1.        ]]\n",
      "i_s=775 , [[2.642699 0.       1.      ]]\n",
      "i_s=776 , [[2.37398767 0.         1.        ]]\n",
      "i_s=777 , [[2.76152039 0.         0.        ]]\n",
      "i_s=778 , [[2.66595554 0.         1.        ]]\n",
      "i_s=779 , [[2.75398493 0.         0.        ]]\n",
      "i_s=780 , [[2.00393057 0.         1.        ]]\n",
      "i_s=781 , [[0.61359602 1.         1.        ]]\n",
      "i_s=782 , [[2.6364007 0.        1.       ]]\n",
      "i_s=783 , [[1.75310111 0.         1.        ]]\n",
      "i_s=784 , [[1.94900227 0.         1.        ]]\n",
      "i_s=785 , [[1.00652623 1.         1.        ]]\n",
      "i_s=786 , [[0.88102287 1.         1.        ]]\n",
      "i_s=787 , [[2.8293159 0.        1.       ]]\n",
      "i_s=788 , [[2.21630955 0.         1.        ]]\n",
      "i_s=789 , [[3.46247768 0.         0.        ]]\n",
      "i_s=790 , [[1.12528658 1.         1.        ]]\n",
      "i_s=791 , [[6.17500782 0.         0.        ]]\n",
      "i_s=792 , [[5.99558592 0.         0.        ]]\n",
      "i_s=793 , [[3.11140203 0.         0.        ]]\n",
      "i_s=794 , [[0.09156773 1.         1.        ]]\n",
      "i_s=795 , [[1.48239267 0.         1.        ]]\n",
      "i_s=796 , [[3.01009297 0.         1.        ]]\n",
      "i_s=797 , [[0.82909483 1.         1.        ]]\n",
      "i_s=798 , [[2.54758644 0.         1.        ]]\n",
      "i_s=799 , [[3.22368193 0.         0.        ]]\n",
      "i_s=800 , [[1.10131752 1.         1.        ]]\n",
      "i_s=801 , [[2.46561289 0.         1.        ]]\n",
      "i_s=802 , [[1.49768138 1.         1.        ]]\n",
      "i_s=803 , [[1.83190596 0.         1.        ]]\n",
      "i_s=804 , [[1.75673735 0.         1.        ]]\n",
      "i_s=805 , [[3.48643923 0.         0.        ]]\n",
      "i_s=806 , [[0.86623484 1.         1.        ]]\n",
      "i_s=807 , [[0.26229307 1.         1.        ]]\n",
      "i_s=808 , [[0.73268795 1.         1.        ]]\n",
      "i_s=809 , [[0.54008156 1.         1.        ]]\n",
      "i_s=810 , [[0.56460011 1.         1.        ]]\n",
      "i_s=811 , [[0.80972904 1.         1.        ]]\n",
      "i_s=812 , [[2.09011507 0.         1.        ]]\n",
      "i_s=813 , [[3.27016592 0.         1.        ]]\n",
      "i_s=814 , [[2.00763559 0.         1.        ]]\n",
      "i_s=815 , [[1.60311079 1.         1.        ]]\n",
      "i_s=816 , [[0.44363841 1.         1.        ]]\n",
      "i_s=817 , [[1.33648074 0.         1.        ]]\n",
      "i_s=818 , [[0.73384815 1.         1.        ]]\n",
      "i_s=819 , [[0.85212851 1.         1.        ]]\n",
      "i_s=820 , [[1.14968395 0.         1.        ]]\n",
      "i_s=821 , [[1.22572041 1.         1.        ]]\n",
      "i_s=822 , [[1.30415368 1.         1.        ]]\n",
      "i_s=823 , [[2.14617968 0.         1.        ]]\n",
      "i_s=824 , [[0.8944838 1.        1.       ]]\n",
      "i_s=825 , [[3.16227579 0.         0.        ]]\n",
      "i_s=826 , [[0.41730267 1.         1.        ]]\n",
      "i_s=827 , [[2.64491224 0.         0.        ]]\n",
      "i_s=828 , [[2.27822447 0.         1.        ]]\n",
      "i_s=829 , [[1.98187602 0.         1.        ]]\n",
      "i_s=830 , [[1.95509005 0.         1.        ]]\n",
      "i_s=831 , [[1.07666183 1.         1.        ]]\n",
      "i_s=832 , [[5.91580153 0.         0.        ]]\n",
      "i_s=833 , [[2.19470906 0.         1.        ]]\n",
      "i_s=834 , [[1.48210931 0.         1.        ]]\n",
      "i_s=835 , [[1.19619453 1.         1.        ]]\n",
      "i_s=836 , [[4.60536003 0.         0.        ]]\n",
      "i_s=837 , [[2.63869023 0.         1.        ]]\n",
      "i_s=838 , [[2.87906075 0.         1.        ]]\n",
      "i_s=839 , [[2.38172674 0.         0.        ]]\n",
      "i_s=840 , [[0.69535571 1.         1.        ]]\n",
      "i_s=841 , [[1.64686453 1.         1.        ]]\n",
      "i_s=842 , [[1.70735812 0.         1.        ]]\n",
      "i_s=843 , [[3.44278431 0.         0.        ]]\n",
      "i_s=844 , [[2.87454128 0.         0.        ]]\n",
      "i_s=845 , [[1.26542699 1.         1.        ]]\n",
      "i_s=846 , [[1.14392996 1.         1.        ]]\n",
      "i_s=847 , [[2.8059864 0.        0.       ]]\n",
      "i_s=848 , [[2.86982656 0.         1.        ]]\n",
      "i_s=849 , [[3.27418566 0.         0.        ]]\n",
      "i_s=850 , [[1.05003321 1.         1.        ]]\n",
      "i_s=851 , [[0.3943128 1.        1.       ]]\n",
      "i_s=852 , [[1.3843441 1.        1.       ]]\n",
      "i_s=853 , [[0.74498701 1.         1.        ]]\n",
      "i_s=854 , [[1.18318641 0.         1.        ]]\n",
      "i_s=855 , [[0.74920893 1.         1.        ]]\n",
      "i_s=856 , [[0.97868204 1.         1.        ]]\n",
      "i_s=857 , [[3.37834668 0.         0.        ]]\n",
      "i_s=858 , [[3.21485233 0.         0.        ]]\n",
      "i_s=859 , [[0.29659441 1.         1.        ]]\n",
      "i_s=860 , [[1.84465396 0.         1.        ]]\n",
      "i_s=861 , [[2.38269591 0.         1.        ]]\n",
      "i_s=862 , [[0.79522955 1.         1.        ]]\n",
      "i_s=863 , [[2.44210267 0.         1.        ]]\n",
      "i_s=864 , [[2.15830469 0.         1.        ]]\n",
      "i_s=865 , [[3.79928446 0.         0.        ]]\n",
      "i_s=866 , [[1.57676637 1.         1.        ]]\n",
      "i_s=867 , [[2.51220918 0.         1.        ]]\n",
      "i_s=868 , [[2.88220549 0.         0.        ]]\n",
      "i_s=869 , [[4.17151976 0.         0.        ]]\n",
      "i_s=870 , [[0.73288524 1.         1.        ]]\n",
      "i_s=871 , [[0.7940436 1.        1.       ]]\n",
      "i_s=872 , [[2.6353898 0.        1.       ]]\n",
      "i_s=873 , [[1.77289724 0.         1.        ]]\n",
      "i_s=874 , [[1.20553374 1.         1.        ]]\n",
      "i_s=875 , [[2.09478593 0.         1.        ]]\n",
      "i_s=876 , [[1.60987437 0.         1.        ]]\n",
      "i_s=877 , [[2.45121145 0.         0.        ]]\n",
      "i_s=878 , [[1.77082253 1.         1.        ]]\n",
      "i_s=879 , [[2.15605283 0.         1.        ]]\n",
      "i_s=880 , [[3.59358764 0.         1.        ]]\n",
      "i_s=881 , [[0.82142252 1.         1.        ]]\n",
      "i_s=882 , [[3.29505873 0.         0.        ]]\n",
      "i_s=883 , [[0.42755145 1.         1.        ]]\n",
      "i_s=884 , [[2.92761374 0.         0.        ]]\n",
      "i_s=885 , [[2.61769557 0.         1.        ]]\n",
      "i_s=886 , [[0.47812885 1.         1.        ]]\n",
      "i_s=887 , [[0.76116824 1.         1.        ]]\n",
      "i_s=888 , [[3.13412476 0.         0.        ]]\n",
      "i_s=889 , [[2.88680816 0.         1.        ]]\n",
      "i_s=890 , [[1.59930289 0.         1.        ]]\n",
      "i_s=891 , [[4.61819792 0.         0.        ]]\n",
      "i_s=892 , [[3.58728647 0.         0.        ]]\n",
      "i_s=893 , [[1.44345725 1.         1.        ]]\n",
      "i_s=894 , [[2.66868997 0.         1.        ]]\n",
      "i_s=895 , [[0.57297808 1.         1.        ]]\n",
      "i_s=896 , [[1.53316569 0.         1.        ]]\n",
      "i_s=897 , [[1.51860988 1.         1.        ]]\n",
      "i_s=898 , [[1.247051 0.       1.      ]]\n",
      "i_s=899 , [[2.47545981 0.         1.        ]]\n",
      "i_s=900 , [[1.74995565 0.         1.        ]]\n",
      "i_s=901 , [[1.02064717 1.         1.        ]]\n",
      "i_s=902 , [[0.49930042 1.         1.        ]]\n",
      "i_s=903 , [[5.12135077 0.         0.        ]]\n",
      "i_s=904 , [[1.53052628 1.         1.        ]]\n",
      "i_s=905 , [[3.44327736 0.         0.        ]]\n",
      "i_s=906 , [[3.6567378 0.        0.       ]]\n",
      "i_s=907 , [[2.58466339 0.         1.        ]]\n",
      "i_s=908 , [[3.14120913 0.         0.        ]]\n",
      "i_s=909 , [[1.58595848 1.         1.        ]]\n",
      "i_s=910 , [[2.01628971 0.         1.        ]]\n",
      "i_s=911 , [[3.06770945 0.         1.        ]]\n",
      "i_s=912 , [[1.63141119 0.         1.        ]]\n",
      "i_s=913 , [[2.23533821 0.         1.        ]]\n",
      "i_s=914 , [[2.74956465 0.         1.        ]]\n",
      "i_s=915 , [[2.13914061 0.         1.        ]]\n",
      "i_s=916 , [[1.37036514 1.         1.        ]]\n",
      "i_s=917 , [[1.75355339 0.         1.        ]]\n",
      "i_s=918 , [[1.12508941 1.         1.        ]]\n",
      "i_s=919 , [[1.2875911 1.        1.       ]]\n",
      "i_s=920 , [[1.4490757 1.        1.       ]]\n",
      "i_s=921 , [[3.27318382 0.         0.        ]]\n",
      "i_s=922 , [[0.24822873 1.         1.        ]]\n",
      "i_s=923 , [[1.21459639 1.         1.        ]]\n",
      "i_s=924 , [[2.47098422 0.         1.        ]]\n",
      "i_s=925 , [[2.61726284 0.         0.        ]]\n",
      "i_s=926 , [[2.18799138 0.         1.        ]]\n",
      "i_s=927 , [[2.41813421 0.         1.        ]]\n",
      "i_s=928 , [[2.86762381 0.         0.        ]]\n",
      "i_s=929 , [[1.93648434 0.         1.        ]]\n",
      "i_s=930 , [[3.71113539 0.         0.        ]]\n",
      "i_s=931 , [[2.89883256 0.         1.        ]]\n",
      "i_s=932 , [[4.44015265 0.         0.        ]]\n",
      "i_s=933 , [[1.37651038 0.         1.        ]]\n",
      "i_s=934 , [[1.50021756 1.         1.        ]]\n",
      "i_s=935 , [[0.53836846 1.         1.        ]]\n",
      "i_s=936 , [[2.2007308 0.        1.       ]]\n",
      "i_s=937 , [[2.09082365 0.         1.        ]]\n",
      "i_s=938 , [[4.21334696 0.         0.        ]]\n",
      "i_s=939 , [[0.98520947 1.         1.        ]]\n",
      "i_s=940 , [[4.4449563 0.        0.       ]]\n",
      "i_s=941 , [[0.99344909 1.         1.        ]]\n",
      "i_s=942 , [[2.19659948 0.         1.        ]]\n",
      "i_s=943 , [[1.41260254 1.         1.        ]]\n",
      "i_s=944 , [[0.21082085 1.         1.        ]]\n",
      "i_s=945 , [[1.94012177 0.         1.        ]]\n",
      "i_s=946 , [[2.97405624 0.         0.        ]]\n",
      "i_s=947 , [[1.71468675 0.         1.        ]]\n",
      "i_s=948 , [[2.5059123 0.        1.       ]]\n",
      "i_s=949 , [[1.31874382 1.         1.        ]]\n",
      "i_s=950 , [[2.21903849 1.         1.        ]]\n",
      "i_s=951 , [[3.57316685 0.         0.        ]]\n",
      "i_s=952 , [[5.21574116 0.         0.        ]]\n",
      "i_s=953 , [[2.66287804 0.         0.        ]]\n",
      "i_s=954 , [[2.1166172 0.        1.       ]]\n",
      "i_s=955 , [[0.22009631 1.         1.        ]]\n",
      "i_s=956 , [[2.99938583 0.         0.        ]]\n",
      "i_s=957 , [[0.82959211 1.         1.        ]]\n",
      "i_s=958 , [[0.29025871 1.         1.        ]]\n",
      "i_s=959 , [[1.99010742 0.         1.        ]]\n",
      "i_s=960 , [[1.05161166 1.         1.        ]]\n",
      "i_s=961 , [[0.23352827 1.         1.        ]]\n",
      "i_s=962 , [[0.71913439 1.         1.        ]]\n",
      "i_s=963 , [[0.47333974 1.         1.        ]]\n",
      "i_s=964 , [[3.6685648 0.        1.       ]]\n",
      "i_s=965 , [[1.41007364 1.         1.        ]]\n",
      "i_s=966 , [[2.16017246 0.         1.        ]]\n",
      "i_s=967 , [[1.51165497 1.         1.        ]]\n",
      "i_s=968 , [[1.79540873 0.         1.        ]]\n",
      "i_s=969 , [[1.91712224 0.         1.        ]]\n",
      "i_s=970 , [[0.3937521 1.        1.       ]]\n",
      "i_s=971 , [[1.90897894 0.         1.        ]]\n",
      "i_s=972 , [[1.59411061 0.         1.        ]]\n",
      "i_s=973 , [[4.03288126 0.         0.        ]]\n",
      "i_s=974 , [[2.61546636 0.         0.        ]]\n",
      "i_s=975 , [[2.92937899 0.         0.        ]]\n",
      "i_s=976 , [[2.06252575 0.         1.        ]]\n",
      "i_s=977 , [[3.25840378 0.         0.        ]]\n",
      "i_s=978 , [[0.93371177 1.         1.        ]]\n",
      "i_s=979 , [[2.55687308 0.         1.        ]]\n",
      "i_s=980 , [[2.95090723 0.         1.        ]]\n",
      "i_s=981 , [[4.00520229 0.         0.        ]]\n",
      "i_s=982 , [[0.36345914 1.         1.        ]]\n",
      "i_s=983 , [[1.15478003 1.         1.        ]]\n",
      "i_s=984 , [[2.87171245 0.         1.        ]]\n",
      "i_s=985 , [[1.53888333 1.         1.        ]]\n",
      "i_s=986 , [[1.51400018 1.         1.        ]]\n",
      "i_s=987 , [[1.72572339 0.         1.        ]]\n",
      "i_s=988 , [[1.10348785 1.         1.        ]]\n",
      "i_s=989 , [[0.76370913 1.         1.        ]]\n",
      "i_s=990 , [[2.00340939 0.         1.        ]]\n",
      "i_s=991 , [[3.17006111 0.         0.        ]]\n",
      "i_s=992 , [[5.57503366 0.         0.        ]]\n",
      "i_s=993 , [[1.76994252 0.         1.        ]]\n",
      "i_s=994 , [[0.66551614 1.         1.        ]]\n",
      "i_s=995 , [[0.94746125 1.         1.        ]]\n",
      "i_s=996 , [[0.79698074 1.         1.        ]]\n",
      "i_s=997 , [[1.07289052 1.         1.        ]]\n",
      "i_s=998 , [[3.22168922 0.         1.        ]]\n",
      "i_s=999 , [[0.72562951 1.         1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.11654385, 0.35      , 0.768     ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=7\n",
    "N_EPOCHS=3\n",
    "N_HN_1=128\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=1000\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('-----------------Build DNN model and start the 1st training!!---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "# y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "print(str(x_train.shape))\n",
    "\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print(str(x_val_i.shape))\n",
    "\n",
    "# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score0))\n",
    "print('-----------------Start the loop training!!---------------------')\n",
    "Scores_all=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):\n",
    "    x_train=featuresArrayOverSampler[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    x_val_i=featuresArrayOverSampler[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=0,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    loss_i, acc_i, top5acc_i=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "    Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])\n",
    "    \n",
    "np.mean(Scores_all, axis=0)    \n",
    "# y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "# \n",
    "# model.save('jjs_model_0124V3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验T7\\8\\9，5000Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n",
      "-----------------Build DNN model and start the 1st training!!---------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(162506, 110)\n",
      "(1, 110)\n",
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "Train on 162506 samples, validate on 1 samples\n",
      "Epoch 1/7\n",
      "162506/162506 [==============================] - 55s 338us/step - loss: 2.3322 - accuracy: 0.3052 - top_k_categorical_accuracy: 0.7169 - val_loss: 1.6945 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/7\n",
      "162506/162506 [==============================] - 53s 328us/step - loss: 2.1876 - accuracy: 0.3259 - top_k_categorical_accuracy: 0.7510 - val_loss: 1.7207 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/7\n",
      "162506/162506 [==============================] - 54s 329us/step - loss: 2.1689 - accuracy: 0.3290 - top_k_categorical_accuracy: 0.7547 - val_loss: 1.7543 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "162506/162506 [==============================] - 54s 331us/step - loss: 2.1573 - accuracy: 0.3304 - top_k_categorical_accuracy: 0.7579 - val_loss: 1.5499 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/7\n",
      "162506/162506 [==============================] - 54s 334us/step - loss: 2.1478 - accuracy: 0.3337 - top_k_categorical_accuracy: 0.7610 - val_loss: 1.6387 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "162506/162506 [==============================] - 54s 330us/step - loss: 2.1389 - accuracy: 0.3359 - top_k_categorical_accuracy: 0.7631 - val_loss: 1.6046 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "162506/162506 [==============================] - 54s 333us/step - loss: 2.1310 - accuracy: 0.3367 - top_k_categorical_accuracy: 0.7648 - val_loss: 1.7346 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "[1.7346431016921997, 1.0, 1.0]\n",
      "-----------------Start the loop training!!---------------------\n",
      "i_s=0 , [[0.46827412 1.         1.        ]]\n",
      "i_s=500 , [[1.48951292 0.         1.        ]]\n",
      "i_s=1000 , [[1.26209486 0.         1.        ]]\n",
      "i_s=1500 , [[2.18416667 0.         1.        ]]\n",
      "i_s=2000 , [[6.23316622 0.         0.        ]]\n",
      "i_s=2500 , [[3.7906518 0.        0.       ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.16264676, 0.35      , 0.76066667])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=7\n",
    "N_EPOCHS=3\n",
    "N_HN_1=128\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=3000\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('-----------------Build DNN model and start the 1st training!!---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "# y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "print(str(x_train.shape))\n",
    "\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print(str(x_val_i.shape))\n",
    "\n",
    "# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score0))\n",
    "print('-----------------Start the loop training!!---------------------')\n",
    "Scores_all=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):\n",
    "    x_train=featuresArrayOverSampler[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    x_val_i=featuresArrayOverSampler[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=0,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    loss_i, acc_i, top5acc_i=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    if i_s % 500==0:\n",
    "        print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "    Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])\n",
    "    \n",
    "np.mean(Scores_all, axis=0)    \n",
    "# y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "# \n",
    "# model.save('jjs_model_0124V3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step\n",
      "[[6.4653861e-03 1.2605715e-01 3.7149497e-04 9.7461067e-05 4.2648759e-02\n",
      "  2.8877105e-03 9.5213682e-04 2.3450056e-02 1.6544767e-03 9.3566853e-04\n",
      "  1.0607555e-04 2.6104341e-03 6.1564622e-03 2.1268701e-02 3.2990030e-04\n",
      "  1.4005005e-03 8.2611606e-02 1.6742203e-03 9.3232881e-04 7.2806448e-02\n",
      "  9.3014769e-02 8.9774162e-02 2.5247902e-05 2.1651105e-03 1.4566442e-06\n",
      "  9.7008450e-03 4.3880912e-03 1.3907293e-03 6.0129119e-03 4.0470733e-04\n",
      "  1.4272645e-03 6.5214880e-04 4.4801380e-02 9.1407009e-07 7.7429931e-03\n",
      "  1.8327191e-01 8.3375528e-02 6.4030848e-02 1.2402017e-02]]\n",
      "[1.6967843770980835, 1.0, 1.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score01))\n",
    "print(str(score0))\n",
    "print(y_val_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用打乱顺序的Train和Test集合来反向验证一下，前面的实验是否真的已经按照时间排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上实验的效果明显比按时间排序的要好4个百分点左右，反面证明了实验T1的有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=5.0\n",
    "b=3.0\n",
    "div = a // b \n",
    "div\n",
    "i_s=10000\n",
    "i_s % 500==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0., 444.,   0.,   0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[1, 2, 3]])\n",
    "print(a[0,1])\n",
    "n=np.zeros([1,4])\n",
    "n[0,1]=444\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
