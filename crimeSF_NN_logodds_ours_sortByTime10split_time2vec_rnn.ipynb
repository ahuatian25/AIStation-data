{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from copy import deepcopy\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "trainDF=pd.read_csv(\"./zj_train.csv\")\n",
    "xy_scaler=preprocessing.StandardScaler()\n",
    "xy_scaler.fit(trainDF[[\"X\",\"Y\"]])\n",
    "trainDF[[\"X\",\"Y\"]]=xy_scaler.transform(trainDF[[\"X\",\"Y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")\n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "def Dates2TDMY(x):\n",
    "    DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")\n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    T_D_M_Y=str(time)+str(day)+str(month)+str(year)\n",
    "    return T_D_M_Y\n",
    "#################\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring\n",
    "#############################################\n",
    "def field2Vec(trainDF,fieldStr):\n",
    "    fields=sorted(trainDF[fieldStr].unique())\n",
    "    categories=sorted(trainDF[\"Category\"].unique())\n",
    "    C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "    F_C_counts=trainDF.groupby([fieldStr,\"Category\"]).size()\n",
    "    F_counts=trainDF.groupby([fieldStr]).size()\n",
    "    logodds={}\n",
    "    logoddsPF={}\n",
    "    MIN_CAT_COUNTS=2\n",
    "    default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "    for f in fields:\n",
    "        PA=F_counts[f]/float(len(trainDF))\n",
    "        logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "        logodds[f]=deepcopy(default_logodds)\n",
    "        for cat in F_C_counts[f].keys():\n",
    "            if (F_C_counts[f][cat]>MIN_CAT_COUNTS) and F_C_counts[f][cat]<F_counts[f]:\n",
    "                PA=F_C_counts[f][cat]/float(F_counts[f])\n",
    "                logodds[f][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "        logodds[f]=pd.Series(logodds[f])\n",
    "        logodds[f].index=range(len(categories))\n",
    "    return logodds,logoddsPF\n",
    "####################################################\n",
    "def parse_data(df,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print(\"Creating address features\")###Creating address features###\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds_A[x])\n",
    "    address_features.columns=[\"logodds_A\"+str(x) for x in range(len(address_features.columns))]\n",
    "    print(\"Creating time T_D_M_Y features\")###Creating time T_D_M_Y features###\n",
    "    T_D_M_Y_features=cleanData[\"T_D_M_Y\"].apply(lambda xx: logodds_T[xx])\n",
    "    T_D_M_Y_features.columns=[\"logodds_T\"+str(xx) for xx in range(len(T_D_M_Y_features.columns))]\n",
    "\n",
    "    print(\"Parsing dates\")            ###Creating address features###\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "    #     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    #     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print(\"Creating one-hot variables\")\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPF_A\"]=cleanData[\"Address\"].apply(lambda x: logoddsPF_A[x])\n",
    "    cleanData[\"logoddsPF_T\"]=cleanData[\"T_D_M_Y\"].apply(lambda x: logoddsPF_T[x])\n",
    "    print(\"droping processed columns\")\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)\n",
    "    cleanData=cleanData.drop(\"T_D_M_Y\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    features = cleanData[feature_list].join(dummy_ranks_PD.ix[:,:]).join(dummy_ranks_DAY.ix[:,:]).join(address_features.ix[:,:]).join(T_D_M_Y_features.ix[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(keep='last')).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, Y, lookback, delay, min_index, max_index,shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(X) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           X.shape[-1]))\n",
    "        #targets = str(np.array((len(rows),)))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = X[indices]\n",
    "            targets[j] = Y[rows[j]+delay]\n",
    "        yield samples, targets\n",
    "    #Now here is the data generator that we will use. It yields a tuple (samples, targets) where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\n",
    "        # •data: The original array of floating point data, which we just normalized in the code snippet above.\n",
    "        # •lookback: How many timesteps back should our input data go.\n",
    "        # •delay: How many timesteps in the future should our target be.\n",
    "        # •min_index and max_index: Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another one for testing.\n",
    "        # •shuffle: Whether to shuffle our samples or draw them in chronological order.\n",
    "        # •batch_size: The number of samples per batch.\n",
    "        # •step: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logodds_A,logoddsPF_A=field2Vec(trainDF,\"Address\")\n",
    "print(\"TDMY and Address!\")\n",
    "trainDF[\"T_D_M_Y\"]=trainDF[\"Dates\"].apply(Dates2TDMY)\n",
    "trainDF[\"T_D_M_Y\"]=trainDF[\"T_D_M_Y\"]+trainDF[\"DayOfWeek\"]\n",
    "logodds_T,logoddsPF_T=field2Vec(trainDF,\"T_D_M_Y\")\n",
    "features, labels=parse_data(trainDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T)    \n",
    "print(features.columns.tolist())\n",
    "print(len(features.columns))\n",
    "\n",
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)\n",
    "##########################################################################################\n",
    "N_EPOCHS=2\n",
    "N_HN=256\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "DP=0.5\n",
    "N_CLASS=len(labels.unique())\n",
    "#############################先进行过采样，然后再根据时间来排序##################################\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "featuresArray, labelsArray = ros.fit_resample(features.values,labels.values)#####过采样#####\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "time_temp=featuresArray[:,2]+np.dot(featuresArray[:,3],100)+np.dot(featuresArray[:,4],10000)+np.dot(featuresArray[:,5],1000000)\n",
    "features_label_time=np.column_stack((featuresArray,labelsArray))\n",
    "features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "labelsArray=features_label_time[:,-2]\n",
    "featuresArray=features_label_time[:,0:featuresArray.shape[1]]\n",
    "del features_label_time\n",
    "#############################先进行过采样，然后再根据时间来排序----结束############################\n",
    "\n",
    "NotOnlyLSTM=False\n",
    "SORTbyTime=False #是否需要根据时间顺序，留出后Test_size个样本用于测试\n",
    "TestRate=0.2 #当SORTbyTime=False 时，该值才起作用\n",
    "Test_size=200000#当SORTbyTime=True 时，该值才起作用\n",
    "split_count=4#当SORTbyTime=True 时，该值才起作用\n",
    "split_size=int(Test_size/split_count)#当SORTbyTime=True 时，该值才起作用\n",
    "N_hight=featuresArray.shape[0]\n",
    "for t_i in range(split_count):\n",
    "    if SORTbyTime:\n",
    "        # t_i=t0_i+1\n",
    "        print('--------NNN_spllit_NNN_spllit_NNN_spllit_NNN_spllit_---------')\n",
    "        print(t_i)\n",
    "        x_train=featuresArray[0:N_hight-Test_size+t_i*split_size,:]\n",
    "        y_train=labelsArray[0:N_hight-Test_size+t_i*split_size]\n",
    "\n",
    "        x_test=featuresArray[N_hight-Test_size+t_i*split_size:N_hight-Test_size+(t_i+1)*split_size,:]\n",
    "        y_test=labelsArray[N_hight-Test_size+t_i*split_size:N_hight-Test_size+(t_i+1)*split_size]\n",
    "    else:\n",
    "        x_train,x_test,y_train,y_test = train_test_split(featuresArray,labelsArray,test_size=TestRate,shuffle=True)\n",
    "\n",
    "    y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "    y_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_test)), num_classes=N_CLASS)\n",
    "    \n",
    "    ##########################################################################\n",
    "    input_dim=x_train.shape[1]\n",
    "    output_dim=N_CLASS\n",
    "    model = Sequential()\n",
    "    model.add(Dense(N_HN,input_dim=input_dim,init='glorot_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    # model.add(Dropout(dp))\n",
    "    for i in range(N_LAYERS):\n",
    "        model.add(Dense(N_HN, init='glorot_uniform'))\n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(PReLU())    \n",
    "    #   model.add(Dropout(dp))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(output_dim, init='glorot_uniform'))\n",
    "    model.add(Activation('softmax'))\n",
    "    # model = multi_gpu_model(model, 2)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "    if NotOnlyLSTM:\n",
    "        print('------------Go! Go! Go!!!!-----------')\n",
    "        fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=2,validation_data=(x_test,y_test))\n",
    "        # acc_test, test_score,fitting, model = build_and_fit_model(features_train.values,labels_train,X_test=features_test.values,y_test=labels_test,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)\n",
    "        # model.save('jjs_model_0112.h5')\n",
    "        print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "        acc_test = model.evaluate(x_test,y_test, batch_size=N_BATCH)\n",
    "        print(acc_test)\n",
    "        if SORTbyTime:\n",
    "            del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresLSTM=features.values\n",
    "labelsLSTM = labels.values\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "time_temp=featuresLSTM[:,2]+np.dot(featuresLSTM[:,3],100)+np.dot(featuresLSTM[:,4],10000)+np.dot(featuresLSTM[:,5],1000000)\n",
    "features_label_time=np.column_stack((featuresLSTM,labelsLSTM))\n",
    "features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "labelsLSTM=features_label_time[:,-2]\n",
    "featuresLSTM=features_label_time[:,0:featuresLSTM.shape[1]]\n",
    "del features_label_time\n",
    "\n",
    "N_EPOCHS=20\n",
    "N_HN=256\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "lookback=1024\n",
    "input_dim=x_train.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "train_X, train_Y=generator(featuresLSTM, labelsLSTM, lookback=lookback, delay=1, min_index=0, max_index=136000, shuffle=True, batch_size=N_BATCH, step=1)\n",
    "test_X, test_Y=generator(featuresLSTM, labelsLSTM, lookback=lookback, delay=1, min_index=136001, max_index=None, shuffle=True, batch_size=N_BATCH, step=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback, input_dim)))\n",
    "model.add(layers.Dense(N_HN, activation='relu'))\n",
    "model.add(layers.Dense(output_dim))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_X,train_Y, epochs=N_EPOCHS,verbose=2,validation_data=(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
