{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras import layers,metrics\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils, multi_gpu_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.utils import shuffle as reset\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss,make_scorer\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "# import \n",
    "from matplotlib.pylab import plt\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_DataFrame(data, test_size=0.2, considerTime=True, random_state=None):\n",
    "    # ConsiderTime-------trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "    if considerTime:\n",
    "        data=data.sort_values(by=\"Dates\", ascending=True)\n",
    "    else:\n",
    "        data=reset(data, random_state=random_state)\n",
    "    train=data[int(len(data)*test_size):].reset_index(drop=True)\n",
    "    test=data[:int(len(data)*test_size)].reset_index(drop=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    if '-' in x:\n",
    "        DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")#jjs\n",
    "    else:\n",
    "        DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")#zj    \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "def Dates2TDMY(x):\n",
    "    if '-' in x:\n",
    "        DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")#jjs\n",
    "    else:\n",
    "        DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")#zj  \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    #T_D_M_Y=str(time)+str(day)+str(month)+str(year)\n",
    "    T_D_M_Y=str(time)+str(day)+str(month)\n",
    "    return T_D_M_Y\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field2Vec(trainDF,testDF,fieldStr):\n",
    "    fields=sorted(trainDF[fieldStr].unique())\n",
    "    categories=sorted(trainDF[\"Category\"].unique())\n",
    "    C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "    F_C_counts=trainDF.groupby([fieldStr,\"Category\"]).size()\n",
    "    F_counts=trainDF.groupby([fieldStr]).size()\n",
    "    logodds={}\n",
    "    logoddsPF={}\n",
    "    MIN_CAT_COUNTS=2\n",
    "    default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "    for f in fields:\n",
    "        PA=F_counts[f]/float(len(trainDF))\n",
    "        logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "        logodds[f]=deepcopy(default_logodds)\n",
    "        for cat in F_C_counts[f].keys():\n",
    "            if (F_C_counts[f][cat]>MIN_CAT_COUNTS) and F_C_counts[f][cat]<F_counts[f]:\n",
    "                PA=F_C_counts[f][cat]/float(F_counts[f])\n",
    "                logodds[f][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "        logodds[f]=pd.Series(logodds[f])\n",
    "        logodds[f].index=range(len(categories))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    fieldsTest=sorted(testDF[fieldStr].unique())\n",
    "    for f in fieldsTest:\n",
    "        if f not in fields:\n",
    "            logoddsPF[f]=-50.0  #np.log(0.)-np.log(1.)=-inf,便于计算，改为-99999.0\n",
    "            logodds[f]=deepcopy(default_logodds)\n",
    "            pa=1.0/float(len(categories))\n",
    "            logodds[f][range(len(categories))]=np.log(pa)-np.log(1.0-pa)\n",
    "            logodds[f]=pd.Series(logodds[f])\n",
    "            logodds[f].index=range(len(categories))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    return logodds,logoddsPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(df,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print(\"Creating address features\")###Creating address features###\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds_A[x])\n",
    "    address_features.columns=[\"logodds_A\"+str(x) for x in range(len(address_features.columns))]\n",
    "    print(\"Creating time T_D_M_Y features\")###Creating time T_D_M_Y features###\n",
    "    T_D_M_Y_features=cleanData[\"T_D_M_Y\"].apply(lambda xx: logodds_T[xx])\n",
    "    T_D_M_Y_features.columns=[\"logodds_T\"+str(xx) for xx in range(len(T_D_M_Y_features.columns))]\n",
    "\n",
    "    print(\"Parsing dates\")            ###Creating address features###\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "    #     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    #     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print(\"Creating one-hot variables\")\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPF_A\"]=cleanData[\"Address\"].apply(lambda x: logoddsPF_A[x])\n",
    "    cleanData[\"logoddsPF_T\"]=cleanData[\"T_D_M_Y\"].apply(lambda x: logoddsPF_T[x])\n",
    "    print(\"droping processed columns\")\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)\n",
    "    cleanData=cleanData.drop(\"T_D_M_Y\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:]).join(T_D_M_Y_features.iloc[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(keep='last')).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, Y, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(X) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, X.shape[-1]))\n",
    "        targets = np.zeros((len(rows),Y.shape[1]))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = X[indices]\n",
    "            targets[j] = Y[rows[j]+delay]\n",
    "        yield samples, targets\n",
    "    #Now here is the data generator that we will use. It yields a tuple (samples, targets) where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\n",
    "        # •data: The original array of floating point data, which we just normalized in the code snippet above.\n",
    "        # •lookback: How many timesteps back should our input data go.\n",
    "        # •delay: How many timesteps in the future should our target be.\n",
    "        # •min_index and max_index: Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another one for testing.\n",
    "        # •shuffle: Whether to shuffle our samples or draw them in chronological order.\n",
    "        # •batch_size: The number of samples per batch.\n",
    "        # •step: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 9)\n",
      "(790245, 9)\n",
      "(87804, 9)\n",
      "Address_counts_allDF_trainDF_testDF: 23228_23003_14995\n",
      "-----------LOGOODS: T_D_M_Y-------------\n",
      "-----------LOGOODS: Address-------------\n",
      "-----------LOGOODS: parse_data of Alltrain  -------------\n",
      "Creating address features\n",
      "Creating time T_D_M_Y features\n",
      "Parsing dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n",
      "-----------LOGOODS: parse_data of Alltest  -------------\n",
      "Creating address features\n",
      "Creating time T_D_M_Y features\n",
      "Parsing dates\n",
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n",
      "['X', 'Y', 'Time', 'Day', 'Month', 'Year', 'IsInterection', 'logoddsPF_A', 'logoddsPF_T', 'PD_BAYVIEW', 'PD_CENTRAL', 'PD_INGLESIDE', 'PD_MISSION', 'PD_NORTHERN', 'PD_PARK', 'PD_RICHMOND', 'PD_SOUTHERN', 'PD_TARAVAL', 'PD_TENDERLOIN', 'DAY_Friday', 'DAY_Monday', 'DAY_Saturday', 'DAY_Sunday', 'DAY_Thursday', 'DAY_Tuesday', 'DAY_Wednesday', 'logodds_A0', 'logodds_A1', 'logodds_A2', 'logodds_A3', 'logodds_A4', 'logodds_A5', 'logodds_A6', 'logodds_A7', 'logodds_A8', 'logodds_A9', 'logodds_A10', 'logodds_A11', 'logodds_A12', 'logodds_A13', 'logodds_A14', 'logodds_A15', 'logodds_A16', 'logodds_A17', 'logodds_A18', 'logodds_A19', 'logodds_A20', 'logodds_A21', 'logodds_A22', 'logodds_A23', 'logodds_A24', 'logodds_A25', 'logodds_A26', 'logodds_A27', 'logodds_A28', 'logodds_A29', 'logodds_A30', 'logodds_A31', 'logodds_A32', 'logodds_A33', 'logodds_A34', 'logodds_A35', 'logodds_A36', 'logodds_A37', 'logodds_A38', 'logodds_T0', 'logodds_T1', 'logodds_T2', 'logodds_T3', 'logodds_T4', 'logodds_T5', 'logodds_T6', 'logodds_T7', 'logodds_T8', 'logodds_T9', 'logodds_T10', 'logodds_T11', 'logodds_T12', 'logodds_T13', 'logodds_T14', 'logodds_T15', 'logodds_T16', 'logodds_T17', 'logodds_T18', 'logodds_T19', 'logodds_T20', 'logodds_T21', 'logodds_T22', 'logodds_T23', 'logodds_T24', 'logodds_T25', 'logodds_T26', 'logodds_T27', 'logodds_T28', 'logodds_T29', 'logodds_T30', 'logodds_T31', 'logodds_T32', 'logodds_T33', 'logodds_T34', 'logodds_T35', 'logodds_T36', 'logodds_T37', 'logodds_T38', 'IsDup', 'Awake', 'Summer', 'Fall', 'Winter', 'Spring']\n",
      "110\n",
      "------------RandomOverSampler--------------\n",
      "------------Sort--------------\n"
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "ConsiderTime=False# True##trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "Rate_ALL=0.1\n",
    "allDF=pd.read_csv(\"./train.csv\")\n",
    "print(allDF.shape)\n",
    "trainDF,testDF=train_test_split_DataFrame(allDF, test_size=Rate_ALL, considerTime=ConsiderTime, random_state=120)\n",
    "print(trainDF.shape)\n",
    "print(testDF.shape)\n",
    "print('Address_counts_allDF_trainDF_testDF: ' + str(len(allDF[\"Address\"].unique())) + '_'+ str(len(trainDF[\"Address\"].unique())) + '_' + str(len(testDF[\"Address\"].unique())))\n",
    "\n",
    "N_AllSample=allDF.shape[0]\n",
    "N_AllTrain=trainDF.shape[0]\n",
    "N_AllTest=testDF.shape[0]\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "#################Now proceed as before#################\n",
    "xy_scaler=preprocessing.StandardScaler()\n",
    "xy_scaler.fit(trainDF[[\"X\",\"Y\"]])\n",
    "trainDF[[\"X\",\"Y\"]]=xy_scaler.transform(trainDF[[\"X\",\"Y\"]])\n",
    "\n",
    "trainDF[\"T_D_M_Y\"]=trainDF[\"Dates\"].apply(Dates2TDMY)\n",
    "trainDF[\"T_D_M_Y\"]=trainDF[\"T_D_M_Y\"]+trainDF[\"DayOfWeek\"]\n",
    "\n",
    "testDF[[\"X\",\"Y\"]]=xy_scaler.transform(testDF[[\"X\",\"Y\"]])\n",
    "testDF[\"T_D_M_Y\"]=testDF[\"Dates\"].apply(Dates2TDMY)\n",
    "testDF[\"T_D_M_Y\"]=testDF[\"T_D_M_Y\"]+testDF[\"DayOfWeek\"]\n",
    "\n",
    "print('-----------LOGOODS: T_D_M_Y-------------')\n",
    "logodds_T,logoddsPF_T=field2Vec(trainDF,testDF,\"T_D_M_Y\")\n",
    "print('-----------LOGOODS: Address-------------')\n",
    "logodds_A,logoddsPF_A=field2Vec(trainDF,testDF,\"Address\")\n",
    "print('-----------LOGOODS: parse_data of Alltrain  -------------')\n",
    "features, labels=parse_data(trainDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T) \n",
    "print('-----------LOGOODS: parse_data of Alltest  -------------')\n",
    "features_test, labels_test=parse_data(testDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T)###########和训练集使用同样的时间和地点Logoodds值#####\n",
    "x_test=features_test.values\n",
    "y_test=labels_test.values\n",
    "y_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_test)), num_classes=N_CLASS)\n",
    "\n",
    "print(features.columns.tolist())\n",
    "print(len(features.columns))\n",
    "\n",
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)\n",
    "features_test[collist]=scaler.transform(features_test)###########和训练集使用同样的scaler值#####\n",
    "######################################################\n",
    "#############################先进行过采样，然后再根据时间来排序##################################\n",
    "print('------------RandomOverSampler--------------')\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "featuresArray, labelsArray = ros.fit_resample(features.values,labels.values)#####过采样#####\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "print('------------Sort--------------')\n",
    "time_temp=featuresArray[:,2]+np.dot(featuresArray[:,3],100)+np.dot(featuresArray[:,4],10000)+np.dot(featuresArray[:,5],1000000)\n",
    "features_label_time=np.column_stack((featuresArray,labelsArray))\n",
    "features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "labelsArray=features_label_time[:,-2]\n",
    "featuresArray=features_label_time[:,0:featuresArray.shape[1]]\n",
    "del features_label_time\n",
    "#############################先进行过采样，然后再根据时间来排序----结束############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, input_dim=110, kernel_initializer=\"glorot_uniform\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_initializer=\"glorot_uniform\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(39, kernel_initializer=\"glorot_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 5984412 samples, validate on 158049 samples\n",
      "Epoch 1/21\n",
      "5984412/5984412 [==============================] - 1177s 197us/step - loss: 1.4770 - accuracy: 0.5874 - top_k_categorical_accuracy: 0.8089 - val_loss: 1.2093 - val_accuracy: 0.6565 - val_top_k_categorical_accuracy: 0.8572\n",
      "Epoch 2/21\n",
      "5984412/5984412 [==============================] - 1135s 190us/step - loss: 1.1818 - accuracy: 0.6616 - top_k_categorical_accuracy: 0.8652 - val_loss: 1.0900 - val_accuracy: 0.6845 - val_top_k_categorical_accuracy: 0.8782\n",
      "Epoch 3/21\n",
      "5984412/5984412 [==============================] - 1106s 185us/step - loss: 1.1075 - accuracy: 0.6807 - top_k_categorical_accuracy: 0.8789 - val_loss: 1.0357 - val_accuracy: 0.7011 - val_top_k_categorical_accuracy: 0.8877\n",
      "Epoch 4/21\n",
      "5984412/5984412 [==============================] - 1120s 187us/step - loss: 1.0655 - accuracy: 0.6915 - top_k_categorical_accuracy: 0.8864 - val_loss: 1.0047 - val_accuracy: 0.7101 - val_top_k_categorical_accuracy: 0.8930\n",
      "Epoch 5/21\n",
      "5984412/5984412 [==============================] - 1123s 188us/step - loss: 1.0385 - accuracy: 0.6983 - top_k_categorical_accuracy: 0.8913 - val_loss: 0.9743 - val_accuracy: 0.7157 - val_top_k_categorical_accuracy: 0.8975\n",
      "Epoch 6/21\n",
      "5984412/5984412 [==============================] - 1116s 186us/step - loss: 1.0189 - accuracy: 0.7034 - top_k_categorical_accuracy: 0.8947 - val_loss: 0.9506 - val_accuracy: 0.7227 - val_top_k_categorical_accuracy: 0.9015\n",
      "Epoch 7/21\n",
      "5984412/5984412 [==============================] - 1118s 187us/step - loss: 1.0041 - accuracy: 0.7069 - top_k_categorical_accuracy: 0.8973 - val_loss: 0.9373 - val_accuracy: 0.7244 - val_top_k_categorical_accuracy: 0.9030\n",
      "Epoch 8/21\n",
      "5984412/5984412 [==============================] - 1115s 186us/step - loss: 0.9923 - accuracy: 0.7100 - top_k_categorical_accuracy: 0.8993 - val_loss: 0.9267 - val_accuracy: 0.7280 - val_top_k_categorical_accuracy: 0.9052\n",
      "Epoch 9/21\n",
      "5984412/5984412 [==============================] - 1115s 186us/step - loss: 0.9822 - accuracy: 0.7125 - top_k_categorical_accuracy: 0.9011 - val_loss: 0.9181 - val_accuracy: 0.7311 - val_top_k_categorical_accuracy: 0.9063\n",
      "Epoch 10/21\n",
      "5984412/5984412 [==============================] - 1111s 186us/step - loss: 0.9739 - accuracy: 0.7146 - top_k_categorical_accuracy: 0.9027 - val_loss: 0.9077 - val_accuracy: 0.7328 - val_top_k_categorical_accuracy: 0.9079\n",
      "Epoch 11/21\n",
      "5984412/5984412 [==============================] - 1110s 186us/step - loss: 0.9670 - accuracy: 0.7165 - top_k_categorical_accuracy: 0.9038 - val_loss: 0.8983 - val_accuracy: 0.7344 - val_top_k_categorical_accuracy: 0.9094\n",
      "Epoch 12/21\n",
      "5984412/5984412 [==============================] - 1109s 185us/step - loss: 0.9608 - accuracy: 0.7179 - top_k_categorical_accuracy: 0.9048 - val_loss: 0.8984 - val_accuracy: 0.7350 - val_top_k_categorical_accuracy: 0.9095\n",
      "Epoch 13/21\n",
      "5984412/5984412 [==============================] - 1101s 184us/step - loss: 0.9554 - accuracy: 0.7192 - top_k_categorical_accuracy: 0.9058 - val_loss: 0.8920 - val_accuracy: 0.7366 - val_top_k_categorical_accuracy: 0.9099\n",
      "Epoch 14/21\n",
      "5984412/5984412 [==============================] - 1108s 185us/step - loss: 0.9507 - accuracy: 0.7205 - top_k_categorical_accuracy: 0.9065 - val_loss: 0.8831 - val_accuracy: 0.7386 - val_top_k_categorical_accuracy: 0.9118\n",
      "Epoch 15/21\n",
      "5984412/5984412 [==============================] - 1113s 186us/step - loss: 0.9464 - accuracy: 0.7215 - top_k_categorical_accuracy: 0.9074 - val_loss: 0.8820 - val_accuracy: 0.7399 - val_top_k_categorical_accuracy: 0.9112\n",
      "Epoch 16/21\n",
      "5984412/5984412 [==============================] - 1136s 190us/step - loss: 0.9424 - accuracy: 0.7226 - top_k_categorical_accuracy: 0.9080 - val_loss: 0.8765 - val_accuracy: 0.7407 - val_top_k_categorical_accuracy: 0.9121\n",
      "Epoch 17/21\n",
      "5984412/5984412 [==============================] - 1133s 189us/step - loss: 0.9391 - accuracy: 0.7234 - top_k_categorical_accuracy: 0.9085 - val_loss: 0.8726 - val_accuracy: 0.7417 - val_top_k_categorical_accuracy: 0.9131\n",
      "Epoch 18/21\n",
      "5984412/5984412 [==============================] - 1111s 186us/step - loss: 0.9360 - accuracy: 0.7242 - top_k_categorical_accuracy: 0.9091 - val_loss: 0.8719 - val_accuracy: 0.7425 - val_top_k_categorical_accuracy: 0.9134\n",
      "Epoch 19/21\n",
      "5984412/5984412 [==============================] - 1114s 186us/step - loss: 0.9328 - accuracy: 0.7251 - top_k_categorical_accuracy: 0.9096 - val_loss: 0.8697 - val_accuracy: 0.7419 - val_top_k_categorical_accuracy: 0.9134\n",
      "Epoch 20/21\n",
      "5984412/5984412 [==============================] - 1106s 185us/step - loss: 0.9301 - accuracy: 0.7256 - top_k_categorical_accuracy: 0.9102 - val_loss: 0.8667 - val_accuracy: 0.7432 - val_top_k_categorical_accuracy: 0.9138\n",
      "Epoch 21/21\n",
      "5984412/5984412 [==============================] - 1107s 185us/step - loss: 0.9280 - accuracy: 0.7263 - top_k_categorical_accuracy: 0.9105 - val_loss: 0.8632 - val_accuracy: 0.7443 - val_top_k_categorical_accuracy: 0.9149\n"
     ]
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS=21\n",
    "N_HN_1=512\n",
    "N_HN=256\n",
    "N_LAYERS=2\n",
    "N_BATCH=128\n",
    "lookback=1024\n",
    "Rate_Val=0.2\n",
    "N_Val=int(np.around(N_AllTrain*Rate_Val))\n",
    "N_Train=int(N_AllTrain-N_Val)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArray.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim,init='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "# model.add(Dropout(dp))\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN, init='glorot_uniform'))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(PReLU())    \n",
    "#   model.add(Dropout(dp))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim, init='glorot_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "# model = multi_gpu_model(model, 2)gfyht76\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(featuresArray,labelsArray,test_size=N_Val,shuffle=True)\n",
    "# x_train=featuresArray[0:N_Train,:]\n",
    "# y_train=labelsArray[0:N_Train]\n",
    "y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "# x_val=featuresArray[N_Train:N_AllTrain,:] \n",
    "# y_val=labelsArray[N_Train:N_AllTrain]\n",
    "y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)    \n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=1,validation_data=(x_val,y_val))\n",
    "# acc_test, test_score,fitting, model = build_and_fit_model(features_train.values,labels_train,x_val=features_test.values,y_test=labels_test,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)\n",
    "# model.save('jjs_model_0112.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------DNN Evaluate-------------------\n",
      "87804/87804 [==============================] - 5s 58us/step\n",
      "[1183.449230917548, 0.10530272126197815, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print('-----------DNN Evaluate-------------------')\n",
    "acc_test = model.evaluate(x_test,y_test, batch_size=N_BATCH)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('jjs_model_0124.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------Sort feature and label for LSTM------------------------------------\n",
      "--------------------------generator AllTrain_set, Train_set and Val_set for LSTM---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------Sort feature and label for LSTM------------------------------------')\n",
    "featuresLSTM=features.values\n",
    "labelsLSTM = labels.values\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS=3\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=128\n",
    "lookback=1024\n",
    "Rate_Val=0.2\n",
    "N_Val=np.around(N_AllTrain*Rate_Val)\n",
    "N_Train=N_AllTrain-N_Val\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresLSTM.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "time_temp=featuresLSTM[:,2]+np.dot(featuresLSTM[:,3],100)+np.dot(featuresLSTM[:,4],10000)+np.dot(featuresLSTM[:,5],1000000)\n",
    "features_label_time=np.column_stack((featuresLSTM,labelsLSTM))\n",
    "features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "featuresLSTM=features_label_time[:,0:featuresLSTM.shape[1]]\n",
    "labelsLSTM=features_label_time[:,-2]\n",
    "labelsLSTM = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsLSTM)), num_classes=N_CLASS)\n",
    "del features_label_time\n",
    "\n",
    "print('--------------------------generator AllTrain_set, Train_set and Val_set for LSTM---------------------------------')\n",
    "train_generator=generator(featuresLSTM, labelsLSTM, lookback=lookback, delay=1, min_index=0, max_index=N_Train, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "val_generator=generator(featuresLSTM, labelsLSTM, lookback=lookback, delay=1, min_index=N_Train+1, max_index=None, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "AllTrain_generator=generator(featuresLSTM, labelsLSTM, lookback=lookback, delay=1, min_index=0, max_index=None, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "AllTest_generator=generator(x_test, y_test, lookback=lookback, delay=1, min_index=0, max_index=None, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "val_steps = (N_Val-lookback) // N_BATCH\n",
    "test_steps =(N_AllTest - lookback) // N_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMmodel = Sequential()\n",
    "LSTMmodel.add(layers.Flatten(input_shape=(lookback, input_dim)))\n",
    "LSTMmodel.add(layers.Dense(N_HN, activation='relu'))\n",
    "LSTMmodel.add(layers.Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "LSTMmodel.compile(optimizer=RMSprop(), loss='categorical_crossentropy',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "# LSTMmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy', metrics.top_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------LSTM GO GO GO!!!!---------------------------------------------\n",
      "Epoch 1/3\n",
      "50/50 [==============================] - 797s 16s/step - loss: 8.1904 - accuracy: 0.0864 - top_k_categorical_accuracy: 0.3411 - val_loss: 7.9591 - val_accuracy: 0.1394 - val_top_k_categorical_accuracy: 0.4280\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 784s 16s/step - loss: 8.0796 - accuracy: 0.1109 - top_k_categorical_accuracy: 0.3505 - val_loss: 7.2991 - val_accuracy: 0.0774 - val_top_k_categorical_accuracy: 0.3846\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 788s 16s/step - loss: 8.1813 - accuracy: 0.1255 - top_k_categorical_accuracy: 0.3256 - val_loss: 8.9437 - val_accuracy: 0.1139 - val_top_k_categorical_accuracy: 0.3852\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------LSTM GO GO GO!!!!---------------------------------------------')\n",
    "history = LSTMmodel.fit_generator(train_generator,steps_per_epoch=50,epochs=N_EPOCHS,verbose=1,validation_data=val_generator,validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------LSTM ReTraining On AllTrainSet (include trainSet and valSet) !!!!---------------------------------\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 820s 16s/step - loss: 8.1710 - accuracy: 0.1575 - top_k_categorical_accuracy: 0.3841 - val_loss: 7.1834 - val_accuracy: 0.1535 - val_top_k_categorical_accuracy: 0.4441\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------LSTM ReTraining On AllTrainSet (include trainSet and valSet) !!!!---------------------------------')\n",
    "historyAll = LSTMmodel.fit_generator(AllTrain_generator,steps_per_epoch=50, epochs=1,verbose=1,validation_data=val_generator,validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9226/10000 [==========================>...] - ETA: 2:14"
     ]
    }
   ],
   "source": [
    "acc_test = LSTMmodel.evaluate_generator(AllTest_generator, steps=10000,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------LSTM Evaluate on AllTestSet-------------------\n",
      "87804/87804 [==============================] - 19971s 227ms/step\n",
      "[9.870048522949219, 0.00019201147370040417, 0.28379303216934204]\n"
     ]
    }
   ],
   "source": [
    "print('-----------LSTM Evaluate on AllTestSet-------------------')\n",
    "####\n",
    "#需要将train_X_ALL和test_X拼接在一起，先计算一下train_X_ALL的行数，然后，根据lookback来确定test_X从哪一行开始\n",
    "####\n",
    "#evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "acc_test = LSTMmodel.evaluate_generator(AllTest_generator, steps=N_AllTest,  verbose=1)\n",
    "# acc_test = LSTMmodel.evaluate(test_X,test_Y, batch_size=N_BATCH)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Begin LSTM--------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7acc29103aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfeatures_label_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesLSTM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabelsLSTM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfeatures_label_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_label_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfeatures_label_time\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfeatures_label_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_label_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfeaturesLSTM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_label_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfeaturesLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlabelsLSTM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_label_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "OnlyLSTM=True#False\n",
    "if OnlyLSTM:\n",
    "    print('------------Begin LSTM--------------')\n",
    "    featuresLSTM=features.values\n",
    "    labelsLSTM = labels.values\n",
    "        #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "    time_temp=featuresLSTM[:,2]+np.dot(featuresLSTM[:,3],100)+np.dot(featuresLSTM[:,4],10000)+np.dot(featuresLSTM[:,5],1000000)\n",
    "    features_label_time=np.column_stack((featuresLSTM,labelsLSTM))\n",
    "    features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "    features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "    featuresLSTM=features_label_time[:,0:featuresLSTM.shape[1]]\n",
    "    labelsLSTM=features_label_time[:,-2]\n",
    "    labelsLSTM = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsLSTM)), num_classes=N_CLASS)\n",
    "    del features_label_time\n",
    "\n",
    "    N_EPOCHS=40\n",
    "    N_HN=128\n",
    "    N_LAYERS=1\n",
    "    N_BATCH=64\n",
    "    lookback=4056\n",
    "    TestRate=0.2\n",
    "    size_Train=trainDF.shape[0]*TestRate\n",
    "    input_dim=featuresLSTM.shape[1]\n",
    "    output_dim=N_CLASS\n",
    "    \n",
    "    print('--------------------------generator Train_set and Val_set for LSTM---------------------------------')\n",
    "    train_X, train_Y=generator(featuresLSTM, labelsLSTM, lookback=lookback, delay=1, min_index=0, max_index=size_Train, shuffle=True, batch_size=N_BATCH, step=1)\n",
    "    val_X, val_Y=generator(featuresLSTM, labelsLSTM, lookback=lookback, delay=1, min_index=size_Train+1, max_index=None, shuffle=True, batch_size=N_BATCH, step=1)\n",
    " \n",
    "    LSTMmodel = Sequential()\n",
    "    LSTMmodel.add(layers.Flatten(input_shape=(lookback, input_dim)))\n",
    "    LSTMmodel.add(layers.Dense(N_HN, activation='relu'))\n",
    "    LSTMmodel.add(layers.Dense(output_dim))\n",
    "    LSTMmodel.compile(optimizer=RMSprop(), loss='categorical_crossentropy',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "    # LSTMmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "    print('---------------------------------------LSTM GO GO GO!!!!---------------------------------------------')\n",
    "    history = LSTMmodel.fit(train_X,train_Y, epochs=N_EPOCHS,verbose=1,validation_data=(val_X, val_Y))\n",
    "\n",
    "    print('--------------------------LSTM ReTraining On trainSet and valSet!!!!---------------------------------')\n",
    "    train_X_ALL, train_Y_ALL=generator(featuresLSTM, labelsLSTM, lookback=lookback, delay=1, min_index=0, max_index=None, shuffle=True, batch_size=N_BATCH, step=1)\n",
    "    historyAll = LSTMmodel.fit(train_X_ALL, train_Y_ALL, epochs=N_EPOCHS,verbose=1,validation_data=(val_X, val_Y))\n",
    "\n",
    "    print('-----------LSTM Evaluate ALL-------------------')\n",
    "    ####\n",
    "\n",
    "    #需要将train_X_ALL和test_X拼接在一起，先计算一下train_X_ALL的行数，然后，根据lookback来确定test_X从哪一行开始\n",
    "\n",
    "    ####\n",
    "    test_X, test_Y=generator(x_test, y_test, lookback=lookback, delay=1, min_index=0, max_index=None, shuffle=True, batch_size=N_BATCH, step=1)\n",
    "    acc_test = LSTMmodel.evaluate(test_X,test_Y, batch_size=N_BATCH)\n",
    "    print(acc_test)\n",
    "else:   \n",
    "    N_EPOCHS=21\n",
    "    N_HN=256\n",
    "    N_HN_1=512\n",
    "    N_LAYERS=2\n",
    "    N_BATCH=64\n",
    "    DP=0.5\n",
    "    SORTbyTime=False #是否需要根据时间顺序，留出后Test_size个样本用于测试\n",
    "    TestRate=0.2 #当SORTbyTime=False 时，该值才起作用\n",
    "    Test_size=200000#当SORTbyTime=True 时，该值才起作用\n",
    "    split_count=1#当SORTbyTime=True 时，该值才起作用\n",
    "    split_size=int(Test_size/split_count)#当SORTbyTime=True 时，该值才起作用\n",
    "    N_hight=featuresArray.shape[0]\n",
    "    print('------------train_val_split--------------')\n",
    "    for t_i in range(split_count):\n",
    "        if SORTbyTime:\n",
    "            # t_i=t0_i+1\n",
    "            print('--------NNN_spllit_NNN_spllit_NNN_spllit_NNN_spllit_---------')\n",
    "            print(t_i)\n",
    "            x_train=featuresArray[0:N_hight-Test_size+t_i*split_size,:]\n",
    "            y_train=labelsArray[0:N_hight-Test_size+t_i*split_size]\n",
    "\n",
    "            x_val=featuresArray[N_hight-Test_size+t_i*split_size:N_hight-Test_size+(t_i+1)*split_size,:]\n",
    "            y_val=labelsArray[N_hight-Test_size+t_i*split_size:N_hight-Test_size+(t_i+1)*split_size]\n",
    "        else:\n",
    "            print('------------train_val_split_Shuffle--------------')\n",
    "            x_train,x_val,y_train,y_val = train_test_split(featuresArray,labelsArray,test_size=TestRate,shuffle=True)\n",
    "        print('------------to_categorical--------------')\n",
    "        y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "        y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "\n",
    "\n",
    "        ##########################################################################\n",
    "        print('------------Building model--------------')\n",
    "        input_dim=x_train.shape[1]\n",
    "        output_dim=N_CLASS\n",
    "        model = Sequential()\n",
    "        model.add(Dense(N_HN_1,input_dim=input_dim,init='glorot_uniform'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(PReLU())\n",
    "        # model.add(Dropout(dp))\n",
    "        for i in range(N_LAYERS):\n",
    "            model.add(Dense(N_HN, init='glorot_uniform'))\n",
    "            model.add(BatchNormalization())    \n",
    "            model.add(PReLU())    \n",
    "        #   model.add(Dropout(dp))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(output_dim, init='glorot_uniform'))\n",
    "        model.add(Activation('softmax'))\n",
    "        # model = multi_gpu_model(model, 2)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "        if OnlyLSTM:\n",
    "            print('------------Go! Go! Go!!!!-----------')\n",
    "            fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=1,validation_data=(x_val,y_val))\n",
    "            # acc_test, test_score,fitting, model = build_and_fit_model(features_train.values,labels_train,x_val=features_test.values,y_test=labels_test,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)\n",
    "            # model.save('jjs_model_0112.h5')\n",
    "            print('-----------Evaluate-------------------')\n",
    "            acc_test = model.evaluate(x_test,y_test, batch_size=N_BATCH)\n",
    "            print(acc_test)\n",
    "            if SORTbyTime:\n",
    "                del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    " \n",
    "K.clear_session()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
