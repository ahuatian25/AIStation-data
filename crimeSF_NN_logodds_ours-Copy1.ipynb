{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from copy import deepcopy\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "\n",
    "trainDF=pd.read_csv(\"./train.csv\")\n",
    "# trainDF=trainDF[1:50000]\n",
    "#Clean up wrong X and Y values (very few of them)\n",
    "xy_scaler=preprocessing.StandardScaler()\n",
    "xy_scaler.fit(trainDF[[\"X\",\"Y\"]])\n",
    "trainDF[[\"X\",\"Y\"]]=xy_scaler.transform(trainDF[[\"X\",\"Y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(df,logodds,logoddsPA):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print(\"Creating address features\")###Creating address features###\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds[x])\n",
    "    address_features.columns=[\"logodds\"+str(x) for x in range(len(address_features.columns))]\n",
    "    print(\"Parsing dates\")            ###Creating address features###\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "    #     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    #     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print(\"Creating one-hot variables\")\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPA\"]=cleanData[\"Address\"].apply(lambda x: logoddsPA[x])\n",
    "    print(\"droping processed columns\")\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    features = cleanData[feature_list].join(dummy_ranks_PD.ix[:,:]).join(dummy_ranks_DAY.ix[:,:]).join(address_features.ix[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(keep='last')).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "    #label_names=labels.unique()\n",
    "    #labels=labels.cat.rename_categories(range(len(label_names)))\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels\n",
    "    #This part is slower than it needs to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating address features\n",
      "Parsing dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new features\n"
     ]
    }
   ],
   "source": [
    "addresses=sorted(trainDF[\"Address\"].unique())\n",
    "categories=sorted(trainDF[\"Category\"].unique())\n",
    "C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "A_C_counts=trainDF.groupby([\"Address\",\"Category\"]).size()\n",
    "A_counts=trainDF.groupby([\"Address\"]).size()\n",
    "logodds={}\n",
    "logoddsPA={}\n",
    "MIN_CAT_COUNTS=2\n",
    "default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "for addr in addresses:\n",
    "    PA=A_counts[addr]/float(len(trainDF))\n",
    "    logoddsPA[addr]=np.log(PA)-np.log(1.-PA)\n",
    "    logodds[addr]=deepcopy(default_logodds)\n",
    "    for cat in A_C_counts[addr].keys():\n",
    "        if (A_C_counts[addr][cat]>MIN_CAT_COUNTS) and A_C_counts[addr][cat]<A_counts[addr]:\n",
    "            PA=A_C_counts[addr][cat]/float(A_counts[addr])\n",
    "            logodds[addr][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "    logodds[addr]=pd.Series(logodds[addr])\n",
    "    logodds[addr].index=range(len(categories))\n",
    "    ###############################################################\n",
    "features, labels=parse_data(trainDF,logodds,logoddsPA)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'Time', 'Day', 'Month', 'Year', 'IsInterection', 'logoddsPA', 'PD_BAYVIEW', 'PD_CENTRAL', 'PD_INGLESIDE', 'PD_MISSION', 'PD_NORTHERN', 'PD_PARK', 'PD_RICHMOND', 'PD_SOUTHERN', 'PD_TARAVAL', 'PD_TENDERLOIN', 'DAY_Friday', 'DAY_Monday', 'DAY_Saturday', 'DAY_Sunday', 'DAY_Thursday', 'DAY_Tuesday', 'DAY_Wednesday', 'logodds0', 'logodds1', 'logodds2', 'logodds3', 'logodds4', 'logodds5', 'logodds6', 'logodds7', 'logodds8', 'logodds9', 'logodds10', 'logodds11', 'logodds12', 'logodds13', 'logodds14', 'logodds15', 'logodds16', 'logodds17', 'logodds18', 'logodds19', 'logodds20', 'logodds21', 'logodds22', 'logodds23', 'logodds24', 'logodds25', 'logodds26', 'logodds27', 'logodds28', 'logodds29', 'logodds30', 'logodds31', 'logodds32', 'logodds33', 'logodds34', 'logodds35', 'logodds36', 'logodds37', 'logodds38', 'IsDup', 'Awake', 'Summer', 'Fall', 'Winter', 'Spring']\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(features.columns.tolist())\n",
    "print(len(features.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS=21\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "DP=0.5\n",
    "N_CLASS=len(labels.unique())\n",
    "#########################################################################################\n",
    "###########################################################################\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "featuresArray, labelsArray = ros.fit_resample(features.values,labels.values)\n",
    "x_train,x_test,y_train,y_test = train_test_split(featuresArray,labelsArray,test_size=0.2)\n",
    "# y_train=np_utils.to_categorical(y_train.cat.rename_categories(range(N_CLASS)))\n",
    "# y_test=np_utils.to_categorical(y_test.cat.rename_categories(range(N_CLASS)))\n",
    "\n",
    "y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "y_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_test)), num_classes=N_CLASS)\n",
    "##########################################################################\n",
    "input_dim=x_train.shape[1]\n",
    "output_dim=N_CLASS\n",
    "# Y_train=np_utils.to_categorical(y_train.cat.rename_categories(range(len(y_train.unique()))))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "# model.add(Dropout(dp))\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(PReLU())    \n",
    "#   model.add(Dropout(dp))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "#model = multi_gpu_model(model, 2)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go Go Go!\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 5456880 samples, validate on 1364220 samples\n",
      "Epoch 1/21\n",
      " - 940s - loss: 2.1606 - accuracy: 0.4083 - top_k_categorical_accuracy: 0.6675 - val_loss: 1.9744 - val_accuracy: 0.4505 - val_top_k_categorical_accuracy: 0.7001\n",
      "Epoch 2/21\n",
      " - 917s - loss: 2.0215 - accuracy: 0.4409 - top_k_categorical_accuracy: 0.6960 - val_loss: 1.9303 - val_accuracy: 0.4617 - val_top_k_categorical_accuracy: 0.7098\n",
      "Epoch 3/21\n",
      " - 926s - loss: 1.9917 - accuracy: 0.4482 - top_k_categorical_accuracy: 0.7023 - val_loss: 1.9077 - val_accuracy: 0.4666 - val_top_k_categorical_accuracy: 0.7154\n",
      "Epoch 4/21\n",
      " - 906s - loss: 1.9753 - accuracy: 0.4522 - top_k_categorical_accuracy: 0.7059 - val_loss: 1.8953 - val_accuracy: 0.4715 - val_top_k_categorical_accuracy: 0.7170\n",
      "Epoch 5/21\n",
      " - 906s - loss: 1.9646 - accuracy: 0.4547 - top_k_categorical_accuracy: 0.7081 - val_loss: 1.8854 - val_accuracy: 0.4731 - val_top_k_categorical_accuracy: 0.7188\n",
      "Epoch 6/21\n",
      " - 892s - loss: 1.9577 - accuracy: 0.4563 - top_k_categorical_accuracy: 0.7095 - val_loss: 1.8797 - val_accuracy: 0.4763 - val_top_k_categorical_accuracy: 0.7196\n",
      "Epoch 7/21\n",
      " - 891s - loss: 1.9518 - accuracy: 0.4581 - top_k_categorical_accuracy: 0.7109 - val_loss: 1.8706 - val_accuracy: 0.4773 - val_top_k_categorical_accuracy: 0.7218\n",
      "Epoch 8/21\n",
      " - 890s - loss: 1.9472 - accuracy: 0.4593 - top_k_categorical_accuracy: 0.7116 - val_loss: 1.8681 - val_accuracy: 0.4781 - val_top_k_categorical_accuracy: 0.7221\n",
      "Epoch 9/21\n",
      " - 892s - loss: 1.9437 - accuracy: 0.4600 - top_k_categorical_accuracy: 0.7125 - val_loss: 1.8587 - val_accuracy: 0.4805 - val_top_k_categorical_accuracy: 0.7241\n",
      "Epoch 10/21\n",
      " - 880s - loss: 1.9407 - accuracy: 0.4610 - top_k_categorical_accuracy: 0.7131 - val_loss: 1.8606 - val_accuracy: 0.4797 - val_top_k_categorical_accuracy: 0.7249\n",
      "Epoch 11/21\n",
      " - 875s - loss: 1.9385 - accuracy: 0.4616 - top_k_categorical_accuracy: 0.7136 - val_loss: 1.8572 - val_accuracy: 0.4808 - val_top_k_categorical_accuracy: 0.7245\n",
      "Epoch 12/21\n",
      " - 872s - loss: 1.9357 - accuracy: 0.4624 - top_k_categorical_accuracy: 0.7142 - val_loss: 1.8523 - val_accuracy: 0.4828 - val_top_k_categorical_accuracy: 0.7254\n",
      "Epoch 13/21\n",
      " - 872s - loss: 1.9337 - accuracy: 0.4629 - top_k_categorical_accuracy: 0.7144 - val_loss: 1.8505 - val_accuracy: 0.4827 - val_top_k_categorical_accuracy: 0.7259\n",
      "Epoch 14/21\n",
      " - 872s - loss: 1.9318 - accuracy: 0.4634 - top_k_categorical_accuracy: 0.7150 - val_loss: 1.8495 - val_accuracy: 0.4827 - val_top_k_categorical_accuracy: 0.7260\n",
      "Epoch 15/21\n",
      " - 874s - loss: 1.9301 - accuracy: 0.4636 - top_k_categorical_accuracy: 0.7154 - val_loss: 1.8441 - val_accuracy: 0.4838 - val_top_k_categorical_accuracy: 0.7265\n",
      "Epoch 16/21\n",
      " - 885s - loss: 1.9286 - accuracy: 0.4642 - top_k_categorical_accuracy: 0.7157 - val_loss: 1.8415 - val_accuracy: 0.4850 - val_top_k_categorical_accuracy: 0.7271\n",
      "Epoch 17/21\n",
      " - 892s - loss: 1.9269 - accuracy: 0.4644 - top_k_categorical_accuracy: 0.7159 - val_loss: 1.8399 - val_accuracy: 0.4853 - val_top_k_categorical_accuracy: 0.7271\n",
      "Epoch 18/21\n",
      " - 892s - loss: 1.9261 - accuracy: 0.4648 - top_k_categorical_accuracy: 0.7163 - val_loss: 1.8445 - val_accuracy: 0.4840 - val_top_k_categorical_accuracy: 0.7272\n",
      "Epoch 19/21\n",
      " - 891s - loss: 1.9252 - accuracy: 0.4649 - top_k_categorical_accuracy: 0.7166 - val_loss: 1.8420 - val_accuracy: 0.4853 - val_top_k_categorical_accuracy: 0.7278\n",
      "Epoch 20/21\n",
      " - 882s - loss: 1.9235 - accuracy: 0.4655 - top_k_categorical_accuracy: 0.7167 - val_loss: 1.8413 - val_accuracy: 0.4845 - val_top_k_categorical_accuracy: 0.7279\n",
      "Epoch 21/21\n",
      " - 888s - loss: 1.9224 - accuracy: 0.4656 - top_k_categorical_accuracy: 0.7171 - val_loss: 1.8425 - val_accuracy: 0.4855 - val_top_k_categorical_accuracy: 0.7273\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "1364220/1364220 [==============================] - 63s 46us/step\n",
      "[1.8425027325831755, 0.485490620136261, 0.7273460030555725]\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "print('Go Go Go!')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=2,validation_data=(x_test,y_test))\n",
    "# acc_test, test_score,fitting, model = build_and_fit_model(features_train.values,labels_train,X_test=features_test.values,y_test=labels_test,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)\n",
    "model.save('jjs_model_0112.h5')\n",
    "\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "acc_test = model.evaluate(x_test,y_test, batch_size=N_BATCH)\n",
    "print(acc_test)\n",
    "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
