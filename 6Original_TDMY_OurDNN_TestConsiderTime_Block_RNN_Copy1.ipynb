{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验R2-R3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras import layers,metrics\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils, multi_gpu_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.utils import shuffle as reset\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss,make_scorer\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "# import \n",
    "from matplotlib.pylab import plt\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler #https://imbalanced-learn.org/stable/generated/imblearn.over_sampling.RandomOverSampler.html?highlight=randomoversampler\n",
    "from frplayer import FilterResponseNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_DataFrame(data, test_size=0.2, considerTime=True, random_state=None):\n",
    "    # ConsiderTime-------trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "    if considerTime:\n",
    "        data=data.sort_values(by=\"Dates\", ascending=True)\n",
    "    else:\n",
    "        data=reset(data, random_state=random_state)\n",
    "    train=data[int(len(data)*test_size):].reset_index(drop=True)\n",
    "    test=data[:int(len(data)*test_size)].reset_index(drop=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    if '-' in x:\n",
    "        DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")#jjs\n",
    "    else:\n",
    "        DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")#zj    \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "def Dates2TDMY(x):\n",
    "    if '-' in x:\n",
    "        DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")#jjs\n",
    "    else:\n",
    "        DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")#zj  \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    #T_D_M_Y=str(time)+str(day)+str(month)+str(year)\n",
    "    T_D_M_Y=str(time)+str(day)+str(month)\n",
    "    return T_D_M_Y\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field2Vec(trainDF,testDF,fieldStr):\n",
    "    fields=sorted(trainDF[fieldStr].unique())\n",
    "    categories=sorted(trainDF[\"Category\"].unique())\n",
    "    C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "    F_C_counts=trainDF.groupby([fieldStr,\"Category\"]).size()\n",
    "    F_counts=trainDF.groupby([fieldStr]).size()\n",
    "    logodds={}\n",
    "    logoddsPF={}\n",
    "    MIN_CAT_COUNTS=2\n",
    "    default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "    for f in fields:\n",
    "        PA=F_counts[f]/float(len(trainDF))\n",
    "        logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "        logodds[f]=deepcopy(default_logodds)\n",
    "        for cat in F_C_counts[f].keys():\n",
    "            if (F_C_counts[f][cat]>MIN_CAT_COUNTS) and F_C_counts[f][cat]<F_counts[f]:\n",
    "                PA=F_C_counts[f][cat]/float(F_counts[f])\n",
    "                logodds[f][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "        logodds[f]=pd.Series(logodds[f])\n",
    "        logodds[f].index=range(len(categories))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    #fieldsTest=sorted(testDF[fieldStr].unique())\n",
    "    #N_count=0\n",
    "    #for f in fieldsTest:\n",
    "        #if f not in fields:\n",
    "            #logoddsPF[f]=-50.0  #np.log(0.)-np.log(1.)=-inf,便于计算，改为-99999.0\n",
    "            #logodds[f]=deepcopy(default_logodds)\n",
    "            #pa=1.0/float(len(categories))\n",
    "            #logodds[f][range(len(categories))]=np.log(pa)-np.log(1.0-pa)\n",
    "            #logodds[f]=pd.Series(logodds[f])\n",
    "            #logodds[f].index=range(len(categories))\n",
    "            #N_count=N_count+1\n",
    "    #print(fieldStr+' N_count: '+str(N_count))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    #引进代码原作者的新思想\n",
    "    if testDF.shape[0]>0: #如果testDF里有样本,......\n",
    "        print('There are some new:'+fieldStr)\n",
    "        new_fields=sorted(testDF[fieldStr].unique())\n",
    "        new_F_counts=testDF.groupby(fieldStr).size()\n",
    "        only_new=set(new_fields+fields)-set(fields)\n",
    "        only_old=set(new_fields+fields)-set(new_fields)\n",
    "        in_both=set(new_fields).intersection(fields)\n",
    "        print('# only_new_fieldds:'+str(len(only_new)))\n",
    "        for f in only_new:\n",
    "            PA=new_F_counts[f]/float(len(testDF)+len(trainDF))\n",
    "            logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "            logodds[f]=deepcopy(default_logodds)\n",
    "            logodds[f].index=range(len(categories))\n",
    "        for f in in_both:\n",
    "            PA=(F_counts[f]+new_F_counts[f])/float(len(testDF)+len(trainDF))\n",
    "            logoddsPF[f]=np.log(PA)-np.log(1.-PA)    \n",
    "    return logodds,logoddsPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(df,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y=False):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print(\"Creating address features\")###Creating address features###\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds_A[x])\n",
    "    address_features.columns=[\"logodds_A\"+str(x) for x in range(len(address_features.columns))]\n",
    "    if needT_D_M_Y:\n",
    "        print(\"Creating time T_D_M_Y features\")###Creating time T_D_M_Y features###\n",
    "        T_D_M_Y_features=cleanData[\"T_D_M_Y\"].apply(lambda xx: logodds_T[xx])\n",
    "        T_D_M_Y_features.columns=[\"logodds_T\"+str(xx) for xx in range(len(T_D_M_Y_features.columns))]\n",
    "\n",
    "    print(\"Parsing dates\")            ###Creating address features###\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "    #     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    #     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print(\"Creating one-hot variables\")\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPF_A\"]=cleanData[\"Address\"].apply(lambda x: logoddsPF_A[x])\n",
    "    if needT_D_M_Y:\n",
    "        cleanData[\"logoddsPF_T\"]=cleanData[\"T_D_M_Y\"].apply(lambda x: logoddsPF_T[x])\n",
    "    print(\"droping processed columns\")\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)    \n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    if needT_D_M_Y:\n",
    "        cleanData=cleanData.drop(\"T_D_M_Y\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    if needT_D_M_Y:\n",
    "        features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:]).join(T_D_M_Y_features.iloc[:,:])\n",
    "    else:\n",
    "        features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(keep='last')).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, Y, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(X) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)#数值在[low, high)区间。\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, X.shape[-1]))\n",
    "        targets = np.zeros((len(rows),Y.shape[1]))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = X[indices]\n",
    "            targets[j] = Y[rows[j]+delay]\n",
    "#         print('# row of Val: '+str(targets.shape[0]))###Tian\n",
    "        yield samples, targets\n",
    "    #Now here is the data generator that we will use. It yields a tuple (samples, targets) where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\n",
    "        # •data: The original array of floating point data, which we just normalized in the code snippet above.\n",
    "        # •lookback: How many timesteps back should our input data go.\n",
    "        # •delay: How many timesteps in the future should our target be.\n",
    "        # •min_index and max_index: Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another one for testing.\n",
    "        # •shuffle: Whether to shuffle our samples or draw them in chronological order.\n",
    "        # •batch_size: The number of samples per batch.\n",
    "        # •step: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of OrginalAllDF: (878049, 9)\n",
      "The shape of AllDF after del wrong X and Y values: (877982, 9)\n",
      "The shape of AllDF after drop_duplicates: (812529, 9)\n",
      "(689038, 2)\n",
      "Address_counts_allDF_trainDF_testDF: 23191_23191_0\n",
      "The # of AllDF, AllTrain, AllTest, is: 812529,812529,0\n",
      "-----------LOGOODS: Address-------------\n",
      "-----------LOGOODS: T_D_M_Y-------------\n",
      "-----------LOGOODS: parse_data of Alltrain  -------------\n",
      "Creating address features\n",
      "Creating time T_D_M_Y features\n",
      "Parsing dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n",
      "['X', 'Y', 'Time', 'Day', 'Month', 'Year', 'IsInterection', 'logoddsPF_A', 'logoddsPF_T', 'PD_BAYVIEW', 'PD_CENTRAL', 'PD_INGLESIDE', 'PD_MISSION', 'PD_NORTHERN', 'PD_PARK', 'PD_RICHMOND', 'PD_SOUTHERN', 'PD_TARAVAL', 'PD_TENDERLOIN', 'DAY_Friday', 'DAY_Monday', 'DAY_Saturday', 'DAY_Sunday', 'DAY_Thursday', 'DAY_Tuesday', 'DAY_Wednesday', 'logodds_A0', 'logodds_A1', 'logodds_A2', 'logodds_A3', 'logodds_A4', 'logodds_A5', 'logodds_A6', 'logodds_A7', 'logodds_A8', 'logodds_A9', 'logodds_A10', 'logodds_A11', 'logodds_A12', 'logodds_A13', 'logodds_A14', 'logodds_A15', 'logodds_A16', 'logodds_A17', 'logodds_A18', 'logodds_A19', 'logodds_A20', 'logodds_A21', 'logodds_A22', 'logodds_A23', 'logodds_A24', 'logodds_A25', 'logodds_A26', 'logodds_A27', 'logodds_A28', 'logodds_A29', 'logodds_A30', 'logodds_A31', 'logodds_A32', 'logodds_A33', 'logodds_A34', 'logodds_A35', 'logodds_A36', 'logodds_A37', 'logodds_A38', 'logodds_T0', 'logodds_T1', 'logodds_T2', 'logodds_T3', 'logodds_T4', 'logodds_T5', 'logodds_T6', 'logodds_T7', 'logodds_T8', 'logodds_T9', 'logodds_T10', 'logodds_T11', 'logodds_T12', 'logodds_T13', 'logodds_T14', 'logodds_T15', 'logodds_T16', 'logodds_T17', 'logodds_T18', 'logodds_T19', 'logodds_T20', 'logodds_T21', 'logodds_T22', 'logodds_T23', 'logodds_T24', 'logodds_T25', 'logodds_T26', 'logodds_T27', 'logodds_T28', 'logodds_T29', 'logodds_T30', 'logodds_T31', 'logodds_T32', 'logodds_T33', 'logodds_T34', 'logodds_T35', 'logodds_T36', 'logodds_T37', 'logodds_T38', 'IsDup', 'Awake', 'Summer', 'Fall', 'Winter', 'Spring']\n",
      "110\n",
      "------------Attention: we do not RandomOverSampler---------------\n",
      "------------ConsiderTime:  Sorting--------------\n"
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "ConsiderTime=True#False# True##trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "Rate_ALL=0.0 #0.0即不保留测试机\n",
    "needOverSampler=False\n",
    "needT_D_M_Y=True #False  使用_T_D_M_Y和周几\n",
    "allDF=pd.read_csv(\"./train_addrCorrect.csv\")\n",
    "print('The shape of OrginalAllDF: '+str(allDF.shape))\n",
    "\n",
    "xy_scaler=preprocessing.StandardScaler()\n",
    "xy_scaler.fit(allDF[[\"X\",\"Y\"]])\n",
    "allDF[[\"X\",\"Y\"]]=xy_scaler.transform(allDF[[\"X\",\"Y\"]])\n",
    "allDF=allDF[abs(allDF[\"Y\"])<100]\n",
    "allDF.index=range(len(allDF))\n",
    "print('The shape of AllDF after del wrong X and Y values: '+str(allDF.shape))\n",
    "\n",
    "def listCat(x):\n",
    "    return list(x)\n",
    "allDF.drop_duplicates(inplace=True,subset=['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'Category'])\n",
    "Train_duplicated=pd.pivot_table(allDF,index=['Dates','DayOfWeek','PdDistrict', 'Address', 'X', 'Y'], values='Category',aggfunc=[len,listCat])\n",
    "print('The shape of AllDF after drop_duplicates: '+str(allDF.shape))\n",
    "print(Train_duplicated.shape)\n",
    "\n",
    "trainDF,testDF=train_test_split_DataFrame(allDF, test_size=Rate_ALL, considerTime=ConsiderTime, random_state=None)\n",
    "print('Address_counts_allDF_trainDF_testDF: ' + str(len(allDF[\"Address\"].unique())) + '_'+ str(len(trainDF[\"Address\"].unique())) + '_' + str(len(testDF[\"Address\"].unique())))\n",
    "\n",
    "N_AllSample=allDF.shape[0]\n",
    "N_AllTrain=trainDF.shape[0]\n",
    "N_AllTest=testDF.shape[0]\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "print('The # of AllDF, AllTrain, AllTest, is: '+str(N_AllSample)+','+str(N_AllTrain)+','+str(N_AllTest))\n",
    "#################Now proceed as before#################\n",
    "print('-----------LOGOODS: Address-------------')\n",
    "logodds_A,logoddsPF_A=field2Vec(trainDF,testDF,\"Address\")\n",
    "if needT_D_M_Y:\n",
    "    trainDF[\"T_D_M_Y\"]=trainDF[\"Dates\"].apply(Dates2TDMY)\n",
    "    trainDF[\"T_D_M_Y\"]=trainDF[\"T_D_M_Y\"]+trainDF[\"DayOfWeek\"]\n",
    "    if Rate_ALL>0:\n",
    "        testDF[[\"X\",\"Y\"]]=xy_scaler.transform(testDF[[\"X\",\"Y\"]])\n",
    "        testDF[\"T_D_M_Y\"]=testDF[\"Dates\"].apply(Dates2TDMY)\n",
    "        testDF[\"T_D_M_Y\"]=testDF[\"T_D_M_Y\"]+testDF[\"DayOfWeek\"]\n",
    "    print('-----------LOGOODS: T_D_M_Y-------------')\n",
    "    logodds_T,logoddsPF_T=field2Vec(trainDF,testDF,\"T_D_M_Y\")    \n",
    "else:\n",
    "    logodds_T=None\n",
    "    logoddsPF_T=None\n",
    "    \n",
    "print('-----------LOGOODS: parse_data of Alltrain  -------------')\n",
    "features, labels=parse_data(trainDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y) \n",
    "if Rate_ALL>0:\n",
    "    print('-----------LOGOODS: parse_data of Alltest  -------------')\n",
    "    features_test, labels_test=parse_data(testDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y)###########和训练集使用同样的时间和地点Logoodds值#####\n",
    "    x_test=features_test.values\n",
    "    y_test=labels_test.values\n",
    "    y_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_test)), num_classes=N_CLASS)\n",
    "\n",
    "print(features.columns.tolist())\n",
    "print(len(features.columns))\n",
    "\n",
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)\n",
    "if Rate_ALL>0:\n",
    "    features_test[collist]=scaler.transform(features_test)###########和训练集使用同样的scaler值#####\n",
    "######################################################\n",
    "#############################先进行过采样，然后再根据时间来排序##################################\n",
    "if needOverSampler:\n",
    "    print('------------RandomOverSampler--------------')\n",
    "    ros = RandomOverSampler()\n",
    "    featuresArrayOverSampler, labelsArrayOverSampler = ros.fit_resample(features.values,labels.values)#####过采样#####\n",
    "    N_AllTrain_OverSampler=int(featuresArrayOverSampler.shape[0])\n",
    "    print('Shape of OverSampler of AllTrain: '+str(featuresArrayOverSampler.shape))\n",
    "else:\n",
    "    featuresArrayOverSampler=features.values\n",
    "    labelsArrayOverSampler=labels.values\n",
    "    N_AllTrain_OverSampler=int(featuresArrayOverSampler.shape[0])\n",
    "    print('------------Attention: we do not RandomOverSampler---------------')\n",
    "if ConsiderTime:\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "    print('------------ConsiderTime:  Sorting--------------')\n",
    "    time_temp=featuresArrayOverSampler[:,2]+np.dot(featuresArrayOverSampler[:,3],100)+np.dot(featuresArrayOverSampler[:,4],10000)+np.dot(featuresArrayOverSampler[:,5],1000000)\n",
    "    features_label_time=np.column_stack((featuresArrayOverSampler,labelsArrayOverSampler))\n",
    "    features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "    features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "    labelsArrayOverSampler=features_label_time[:,-2]\n",
    "    featuresArrayOverSampler=features_label_time[:,0:featuresArrayOverSampler.shape[1]]\n",
    "    del features_label_time\n",
    "    #############################先进行过采样，然后再根据时间来排序----结束############################\n",
    "if Rate_ALL>0:\n",
    "    print('------------RandomOverSampler for AllTest--------------')\n",
    "    ros = RandomOverSampler()\n",
    "    featuresArray_test, labelsArray_test = ros.fit_resample(features_test.values,labels_test.values)#####过采样#####\n",
    "    N_AllTest_OverSampler=int(featuresArray_test.shape[0])\n",
    "    labelsArray_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArray_test)), num_classes=N_CLASS)\n",
    "    print('Shape of OverSampler of AllTest: '+str(featuresArray_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building LSTM model--------------\n",
      "N_Train_OverSampler= 162506\n",
      "BlockSize is: 1300\n"
     ]
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building LSTM model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=2\n",
    "N_EPOCHS=1\n",
    "N_HN_1=128\n",
    "N_HN=64\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "\n",
    "\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "print('N_Train_OverSampler= '+str(N_Train_OverSampler))\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=500\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('BlockSize is: '+str(BlockSize)) \n",
    "lookback=int(1.5*BlockSize)\n",
    "delay=-1\n",
    "\n",
    "RNNmodel = Sequential()\n",
    "RNNmodel.add(LSTM(N_HN_1,dropout=0.5, recurrent_dropout=0.5,return_sequences=True,input_shape=(None,input_dim)))\n",
    "RNNmodel.add(LSTM(N_HN,dropout=0.5, recurrent_dropout=0.5,return_sequences=True,))\n",
    "RNNmodel.add(LSTM(N_HN,dropout=0.5, recurrent_dropout=0.5))\n",
    "RNNmodel.add(Dense(output_dim))\n",
    "RNNmodel.add(Activation('softmax'))\n",
    "RNNmodel.compile(loss='categorical_crossentropy', optimizer=RMSprop(),metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------generator AllTrain_set, Train_set and Val_set for LSTM---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------generator AllTrain_set, Train_set and Val_set for LSTM---------------------------------')\n",
    "train_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=0, max_index=N_Train_OverSampler, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "val_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=N_Train_OverSampler-lookback, max_index=N_Train_OverSampler+1, shuffle=False, batch_size=N_BATCH, step=1)\n",
    "#数值在[min_index, max_index)区间。当delay=1时，就是用[min_index, max_index)区间的样本预测，第max_index个样本。当delay=2时，就是预测第max_index+1个\n",
    "train_steps= (N_Train_OverSampler-lookback) // N_BATCH\n",
    "val_steps =  1 #(N_Val-lookback) // N_BATCH\n",
    "# test_steps =(N_AllTest - lookback) // N_BATCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------LSTM GO GO GO!!!!---------------------------------------------')\n",
    "# history = RNNmodel.fit_generator(train_generator,steps_per_epoch=10,epochs=N_EPOCHS_0,verbose=1)\n",
    "history = RNNmodel.fit_generator(train_generator,steps_per_epoch=train_steps,epochs=N_EPOCHS_0,verbose=1,validation_data=val_generator,validation_steps=val_steps)\n",
    "print('------------LSTM train finished--------------------')\n",
    "RNNmodel.save('jjs_model_0203LSTMV1.h5')\n",
    "# RNNmodel.evaluate_generator(val_generator,steps=1, callbacks=None,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNmodel=load_model('jjs_model_0203LSTMV1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback=int(BlockSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Start the loop training!!---------------------\n",
      "i_s=0 in 500;  max_index=163807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.4498 - accuracy: 0.9393 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.5109 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.9393382]\n",
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0]\n",
      "i_s=1 in 500;  max_index=165107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 67s 4s/step - loss: 0.1233 - accuracy: 0.9917 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0893 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=2 in 500;  max_index=166407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 78s 5s/step - loss: 2.2467 - accuracy: 0.4632 - top_k_categorical_accuracy: 0.7528 - val_loss: 3.9398 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=3 in 500;  max_index=167707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 75s 4s/step - loss: 2.1141 - accuracy: 0.1544 - top_k_categorical_accuracy: 0.9504 - val_loss: 1.8774 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=4 in 500;  max_index=169007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 82s 5s/step - loss: 2.0190 - accuracy: 0.5303 - top_k_categorical_accuracy: 0.7767 - val_loss: 2.0577 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=5 in 500;  max_index=170307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 83s 5s/step - loss: 0.5796 - accuracy: 0.9072 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.3548 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=6 in 500;  max_index=171607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 70s 4s/step - loss: 0.5569 - accuracy: 0.8943 - top_k_categorical_accuracy: 0.9982 - val_loss: 2.7202 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=7 in 500;  max_index=172907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 77s 5s/step - loss: 1.4085 - accuracy: 0.4265 - top_k_categorical_accuracy: 0.9807 - val_loss: 3.1508 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=8 in 500;  max_index=174207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 66s 4s/step - loss: 2.3500 - accuracy: 0.4540 - top_k_categorical_accuracy: 0.7987 - val_loss: 3.5425 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=9 in 500;  max_index=175507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.4821 - accuracy: 0.8998 - top_k_categorical_accuracy: 0.9724 - val_loss: 2.8871 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=10 in 500;  max_index=176807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 71s 4s/step - loss: 1.6891 - accuracy: 0.5377 - top_k_categorical_accuracy: 0.8318 - val_loss: 3.8263 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=11 in 500;  max_index=178107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 85s 5s/step - loss: 0.5939 - accuracy: 0.8392 - top_k_categorical_accuracy: 0.9660 - val_loss: 2.7522 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=12 in 500;  max_index=179407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 80s 5s/step - loss: 1.7972 - accuracy: 0.5377 - top_k_categorical_accuracy: 0.8768 - val_loss: 2.8888 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=13 in 500;  max_index=180707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.1656 - accuracy: 0.9642 - top_k_categorical_accuracy: 0.9991 - val_loss: 2.5676 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=14 in 500;  max_index=182007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 2.3829 - accuracy: 0.3125 - top_k_categorical_accuracy: 0.6379 - val_loss: 2.6078 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=15 in 500;  max_index=183307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 69s 4s/step - loss: 3.2661 - accuracy: 0.2500 - top_k_categorical_accuracy: 0.5414 - val_loss: 3.2128 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=16 in 500;  max_index=184607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 1.2369 - accuracy: 0.6710 - top_k_categorical_accuracy: 0.8557 - val_loss: 2.6213 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=17 in 500;  max_index=185907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 73s 4s/step - loss: 2.6328 - accuracy: 0.3640 - top_k_categorical_accuracy: 0.5938 - val_loss: 3.8175 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=18 in 500;  max_index=187207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 72s 4s/step - loss: 1.4911 - accuracy: 0.5947 - top_k_categorical_accuracy: 0.9439 - val_loss: 3.1791 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=19 in 500;  max_index=188507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.4462 - accuracy: 0.8906 - top_k_categorical_accuracy: 0.9550 - val_loss: 2.6410 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=20 in 500;  max_index=189807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 81s 5s/step - loss: 1.8426 - accuracy: 0.5450 - top_k_categorical_accuracy: 0.7803 - val_loss: 2.5647 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=21 in 500;  max_index=191107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.5361 - accuracy: 0.8906 - top_k_categorical_accuracy: 0.9991 - val_loss: 2.2278 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=22 in 500;  max_index=192407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 65s 4s/step - loss: 2.2719 - accuracy: 0.4577 - top_k_categorical_accuracy: 0.7132 - val_loss: 2.5532 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=23 in 500;  max_index=193707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 70s 4s/step - loss: 1.3336 - accuracy: 0.7105 - top_k_categorical_accuracy: 0.8483 - val_loss: 3.3000 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=24 in 500;  max_index=195007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 68s 4s/step - loss: 1.0093 - accuracy: 0.7197 - top_k_categorical_accuracy: 0.9292 - val_loss: 1.7822 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=25 in 500;  max_index=196307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 65s 4s/step - loss: 1.0372 - accuracy: 0.7077 - top_k_categorical_accuracy: 0.9467 - val_loss: 1.5370 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=26 in 500;  max_index=197607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 1.8432 - accuracy: 0.5028 - top_k_categorical_accuracy: 0.8051 - val_loss: 2.5177 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=27 in 500;  max_index=198907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 1.8147 - accuracy: 0.5616 - top_k_categorical_accuracy: 0.7822 - val_loss: 3.6489 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=28 in 500;  max_index=200207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.8064 - accuracy: 0.7767 - top_k_categorical_accuracy: 0.9393 - val_loss: 3.5268 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=29 in 500;  max_index=201507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.4413 - accuracy: 0.8658 - top_k_categorical_accuracy: 0.9972 - val_loss: 1.0362 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=30 in 500;  max_index=202807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 1.0793 - accuracy: 0.7426 - top_k_categorical_accuracy: 0.9586 - val_loss: 2.5310 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=31 in 500;  max_index=204107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 59s 3s/step - loss: 1.7711 - accuracy: 0.5846 - top_k_categorical_accuracy: 0.8006 - val_loss: 3.1545 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=32 in 500;  max_index=205407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.4772 - accuracy: 0.8778 - top_k_categorical_accuracy: 0.9862 - val_loss: 2.6482 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=33 in 500;  max_index=206707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.6393 - accuracy: 0.8153 - top_k_categorical_accuracy: 0.9724 - val_loss: 3.6907 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=34 in 500;  max_index=208007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.7355 - accuracy: 0.8070 - top_k_categorical_accuracy: 0.9494 - val_loss: 1.9328 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=35 in 500;  max_index=209307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 0.0696 - accuracy: 0.9954 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4711 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=36 in 500;  max_index=210607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 1.6582 - accuracy: 0.5349 - top_k_categorical_accuracy: 0.8851 - val_loss: 2.6793 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=37 in 500;  max_index=211907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.4670 - accuracy: 0.9007 - top_k_categorical_accuracy: 0.9945 - val_loss: 3.0902 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=38 in 500;  max_index=213207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 3.4849 - accuracy: 0.1700 - top_k_categorical_accuracy: 0.4531 - val_loss: 3.4528 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=39 in 500;  max_index=214507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 71s 4s/step - loss: 2.0720 - accuracy: 0.4412 - top_k_categorical_accuracy: 0.8143 - val_loss: 4.6181 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=40 in 500;  max_index=215807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.5844 - accuracy: 0.8943 - top_k_categorical_accuracy: 0.9807 - val_loss: 1.8547 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=41 in 500;  max_index=217107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.7934 - accuracy: 0.7803 - top_k_categorical_accuracy: 0.9550 - val_loss: 1.6649 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=42 in 500;  max_index=218407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.7377 - accuracy: 0.7730 - top_k_categorical_accuracy: 0.9825 - val_loss: 1.8676 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=43 in 500;  max_index=219707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 3.3620 - accuracy: 0.3401 - top_k_categorical_accuracy: 0.5542 - val_loss: 3.0634 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=44 in 500;  max_index=221007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.4366 - accuracy: 0.8842 - top_k_categorical_accuracy: 0.9761 - val_loss: 1.5553 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=45 in 500;  max_index=222307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.0725 - accuracy: 0.9972 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.1305 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=46 in 500;  max_index=223607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 72s 4s/step - loss: 1.1291 - accuracy: 0.7270 - top_k_categorical_accuracy: 0.9770 - val_loss: 1.9534 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=47 in 500;  max_index=224907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.2250 - accuracy: 0.9430 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.1865 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=48 in 500;  max_index=226207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 66s 4s/step - loss: 2.0382 - accuracy: 0.6066 - top_k_categorical_accuracy: 0.8713 - val_loss: 3.5557 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=49 in 500;  max_index=227507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 2.0392 - accuracy: 0.5533 - top_k_categorical_accuracy: 0.7509 - val_loss: 3.1109 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191]\n",
      "i_s=50 in 500;  max_index=228807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.7457 - accuracy: 0.8006 - top_k_categorical_accuracy: 0.9926 - val_loss: 3.2615 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=51 in 500;  max_index=230107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.3375 - accuracy: 0.8879 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0379 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=52 in 500;  max_index=231407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 67s 4s/step - loss: 0.7034 - accuracy: 0.8327 - top_k_categorical_accuracy: 0.9733 - val_loss: 2.7269 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=53 in 500;  max_index=232707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 1.7147 - accuracy: 0.6599 - top_k_categorical_accuracy: 0.8272 - val_loss: 2.9626 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=54 in 500;  max_index=234007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.2656 - accuracy: 0.9145 - top_k_categorical_accuracy: 0.9972 - val_loss: 1.3355 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=55 in 500;  max_index=235307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 2.2236 - accuracy: 0.5074 - top_k_categorical_accuracy: 0.7629 - val_loss: 2.7918 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=56 in 500;  max_index=236607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 1.4668 - accuracy: 0.6360 - top_k_categorical_accuracy: 0.9292 - val_loss: 3.5741 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=57 in 500;  max_index=237907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 71s 4s/step - loss: 3.5541 - accuracy: 0.1572 - top_k_categorical_accuracy: 0.4292 - val_loss: 4.4123 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=58 in 500;  max_index=239207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 60s 4s/step - loss: 1.3871 - accuracy: 0.6507 - top_k_categorical_accuracy: 0.8254 - val_loss: 0.6834 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=59 in 500;  max_index=240507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 68s 4s/step - loss: 2.9723 - accuracy: 0.2822 - top_k_categorical_accuracy: 0.5901 - val_loss: 4.5194 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=60 in 500;  max_index=241807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 1.2538 - accuracy: 0.7298 - top_k_categorical_accuracy: 0.8897 - val_loss: 2.0641 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=61 in 500;  max_index=243107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.5607 - accuracy: 0.8695 - top_k_categorical_accuracy: 0.9476 - val_loss: 2.6892 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=62 in 500;  max_index=244407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 3.0073 - accuracy: 0.3199 - top_k_categorical_accuracy: 0.6314 - val_loss: 3.8615 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=63 in 500;  max_index=245707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 1.7379 - accuracy: 0.5377 - top_k_categorical_accuracy: 0.7950 - val_loss: 3.9687 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=64 in 500;  max_index=247007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.7896 - accuracy: 0.7812 - top_k_categorical_accuracy: 0.9200 - val_loss: 2.2505 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=65 in 500;  max_index=248307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 2.4036 - accuracy: 0.5046 - top_k_categorical_accuracy: 0.6746 - val_loss: 3.7997 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=66 in 500;  max_index=249607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.3866 - accuracy: 0.9090 - top_k_categorical_accuracy: 0.9724 - val_loss: 2.6915 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=67 in 500;  max_index=250907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 64s 4s/step - loss: 1.8180 - accuracy: 0.5221 - top_k_categorical_accuracy: 0.8759 - val_loss: 3.1761 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=68 in 500;  max_index=252207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 59s 3s/step - loss: 0.2445 - accuracy: 0.9246 - top_k_categorical_accuracy: 0.9945 - val_loss: 1.1579 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=69 in 500;  max_index=253507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0299 - accuracy: 0.9982 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.8858 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=70 in 500;  max_index=254807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 71s 4s/step - loss: 0.9088 - accuracy: 0.7831 - top_k_categorical_accuracy: 0.9972 - val_loss: 1.0301 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=71 in 500;  max_index=256107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.9609 - accuracy: 0.7785 - top_k_categorical_accuracy: 0.9357 - val_loss: 2.1856 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=72 in 500;  max_index=257407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.8124 - accuracy: 0.8199 - top_k_categorical_accuracy: 0.9660 - val_loss: 3.6134 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=73 in 500;  max_index=258707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.2584 - accuracy: 0.9246 - top_k_categorical_accuracy: 0.9926 - val_loss: 0.9621 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=74 in 500;  max_index=260007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.2458 - accuracy: 0.9357 - top_k_categorical_accuracy: 0.9954 - val_loss: 1.6812 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=75 in 500;  max_index=261307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.7291 - accuracy: 0.8079 - top_k_categorical_accuracy: 0.9733 - val_loss: 2.5360 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=76 in 500;  max_index=262607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.8183 - accuracy: 0.7914 - top_k_categorical_accuracy: 0.9651 - val_loss: 3.1297 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=77 in 500;  max_index=263907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.1484 - accuracy: 0.9632 - top_k_categorical_accuracy: 0.9972 - val_loss: 2.8010 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=78 in 500;  max_index=265207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 2.1077 - accuracy: 0.4871 - top_k_categorical_accuracy: 0.7472 - val_loss: 3.2597 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=79 in 500;  max_index=266507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 69s 4s/step - loss: 1.5902 - accuracy: 0.5689 - top_k_categorical_accuracy: 0.8557 - val_loss: 2.9900 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=80 in 500;  max_index=267807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.7981 - accuracy: 0.8061 - top_k_categorical_accuracy: 0.9743 - val_loss: 3.3637 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=81 in 500;  max_index=269107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 68s 4s/step - loss: 1.2007 - accuracy: 0.6691 - top_k_categorical_accuracy: 0.9347 - val_loss: 2.9959 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=82 in 500;  max_index=270407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.9771 - accuracy: 0.7748 - top_k_categorical_accuracy: 0.8915 - val_loss: 2.5140 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=83 in 500;  max_index=271707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 1.4090 - accuracy: 0.6801 - top_k_categorical_accuracy: 0.8428 - val_loss: 2.2322 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=84 in 500;  max_index=273007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 72s 4s/step - loss: 0.9204 - accuracy: 0.7656 - top_k_categorical_accuracy: 0.8925 - val_loss: 3.1446 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=85 in 500;  max_index=274307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 67s 4s/step - loss: 0.8255 - accuracy: 0.7436 - top_k_categorical_accuracy: 0.9706 - val_loss: 2.4612 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=86 in 500;  max_index=275607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 1.0643 - accuracy: 0.7684 - top_k_categorical_accuracy: 0.9219 - val_loss: 3.0300 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=87 in 500;  max_index=276907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.6232 - accuracy: 0.8594 - top_k_categorical_accuracy: 0.9770 - val_loss: 3.1917 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=88 in 500;  max_index=278207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.1805 - accuracy: 0.9651 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0885 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=89 in 500;  max_index=279507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 1.1250 - accuracy: 0.7132 - top_k_categorical_accuracy: 0.8888 - val_loss: 2.3003 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=90 in 500;  max_index=280807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 67s 4s/step - loss: 0.1257 - accuracy: 0.9697 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.5577 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=91 in 500;  max_index=282107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 60s 4s/step - loss: 1.1812 - accuracy: 0.6756 - top_k_categorical_accuracy: 0.9412 - val_loss: 3.0127 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=92 in 500;  max_index=283407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 1.1786 - accuracy: 0.6958 - top_k_categorical_accuracy: 0.9403 - val_loss: 2.2019 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=93 in 500;  max_index=284707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 1.7093 - accuracy: 0.6103 - top_k_categorical_accuracy: 0.8401 - val_loss: 1.5964 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=94 in 500;  max_index=286007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.2009 - accuracy: 0.9531 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.9770 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=95 in 500;  max_index=287307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 2.5707 - accuracy: 0.3824 - top_k_categorical_accuracy: 0.6930 - val_loss: 3.1393 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=96 in 500;  max_index=288607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 2.3684 - accuracy: 0.4145 - top_k_categorical_accuracy: 0.6958 - val_loss: 3.0381 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=97 in 500;  max_index=289907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.3297 - accuracy: 0.8971 - top_k_categorical_accuracy: 0.9862 - val_loss: 2.2041 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=98 in 500;  max_index=291207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.8446 - accuracy: 0.7684 - top_k_categorical_accuracy: 0.9871 - val_loss: 2.4131 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324]\n",
      "i_s=99 in 500;  max_index=292507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.3494 - accuracy: 0.8860 - top_k_categorical_accuracy: 0.9991 - val_loss: 2.4545 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=100 in 500;  max_index=293807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.6947 - accuracy: 0.8079 - top_k_categorical_accuracy: 0.9798 - val_loss: 2.5053 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=101 in 500;  max_index=295107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.3412 - accuracy: 0.8879 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5122 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=102 in 500;  max_index=296407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.3699 - accuracy: 0.8869 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.9905 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=103 in 500;  max_index=297707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 68s 4s/step - loss: 2.8241 - accuracy: 0.4007 - top_k_categorical_accuracy: 0.8539 - val_loss: 3.0855 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=104 in 500;  max_index=299007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 1.0819 - accuracy: 0.6342 - top_k_categorical_accuracy: 0.9761 - val_loss: 2.5762 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=105 in 500;  max_index=300307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.6027 - accuracy: 0.8520 - top_k_categorical_accuracy: 0.9458 - val_loss: 2.0635 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=106 in 500;  max_index=301607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.5048 - accuracy: 0.8722 - top_k_categorical_accuracy: 0.9963 - val_loss: 2.0465 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=107 in 500;  max_index=302907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.0286 - accuracy: 0.9982 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.7077 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=108 in 500;  max_index=304207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.1389 - accuracy: 0.9632 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.1050 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=109 in 500;  max_index=305507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 59s 3s/step - loss: 0.6217 - accuracy: 0.8355 - top_k_categorical_accuracy: 0.9926 - val_loss: 1.4537 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=110 in 500;  max_index=306807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 3.4993 - accuracy: 0.3961 - top_k_categorical_accuracy: 0.6213 - val_loss: 3.9125 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=111 in 500;  max_index=308107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.8356 - accuracy: 0.7721 - top_k_categorical_accuracy: 0.9210 - val_loss: 2.8958 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=112 in 500;  max_index=309407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 1.0433 - accuracy: 0.7399 - top_k_categorical_accuracy: 0.8906 - val_loss: 1.9512 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=113 in 500;  max_index=310707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.6484 - accuracy: 0.8079 - top_k_categorical_accuracy: 0.9945 - val_loss: 0.5444 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=114 in 500;  max_index=312007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 70s 4s/step - loss: 0.1211 - accuracy: 0.9596 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.8346 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=115 in 500;  max_index=313307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 1.3397 - accuracy: 0.6783 - top_k_categorical_accuracy: 0.9108 - val_loss: 3.7414 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=116 in 500;  max_index=314607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 0.6659 - accuracy: 0.8401 - top_k_categorical_accuracy: 0.9835 - val_loss: 2.6073 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=117 in 500;  max_index=315907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.4884 - accuracy: 0.8447 - top_k_categorical_accuracy: 0.9853 - val_loss: 1.4336 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=118 in 500;  max_index=317207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 3.5647 - accuracy: 0.3272 - top_k_categorical_accuracy: 0.5818 - val_loss: 3.4664 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=119 in 500;  max_index=318507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.7546 - accuracy: 0.7693 - top_k_categorical_accuracy: 0.9926 - val_loss: 2.3616 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=120 in 500;  max_index=319807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 1.6607 - accuracy: 0.5938 - top_k_categorical_accuracy: 0.8162 - val_loss: 4.2504 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=121 in 500;  max_index=321107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 0.6246 - accuracy: 0.8805 - top_k_categorical_accuracy: 0.9761 - val_loss: 3.1721 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=122 in 500;  max_index=322407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 59s 3s/step - loss: 0.9497 - accuracy: 0.7610 - top_k_categorical_accuracy: 0.9412 - val_loss: 2.1444 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=123 in 500;  max_index=323707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 59s 3s/step - loss: 0.9345 - accuracy: 0.7050 - top_k_categorical_accuracy: 0.9825 - val_loss: 1.1410 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=124 in 500;  max_index=325007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.3568 - accuracy: 0.9127 - top_k_categorical_accuracy: 0.9688 - val_loss: 2.7994 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=125 in 500;  max_index=326307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.0460 - accuracy: 0.9926 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.3857 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=126 in 500;  max_index=327607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.4502 - accuracy: 0.8704 - top_k_categorical_accuracy: 0.9945 - val_loss: 1.2380 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=127 in 500;  max_index=328907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.7451 - accuracy: 0.8042 - top_k_categorical_accuracy: 0.9945 - val_loss: 0.7509 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=128 in 500;  max_index=330207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.0745 - accuracy: 0.9770 - top_k_categorical_accuracy: 0.9991 - val_loss: 2.7908 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=129 in 500;  max_index=331507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.1479 - accuracy: 0.9485 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.6509 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=130 in 500;  max_index=332807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.1307 - accuracy: 0.9513 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.9961 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=131 in 500;  max_index=334107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.6512 - accuracy: 0.8235 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.8737 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=132 in 500;  max_index=335407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.0278 - accuracy: 0.9963 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3580 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=133 in 500;  max_index=336707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.0140 - accuracy: 0.9991 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5459 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=134 in 500;  max_index=338007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.0564 - accuracy: 0.9835 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=135 in 500;  max_index=339307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 1.2657 - accuracy: 0.7123 - top_k_categorical_accuracy: 0.9228 - val_loss: 5.5357 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=136 in 500;  max_index=340607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 59s 3s/step - loss: 1.2159 - accuracy: 0.6884 - top_k_categorical_accuracy: 0.9706 - val_loss: 2.9627 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=137 in 500;  max_index=341907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 2.6869 - accuracy: 0.3971 - top_k_categorical_accuracy: 0.6673 - val_loss: 5.3911 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=138 in 500;  max_index=343207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.1230 - accuracy: 0.9844 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.2062 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=139 in 500;  max_index=344507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 67s 4s/step - loss: 0.7927 - accuracy: 0.7675 - top_k_categorical_accuracy: 0.9706 - val_loss: 1.4231 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=140 in 500;  max_index=345807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 3.8509 - accuracy: 0.1985 - top_k_categorical_accuracy: 0.4228 - val_loss: 4.2672 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=141 in 500;  max_index=347107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.1234 - accuracy: 0.9688 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.3197 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=142 in 500;  max_index=348407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 1.9954 - accuracy: 0.5322 - top_k_categorical_accuracy: 0.8428 - val_loss: 5.8976 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=143 in 500;  max_index=349707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 1.3652 - accuracy: 0.6884 - top_k_categorical_accuracy: 0.8612 - val_loss: 3.0372 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=144 in 500;  max_index=351007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.4374 - accuracy: 0.8879 - top_k_categorical_accuracy: 0.9733 - val_loss: 2.5142 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=145 in 500;  max_index=352307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.5448 - accuracy: 0.8254 - top_k_categorical_accuracy: 0.9982 - val_loss: 2.6549 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=146 in 500;  max_index=353607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.1671 - accuracy: 0.7050 - top_k_categorical_accuracy: 0.8722 - val_loss: 2.5896 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=147 in 500;  max_index=354907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.2264 - accuracy: 0.9338 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.8152 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824, 0.8860294, 0.8079044, 0.8878676, 0.8869485, 0.4007353, 0.63419116, 0.85202205, 0.8722426, 0.9981618, 0.9632353, 0.83547795, 0.3961397, 0.77205884, 0.7398897, 0.8079044, 0.95955884, 0.67830884, 0.8400735, 0.8446691, 0.3272059, 0.7693015, 0.59375, 0.8805147, 0.7610294, 0.7049632, 0.91268384, 0.99264705, 0.8704044, 0.80422795, 0.97702205, 0.9485294, 0.9512868, 0.8235294, 0.9963235, 0.9990809, 0.9834559, 0.71231616, 0.6884191, 0.3970588, 0.984375, 0.7674632, 0.1985294, 0.96875, 0.5321691, 0.6884191, 0.8878676, 0.8253676, 0.7049632, 0.9338235]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324, 0.9990809, 0.9797794, 1.0, 0.9990809, 0.8538603, 0.97610295, 0.94577205, 0.9963235, 1.0, 1.0, 0.99264705, 0.6213235, 0.9209559, 0.890625, 0.9944853, 1.0, 0.9108456, 0.9834559, 0.9852941, 0.5818015, 0.99264705, 0.8161765, 0.97610295, 0.9411765, 0.9825368, 0.96875, 1.0, 0.9944853, 0.9944853, 0.9990809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9227941, 0.9705882, 0.6672794, 1.0, 0.9705882, 0.4227941, 0.9990809, 0.8428309, 0.8612132, 0.9733456, 0.9981618, 0.8722426, 0.9990809]\n",
      "i_s=148 in 500;  max_index=356207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 1.9964 - accuracy: 0.5873 - top_k_categorical_accuracy: 0.7629 - val_loss: 4.9882 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=149 in 500;  max_index=357507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 1.0348 - accuracy: 0.7353 - top_k_categorical_accuracy: 0.9963 - val_loss: 0.6778 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=150 in 500;  max_index=358807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 1.3317 - accuracy: 0.6783 - top_k_categorical_accuracy: 0.8879 - val_loss: 5.1332 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=151 in 500;  max_index=360107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 69s 4s/step - loss: 0.2993 - accuracy: 0.9062 - top_k_categorical_accuracy: 0.9936 - val_loss: 2.6537 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=152 in 500;  max_index=361407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.0577 - accuracy: 0.9844 - top_k_categorical_accuracy: 0.9982 - val_loss: 1.4166 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=153 in 500;  max_index=362707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 0.0220 - accuracy: 0.9954 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.4335 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=154 in 500;  max_index=364007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.6710 - accuracy: 0.8318 - top_k_categorical_accuracy: 0.9586 - val_loss: 1.6280 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=155 in 500;  max_index=365307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 2.0944 - accuracy: 0.5772 - top_k_categorical_accuracy: 0.8290 - val_loss: 2.5116 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=156 in 500;  max_index=366607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.3640 - accuracy: 0.8989 - top_k_categorical_accuracy: 0.9963 - val_loss: 3.5602 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=157 in 500;  max_index=367907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 3.0286 - accuracy: 0.3833 - top_k_categorical_accuracy: 0.6461 - val_loss: 3.7484 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=158 in 500;  max_index=369207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 2.7241 - accuracy: 0.2757 - top_k_categorical_accuracy: 0.7059 - val_loss: 5.5257 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=159 in 500;  max_index=370507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.8968 - accuracy: 0.7638 - top_k_categorical_accuracy: 0.9145 - val_loss: 3.3982 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=160 in 500;  max_index=371807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.4151 - accuracy: 0.8888 - top_k_categorical_accuracy: 0.9862 - val_loss: 3.5769 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=161 in 500;  max_index=373107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 1.3238 - accuracy: 0.6930 - top_k_categorical_accuracy: 0.9044 - val_loss: 3.5372 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=162 in 500;  max_index=374407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.4625 - accuracy: 0.8713 - top_k_categorical_accuracy: 0.9697 - val_loss: 0.4638 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=163 in 500;  max_index=375707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 2.9034 - accuracy: 0.5726 - top_k_categorical_accuracy: 0.7426 - val_loss: 5.2841 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=164 in 500;  max_index=377007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.9677 - accuracy: 0.7353 - top_k_categorical_accuracy: 0.9210 - val_loss: 1.9435 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=165 in 500;  max_index=378307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 67s 4s/step - loss: 0.2288 - accuracy: 0.9449 - top_k_categorical_accuracy: 0.9926 - val_loss: 3.3346 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=166 in 500;  max_index=379607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 0.7577 - accuracy: 0.8364 - top_k_categorical_accuracy: 0.9752 - val_loss: 3.7814 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=167 in 500;  max_index=380907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 57s 3s/step - loss: 2.0890 - accuracy: 0.5496 - top_k_categorical_accuracy: 0.8511 - val_loss: 1.2485 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=168 in 500;  max_index=382207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 59s 3s/step - loss: 0.0898 - accuracy: 0.9844 - top_k_categorical_accuracy: 0.9991 - val_loss: 2.9122 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=169 in 500;  max_index=383507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 1.3390 - accuracy: 0.6636 - top_k_categorical_accuracy: 0.8805 - val_loss: 2.4799 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=170 in 500;  max_index=384807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 1.0859 - accuracy: 0.6994 - top_k_categorical_accuracy: 0.9274 - val_loss: 3.2541 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=171 in 500;  max_index=386107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.2673 - accuracy: 0.9228 - top_k_categorical_accuracy: 0.9963 - val_loss: 1.7630 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=172 in 500;  max_index=387407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.7007 - accuracy: 0.7877 - top_k_categorical_accuracy: 0.9963 - val_loss: 0.6919 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=173 in 500;  max_index=388707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.3325 - accuracy: 0.9072 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.8941 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=174 in 500;  max_index=390007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.6714 - accuracy: 0.8051 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.6727 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=175 in 500;  max_index=391307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 2.2327 - accuracy: 0.5285 - top_k_categorical_accuracy: 0.8686 - val_loss: 4.6926 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=176 in 500;  max_index=392607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.3265 - accuracy: 0.8879 - top_k_categorical_accuracy: 0.9963 - val_loss: 1.8308 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=177 in 500;  max_index=393907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 1.0330 - accuracy: 0.7482 - top_k_categorical_accuracy: 0.9724 - val_loss: 1.6137 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=178 in 500;  max_index=395207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 2.9472 - accuracy: 0.3244 - top_k_categorical_accuracy: 0.6976 - val_loss: 3.1624 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=179 in 500;  max_index=396507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 59s 3s/step - loss: 0.5156 - accuracy: 0.8640 - top_k_categorical_accuracy: 0.9724 - val_loss: 1.6362 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=180 in 500;  max_index=397807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 1.0553 - accuracy: 0.7399 - top_k_categorical_accuracy: 0.8980 - val_loss: 2.8759 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=181 in 500;  max_index=399107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.5080 - accuracy: 0.8649 - top_k_categorical_accuracy: 0.9862 - val_loss: 3.8591 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=182 in 500;  max_index=400407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.9973 - accuracy: 0.7160 - top_k_categorical_accuracy: 0.9494 - val_loss: 2.9404 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=183 in 500;  max_index=401707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 2.6426 - accuracy: 0.3254 - top_k_categorical_accuracy: 0.6985 - val_loss: 3.6268 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=184 in 500;  max_index=403007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.6959 - accuracy: 0.8272 - top_k_categorical_accuracy: 0.9568 - val_loss: 3.7594 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=185 in 500;  max_index=404307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 2.4858 - accuracy: 0.3906 - top_k_categorical_accuracy: 0.6774 - val_loss: 3.6306 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=186 in 500;  max_index=405607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.8274 - accuracy: 0.7748 - top_k_categorical_accuracy: 0.8888 - val_loss: 1.8579 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=187 in 500;  max_index=406907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 58s 3s/step - loss: 0.1386 - accuracy: 0.9632 - top_k_categorical_accuracy: 0.9954 - val_loss: 0.5399 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=188 in 500;  max_index=408207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.1368 - accuracy: 0.7325 - top_k_categorical_accuracy: 0.9577 - val_loss: 3.1098 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=189 in 500;  max_index=409507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 2.0002 - accuracy: 0.5708 - top_k_categorical_accuracy: 0.7858 - val_loss: 2.9563 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=190 in 500;  max_index=410807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.0004 - accuracy: 0.7564 - top_k_categorical_accuracy: 0.9568 - val_loss: 3.7967 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=191 in 500;  max_index=412107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.2890 - accuracy: 0.9210 - top_k_categorical_accuracy: 0.9917 - val_loss: 1.7350 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=192 in 500;  max_index=413407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.7291 - accuracy: 0.8162 - top_k_categorical_accuracy: 0.9458 - val_loss: 2.3840 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=193 in 500;  max_index=414707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.4373 - accuracy: 0.8934 - top_k_categorical_accuracy: 0.9789 - val_loss: 2.7627 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=194 in 500;  max_index=416007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.1688 - accuracy: 0.9485 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.9625 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=195 in 500;  max_index=417307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 3.9876 - accuracy: 0.1893 - top_k_categorical_accuracy: 0.4945 - val_loss: 4.7586 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=196 in 500;  max_index=418607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 2.2867 - accuracy: 0.3621 - top_k_categorical_accuracy: 0.8346 - val_loss: 5.4583 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824, 0.8860294, 0.8079044, 0.8878676, 0.8869485, 0.4007353, 0.63419116, 0.85202205, 0.8722426, 0.9981618, 0.9632353, 0.83547795, 0.3961397, 0.77205884, 0.7398897, 0.8079044, 0.95955884, 0.67830884, 0.8400735, 0.8446691, 0.3272059, 0.7693015, 0.59375, 0.8805147, 0.7610294, 0.7049632, 0.91268384, 0.99264705, 0.8704044, 0.80422795, 0.97702205, 0.9485294, 0.9512868, 0.8235294, 0.9963235, 0.9990809, 0.9834559, 0.71231616, 0.6884191, 0.3970588, 0.984375, 0.7674632, 0.1985294, 0.96875, 0.5321691, 0.6884191, 0.8878676, 0.8253676, 0.7049632, 0.9338235, 0.58731616, 0.7352941, 0.67830884, 0.90625, 0.984375, 0.9954044, 0.8318015, 0.5772059, 0.89889705, 0.38327205, 0.2757353, 0.7637868, 0.8887868, 0.6930147, 0.8713235, 0.5726103, 0.7352941, 0.94485295, 0.83639705, 0.5496324, 0.984375, 0.66360295, 0.6994485, 0.9227941, 0.78768384, 0.9071691, 0.80514705, 0.5284926, 0.8878676, 0.7481618, 0.32444853, 0.8639706, 0.7398897, 0.8648897, 0.7159926, 0.32536766, 0.8272059, 0.390625, 0.77481616, 0.9632353, 0.7325368, 0.57077205, 0.75643384, 0.9209559, 0.8161765, 0.8933824, 0.9485294, 0.18933824, 0.36213234]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324, 0.9990809, 0.9797794, 1.0, 0.9990809, 0.8538603, 0.97610295, 0.94577205, 0.9963235, 1.0, 1.0, 0.99264705, 0.6213235, 0.9209559, 0.890625, 0.9944853, 1.0, 0.9108456, 0.9834559, 0.9852941, 0.5818015, 0.99264705, 0.8161765, 0.97610295, 0.9411765, 0.9825368, 0.96875, 1.0, 0.9944853, 0.9944853, 0.9990809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9227941, 0.9705882, 0.6672794, 1.0, 0.9705882, 0.4227941, 0.9990809, 0.8428309, 0.8612132, 0.9733456, 0.9981618, 0.8722426, 0.9990809, 0.7628676, 0.9963235, 0.8878676, 0.99356616, 0.9981618, 1.0, 0.9586397, 0.8290441, 0.9963235, 0.6461397, 0.7058824, 0.91452205, 0.9862132, 0.9044118, 0.9696691, 0.74264705, 0.9209559, 0.99264705, 0.97518384, 0.85110295, 0.9990809, 0.8805147, 0.9273897, 0.9963235, 0.9963235, 1.0, 1.0, 0.86856616, 0.9963235, 0.9724265, 0.6976103, 0.9724265, 0.89797795, 0.9862132, 0.9494485, 0.6985294, 0.9568015, 0.6773897, 0.8887868, 0.9954044, 0.9577206, 0.7858456, 0.9568015, 0.99172795, 0.94577205, 0.9788603, 0.9990809, 0.4944853, 0.83455884]\n",
      "i_s=197 in 500;  max_index=419907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 1.2887 - accuracy: 0.6847 - top_k_categorical_accuracy: 0.8750 - val_loss: 3.6189 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=198 in 500;  max_index=421207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.2084 - accuracy: 0.9449 - top_k_categorical_accuracy: 0.9972 - val_loss: 4.1358 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=199 in 500;  max_index=422507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.7491 - accuracy: 0.7730 - top_k_categorical_accuracy: 0.9917 - val_loss: 2.7680 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=200 in 500;  max_index=423807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.9940 - accuracy: 0.7638 - top_k_categorical_accuracy: 0.9798 - val_loss: 1.9100 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=201 in 500;  max_index=425107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.1638 - accuracy: 0.9568 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.6007 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=202 in 500;  max_index=426407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 2.1151 - accuracy: 0.5138 - top_k_categorical_accuracy: 0.7987 - val_loss: 2.0493 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=203 in 500;  max_index=427707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 0.4526 - accuracy: 0.8971 - top_k_categorical_accuracy: 0.9871 - val_loss: 2.4913 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=204 in 500;  max_index=429007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 2.9408 - accuracy: 0.5184 - top_k_categorical_accuracy: 0.7233 - val_loss: 6.0554 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=205 in 500;  max_index=430307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.6487 - accuracy: 0.8888 - top_k_categorical_accuracy: 0.9357 - val_loss: 2.1079 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=206 in 500;  max_index=431607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 2.4291 - accuracy: 0.3741 - top_k_categorical_accuracy: 0.7619 - val_loss: 3.5388 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=207 in 500;  max_index=432907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.4881 - accuracy: 0.8603 - top_k_categorical_accuracy: 0.9890 - val_loss: 1.9486 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=208 in 500;  max_index=434207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 68s 4s/step - loss: 3.3331 - accuracy: 0.3033 - top_k_categorical_accuracy: 0.5441 - val_loss: 6.0677 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=209 in 500;  max_index=435507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.1173 - accuracy: 0.9899 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0908 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=210 in 500;  max_index=436807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.0821 - accuracy: 0.9789 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.6733 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=211 in 500;  max_index=438107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 2.9589 - accuracy: 0.3493 - top_k_categorical_accuracy: 0.6673 - val_loss: 4.3979 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=212 in 500;  max_index=439407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.9524 - accuracy: 0.7877 - top_k_categorical_accuracy: 0.9136 - val_loss: 6.4211 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=213 in 500;  max_index=440707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 2.0883 - accuracy: 0.5230 - top_k_categorical_accuracy: 0.7381 - val_loss: 2.4513 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=214 in 500;  max_index=442007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 2.7713 - accuracy: 0.4007 - top_k_categorical_accuracy: 0.6149 - val_loss: 3.5940 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=215 in 500;  max_index=443307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.9659 - accuracy: 0.7472 - top_k_categorical_accuracy: 0.9182 - val_loss: 4.6571 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=216 in 500;  max_index=444607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.5892 - accuracy: 0.8511 - top_k_categorical_accuracy: 0.9614 - val_loss: 2.5353 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=217 in 500;  max_index=445907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 56s 3s/step - loss: 1.4109 - accuracy: 0.6158 - top_k_categorical_accuracy: 0.9219 - val_loss: 1.7875 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=218 in 500;  max_index=447207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.7313 - accuracy: 0.7923 - top_k_categorical_accuracy: 0.9779 - val_loss: 0.5607 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=219 in 500;  max_index=448507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.1463 - accuracy: 0.9660 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.3808 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=220 in 500;  max_index=449807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 1.3283 - accuracy: 0.7344 - top_k_categorical_accuracy: 0.9697 - val_loss: 2.7245 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=221 in 500;  max_index=451107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.1464 - accuracy: 0.9596 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.1720 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=222 in 500;  max_index=452407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 1.9440 - accuracy: 0.5744 - top_k_categorical_accuracy: 0.7831 - val_loss: 2.8301 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=223 in 500;  max_index=453707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.5205 - accuracy: 0.8585 - top_k_categorical_accuracy: 0.9825 - val_loss: 1.8453 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=224 in 500;  max_index=455007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 53s 3s/step - loss: 2.0921 - accuracy: 0.4724 - top_k_categorical_accuracy: 0.8392 - val_loss: 3.4392 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=225 in 500;  max_index=456307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.2324 - accuracy: 0.9393 - top_k_categorical_accuracy: 0.9899 - val_loss: 0.1022 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=226 in 500;  max_index=457607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.5791 - accuracy: 0.8355 - top_k_categorical_accuracy: 0.9917 - val_loss: 4.3551 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=227 in 500;  max_index=458907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.1112 - accuracy: 0.9733 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.8900 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=228 in 500;  max_index=460207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.6259 - accuracy: 0.8585 - top_k_categorical_accuracy: 0.9982 - val_loss: 2.0300 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=229 in 500;  max_index=461507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 1.6697 - accuracy: 0.6140 - top_k_categorical_accuracy: 0.8483 - val_loss: 3.8514 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=230 in 500;  max_index=462807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.9706 - accuracy: 0.5671 - top_k_categorical_accuracy: 0.7629 - val_loss: 5.2063 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=231 in 500;  max_index=464107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 2.7492 - accuracy: 0.3796 - top_k_categorical_accuracy: 0.7233 - val_loss: 3.6363 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=232 in 500;  max_index=465407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.8666 - accuracy: 0.8024 - top_k_categorical_accuracy: 0.9007 - val_loss: 2.9672 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=233 in 500;  max_index=466707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.6666 - accuracy: 0.8474 - top_k_categorical_accuracy: 0.9798 - val_loss: 1.8960 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=234 in 500;  max_index=468007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 2.1698 - accuracy: 0.4972 - top_k_categorical_accuracy: 0.7877 - val_loss: 2.5280 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=235 in 500;  max_index=469307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3862 - accuracy: 0.8759 - top_k_categorical_accuracy: 0.9945 - val_loss: 3.7472 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=236 in 500;  max_index=470607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.4717 - accuracy: 0.8695 - top_k_categorical_accuracy: 0.9908 - val_loss: 3.5176 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=237 in 500;  max_index=471907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 2.8379 - accuracy: 0.3658 - top_k_categorical_accuracy: 0.7160 - val_loss: 3.8812 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=238 in 500;  max_index=473207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.2225 - accuracy: 0.6866 - top_k_categorical_accuracy: 0.8906 - val_loss: 2.3083 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=239 in 500;  max_index=474507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.9399 - accuracy: 0.7518 - top_k_categorical_accuracy: 0.9292 - val_loss: 2.2700 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=240 in 500;  max_index=475807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.3328 - accuracy: 0.8897 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.8329 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=241 in 500;  max_index=477107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.3377 - accuracy: 0.9062 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1347 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=242 in 500;  max_index=478407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.5409 - accuracy: 0.8566 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0777 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=243 in 500;  max_index=479707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 1.8203 - accuracy: 0.6020 - top_k_categorical_accuracy: 0.8529 - val_loss: 3.3873 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=244 in 500;  max_index=481007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4726 - accuracy: 0.8649 - top_k_categorical_accuracy: 0.9963 - val_loss: 2.5716 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=245 in 500;  max_index=482307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 2.4855 - accuracy: 0.5110 - top_k_categorical_accuracy: 0.7169 - val_loss: 4.4277 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824, 0.8860294, 0.8079044, 0.8878676, 0.8869485, 0.4007353, 0.63419116, 0.85202205, 0.8722426, 0.9981618, 0.9632353, 0.83547795, 0.3961397, 0.77205884, 0.7398897, 0.8079044, 0.95955884, 0.67830884, 0.8400735, 0.8446691, 0.3272059, 0.7693015, 0.59375, 0.8805147, 0.7610294, 0.7049632, 0.91268384, 0.99264705, 0.8704044, 0.80422795, 0.97702205, 0.9485294, 0.9512868, 0.8235294, 0.9963235, 0.9990809, 0.9834559, 0.71231616, 0.6884191, 0.3970588, 0.984375, 0.7674632, 0.1985294, 0.96875, 0.5321691, 0.6884191, 0.8878676, 0.8253676, 0.7049632, 0.9338235, 0.58731616, 0.7352941, 0.67830884, 0.90625, 0.984375, 0.9954044, 0.8318015, 0.5772059, 0.89889705, 0.38327205, 0.2757353, 0.7637868, 0.8887868, 0.6930147, 0.8713235, 0.5726103, 0.7352941, 0.94485295, 0.83639705, 0.5496324, 0.984375, 0.66360295, 0.6994485, 0.9227941, 0.78768384, 0.9071691, 0.80514705, 0.5284926, 0.8878676, 0.7481618, 0.32444853, 0.8639706, 0.7398897, 0.8648897, 0.7159926, 0.32536766, 0.8272059, 0.390625, 0.77481616, 0.9632353, 0.7325368, 0.57077205, 0.75643384, 0.9209559, 0.8161765, 0.8933824, 0.9485294, 0.18933824, 0.36213234, 0.6847426, 0.94485295, 0.77297795, 0.7637868, 0.9568015, 0.5137868, 0.89705884, 0.5183824, 0.8887868, 0.3740809, 0.8602941, 0.3033088, 0.9898897, 0.9788603, 0.3492647, 0.78768384, 0.52297795, 0.4007353, 0.7472426, 0.85110295, 0.61580884, 0.7922794, 0.9659926, 0.734375, 0.95955884, 0.5744485, 0.8584559, 0.47242647, 0.9393382, 0.83547795, 0.9733456, 0.8584559, 0.6139706, 0.5670956, 0.37959558, 0.8023897, 0.8474265, 0.49724266, 0.8759191, 0.8694853, 0.3658088, 0.6865809, 0.7518382, 0.8897059, 0.90625, 0.8566176, 0.60202205, 0.8648897, 0.5110294]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324, 0.9990809, 0.9797794, 1.0, 0.9990809, 0.8538603, 0.97610295, 0.94577205, 0.9963235, 1.0, 1.0, 0.99264705, 0.6213235, 0.9209559, 0.890625, 0.9944853, 1.0, 0.9108456, 0.9834559, 0.9852941, 0.5818015, 0.99264705, 0.8161765, 0.97610295, 0.9411765, 0.9825368, 0.96875, 1.0, 0.9944853, 0.9944853, 0.9990809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9227941, 0.9705882, 0.6672794, 1.0, 0.9705882, 0.4227941, 0.9990809, 0.8428309, 0.8612132, 0.9733456, 0.9981618, 0.8722426, 0.9990809, 0.7628676, 0.9963235, 0.8878676, 0.99356616, 0.9981618, 1.0, 0.9586397, 0.8290441, 0.9963235, 0.6461397, 0.7058824, 0.91452205, 0.9862132, 0.9044118, 0.9696691, 0.74264705, 0.9209559, 0.99264705, 0.97518384, 0.85110295, 0.9990809, 0.8805147, 0.9273897, 0.9963235, 0.9963235, 1.0, 1.0, 0.86856616, 0.9963235, 0.9724265, 0.6976103, 0.9724265, 0.89797795, 0.9862132, 0.9494485, 0.6985294, 0.9568015, 0.6773897, 0.8887868, 0.9954044, 0.9577206, 0.7858456, 0.9568015, 0.99172795, 0.94577205, 0.9788603, 0.9990809, 0.4944853, 0.83455884, 0.875, 0.9972426, 0.99172795, 0.9797794, 0.9990809, 0.7987132, 0.9871324, 0.7233456, 0.9356618, 0.7619485, 0.9889706, 0.5441176, 1.0, 1.0, 0.6672794, 0.91360295, 0.7380515, 0.6148897, 0.9181985, 0.96139705, 0.921875, 0.97794116, 1.0, 0.9696691, 1.0, 0.7830882, 0.9825368, 0.8391544, 0.9898897, 0.99172795, 1.0, 0.9981618, 0.8483456, 0.7628676, 0.7233456, 0.9007353, 0.9797794, 0.78768384, 0.9944853, 0.99080884, 0.7159926, 0.890625, 0.92922795, 1.0, 1.0, 1.0, 0.85294116, 0.9963235, 0.7169118]\n",
      "i_s=246 in 500;  max_index=483607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.3115 - accuracy: 0.9697 - top_k_categorical_accuracy: 0.9991 - val_loss: 4.0639 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=247 in 500;  max_index=484907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.9794 - accuracy: 0.7472 - top_k_categorical_accuracy: 0.9200 - val_loss: 3.1020 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=248 in 500;  max_index=486207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.3642 - accuracy: 0.9044 - top_k_categorical_accuracy: 0.9899 - val_loss: 2.4961 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=249 in 500;  max_index=487507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 2.1198 - accuracy: 0.4991 - top_k_categorical_accuracy: 0.8042 - val_loss: 3.6326 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=250 in 500;  max_index=488807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.7731 - accuracy: 0.7868 - top_k_categorical_accuracy: 0.9467 - val_loss: 3.3332 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=251 in 500;  max_index=490107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.1650 - accuracy: 0.9494 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=252 in 500;  max_index=491407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 2.5342 - accuracy: 0.5855 - top_k_categorical_accuracy: 0.7629 - val_loss: 3.8346 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=253 in 500;  max_index=492707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.9038 - accuracy: 0.7445 - top_k_categorical_accuracy: 0.9586 - val_loss: 2.8504 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=254 in 500;  max_index=494007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.4290 - accuracy: 0.8722 - top_k_categorical_accuracy: 0.9715 - val_loss: 1.9888 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=255 in 500;  max_index=495307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.0289 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3604 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=256 in 500;  max_index=496607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.9474 - accuracy: 0.7785 - top_k_categorical_accuracy: 0.9062 - val_loss: 1.6831 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=257 in 500;  max_index=497907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.3289 - accuracy: 0.9320 - top_k_categorical_accuracy: 0.9954 - val_loss: 3.0650 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=258 in 500;  max_index=499207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.2453 - accuracy: 0.9357 - top_k_categorical_accuracy: 0.9982 - val_loss: 0.1085 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=259 in 500;  max_index=500507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.5605 - accuracy: 0.8428 - top_k_categorical_accuracy: 0.9770 - val_loss: 1.5935 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=260 in 500;  max_index=501807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.7772 - accuracy: 0.8051 - top_k_categorical_accuracy: 0.9660 - val_loss: 3.4594 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=261 in 500;  max_index=503107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.8873 - accuracy: 0.7693 - top_k_categorical_accuracy: 0.9844 - val_loss: 0.0585 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=262 in 500;  max_index=504407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.5531 - accuracy: 0.8603 - top_k_categorical_accuracy: 0.9844 - val_loss: 2.3836 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=263 in 500;  max_index=505707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.1680 - accuracy: 0.9485 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2807 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=264 in 500;  max_index=507007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.6600 - accuracy: 0.8493 - top_k_categorical_accuracy: 0.9926 - val_loss: 4.1564 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=265 in 500;  max_index=508307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.8679 - accuracy: 0.7693 - top_k_categorical_accuracy: 0.9697 - val_loss: 1.7463 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=266 in 500;  max_index=509607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 4.5549 - accuracy: 0.0625 - top_k_categorical_accuracy: 0.3805 - val_loss: 4.3677 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=267 in 500;  max_index=510907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.2329 - accuracy: 0.9540 - top_k_categorical_accuracy: 0.9963 - val_loss: 1.0008 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=268 in 500;  max_index=512207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.8428 - accuracy: 0.7868 - top_k_categorical_accuracy: 0.9890 - val_loss: 0.4982 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=269 in 500;  max_index=513507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 1.8523 - accuracy: 0.5460 - top_k_categorical_accuracy: 0.8961 - val_loss: 4.7331 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=270 in 500;  max_index=514807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 2.1740 - accuracy: 0.3824 - top_k_categorical_accuracy: 0.8842 - val_loss: 3.8814 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=271 in 500;  max_index=516107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.2059 - accuracy: 0.9531 - top_k_categorical_accuracy: 0.9972 - val_loss: 1.8410 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=272 in 500;  max_index=517407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.0443 - accuracy: 0.7426 - top_k_categorical_accuracy: 0.9090 - val_loss: 3.0294 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=273 in 500;  max_index=518707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.6747 - accuracy: 0.8373 - top_k_categorical_accuracy: 0.9531 - val_loss: 3.6688 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=274 in 500;  max_index=520007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.3354 - accuracy: 0.9145 - top_k_categorical_accuracy: 0.9835 - val_loss: 1.4357 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=275 in 500;  max_index=521307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.2036 - accuracy: 0.9412 - top_k_categorical_accuracy: 0.9991 - val_loss: 0.4329 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=276 in 500;  max_index=522607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.2678 - accuracy: 0.9265 - top_k_categorical_accuracy: 0.9862 - val_loss: 3.9398 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=277 in 500;  max_index=523907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 3.0084 - accuracy: 0.3961 - top_k_categorical_accuracy: 0.6425 - val_loss: 3.0195 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=278 in 500;  max_index=525207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.5251 - accuracy: 0.6443 - top_k_categorical_accuracy: 0.8493 - val_loss: 3.4210 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=279 in 500;  max_index=526507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.4337 - accuracy: 0.6756 - top_k_categorical_accuracy: 0.9200 - val_loss: 3.3730 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=280 in 500;  max_index=527807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.1887 - accuracy: 0.9586 - top_k_categorical_accuracy: 0.9963 - val_loss: 2.4983 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=281 in 500;  max_index=529107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.2578 - accuracy: 0.9283 - top_k_categorical_accuracy: 0.9890 - val_loss: 2.5621 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=282 in 500;  max_index=530407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.3178 - accuracy: 0.9035 - top_k_categorical_accuracy: 0.9936 - val_loss: 3.1281 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=283 in 500;  max_index=531707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.0383 - accuracy: 0.9908 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.6861 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=284 in 500;  max_index=533007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3525 - accuracy: 0.8961 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.6780 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=285 in 500;  max_index=534307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.1661 - accuracy: 0.7978 - top_k_categorical_accuracy: 0.8998 - val_loss: 3.2883 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=286 in 500;  max_index=535607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 1.4154 - accuracy: 0.6278 - top_k_categorical_accuracy: 0.8566 - val_loss: 3.8079 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=287 in 500;  max_index=536907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 1.3453 - accuracy: 0.7325 - top_k_categorical_accuracy: 0.8695 - val_loss: 3.9459 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=288 in 500;  max_index=538207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 1.6807 - accuracy: 0.5404 - top_k_categorical_accuracy: 0.8456 - val_loss: 3.4891 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=289 in 500;  max_index=539507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.3620 - accuracy: 0.9246 - top_k_categorical_accuracy: 0.9899 - val_loss: 3.0906 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=290 in 500;  max_index=540807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 1.7549 - accuracy: 0.4917 - top_k_categorical_accuracy: 0.9449 - val_loss: 3.3313 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=291 in 500;  max_index=542107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.2157 - accuracy: 0.9449 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.6818 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=292 in 500;  max_index=543407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.5877 - accuracy: 0.8483 - top_k_categorical_accuracy: 0.9724 - val_loss: 2.5248 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=293 in 500;  max_index=544707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 41s 2s/step - loss: 1.3563 - accuracy: 0.6287 - top_k_categorical_accuracy: 0.8934 - val_loss: 5.5538 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=294 in 500;  max_index=546007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.2871 - accuracy: 0.9210 - top_k_categorical_accuracy: 0.9926 - val_loss: 3.7511 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824, 0.8860294, 0.8079044, 0.8878676, 0.8869485, 0.4007353, 0.63419116, 0.85202205, 0.8722426, 0.9981618, 0.9632353, 0.83547795, 0.3961397, 0.77205884, 0.7398897, 0.8079044, 0.95955884, 0.67830884, 0.8400735, 0.8446691, 0.3272059, 0.7693015, 0.59375, 0.8805147, 0.7610294, 0.7049632, 0.91268384, 0.99264705, 0.8704044, 0.80422795, 0.97702205, 0.9485294, 0.9512868, 0.8235294, 0.9963235, 0.9990809, 0.9834559, 0.71231616, 0.6884191, 0.3970588, 0.984375, 0.7674632, 0.1985294, 0.96875, 0.5321691, 0.6884191, 0.8878676, 0.8253676, 0.7049632, 0.9338235, 0.58731616, 0.7352941, 0.67830884, 0.90625, 0.984375, 0.9954044, 0.8318015, 0.5772059, 0.89889705, 0.38327205, 0.2757353, 0.7637868, 0.8887868, 0.6930147, 0.8713235, 0.5726103, 0.7352941, 0.94485295, 0.83639705, 0.5496324, 0.984375, 0.66360295, 0.6994485, 0.9227941, 0.78768384, 0.9071691, 0.80514705, 0.5284926, 0.8878676, 0.7481618, 0.32444853, 0.8639706, 0.7398897, 0.8648897, 0.7159926, 0.32536766, 0.8272059, 0.390625, 0.77481616, 0.9632353, 0.7325368, 0.57077205, 0.75643384, 0.9209559, 0.8161765, 0.8933824, 0.9485294, 0.18933824, 0.36213234, 0.6847426, 0.94485295, 0.77297795, 0.7637868, 0.9568015, 0.5137868, 0.89705884, 0.5183824, 0.8887868, 0.3740809, 0.8602941, 0.3033088, 0.9898897, 0.9788603, 0.3492647, 0.78768384, 0.52297795, 0.4007353, 0.7472426, 0.85110295, 0.61580884, 0.7922794, 0.9659926, 0.734375, 0.95955884, 0.5744485, 0.8584559, 0.47242647, 0.9393382, 0.83547795, 0.9733456, 0.8584559, 0.6139706, 0.5670956, 0.37959558, 0.8023897, 0.8474265, 0.49724266, 0.8759191, 0.8694853, 0.3658088, 0.6865809, 0.7518382, 0.8897059, 0.90625, 0.8566176, 0.60202205, 0.8648897, 0.5110294, 0.9696691, 0.7472426, 0.9044118, 0.4990809, 0.7867647, 0.9494485, 0.58547795, 0.7444853, 0.8722426, 1.0, 0.7784926, 0.9319853, 0.9356618, 0.8428309, 0.80514705, 0.7693015, 0.8602941, 0.9485294, 0.8492647, 0.7693015, 0.0625, 0.9540441, 0.7867647, 0.5459559, 0.38235295, 0.953125, 0.74264705, 0.83731616, 0.91452205, 0.9411765, 0.9264706, 0.3961397, 0.6443015, 0.6755515, 0.9586397, 0.92830884, 0.9034926, 0.99080884, 0.8961397, 0.7977941, 0.6277574, 0.7325368, 0.54044116, 0.9246324, 0.49172795, 0.94485295, 0.8483456, 0.6286765, 0.9209559]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324, 0.9990809, 0.9797794, 1.0, 0.9990809, 0.8538603, 0.97610295, 0.94577205, 0.9963235, 1.0, 1.0, 0.99264705, 0.6213235, 0.9209559, 0.890625, 0.9944853, 1.0, 0.9108456, 0.9834559, 0.9852941, 0.5818015, 0.99264705, 0.8161765, 0.97610295, 0.9411765, 0.9825368, 0.96875, 1.0, 0.9944853, 0.9944853, 0.9990809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9227941, 0.9705882, 0.6672794, 1.0, 0.9705882, 0.4227941, 0.9990809, 0.8428309, 0.8612132, 0.9733456, 0.9981618, 0.8722426, 0.9990809, 0.7628676, 0.9963235, 0.8878676, 0.99356616, 0.9981618, 1.0, 0.9586397, 0.8290441, 0.9963235, 0.6461397, 0.7058824, 0.91452205, 0.9862132, 0.9044118, 0.9696691, 0.74264705, 0.9209559, 0.99264705, 0.97518384, 0.85110295, 0.9990809, 0.8805147, 0.9273897, 0.9963235, 0.9963235, 1.0, 1.0, 0.86856616, 0.9963235, 0.9724265, 0.6976103, 0.9724265, 0.89797795, 0.9862132, 0.9494485, 0.6985294, 0.9568015, 0.6773897, 0.8887868, 0.9954044, 0.9577206, 0.7858456, 0.9568015, 0.99172795, 0.94577205, 0.9788603, 0.9990809, 0.4944853, 0.83455884, 0.875, 0.9972426, 0.99172795, 0.9797794, 0.9990809, 0.7987132, 0.9871324, 0.7233456, 0.9356618, 0.7619485, 0.9889706, 0.5441176, 1.0, 1.0, 0.6672794, 0.91360295, 0.7380515, 0.6148897, 0.9181985, 0.96139705, 0.921875, 0.97794116, 1.0, 0.9696691, 1.0, 0.7830882, 0.9825368, 0.8391544, 0.9898897, 0.99172795, 1.0, 0.9981618, 0.8483456, 0.7628676, 0.7233456, 0.9007353, 0.9797794, 0.78768384, 0.9944853, 0.99080884, 0.7159926, 0.890625, 0.92922795, 1.0, 1.0, 1.0, 0.85294116, 0.9963235, 0.7169118, 0.9990809, 0.9200368, 0.9898897, 0.80422795, 0.94669116, 1.0, 0.7628676, 0.9586397, 0.9715074, 1.0, 0.90625, 0.9954044, 0.9981618, 0.97702205, 0.9659926, 0.984375, 0.984375, 1.0, 0.99264705, 0.9696691, 0.3805147, 0.9963235, 0.9889706, 0.8961397, 0.88419116, 0.9972426, 0.9090074, 0.953125, 0.9834559, 0.9990809, 0.9862132, 0.6424632, 0.8492647, 0.9200368, 0.9963235, 0.9889706, 0.99356616, 1.0, 1.0, 0.89981616, 0.8566176, 0.8694853, 0.8455882, 0.9898897, 0.94485295, 1.0, 0.9724265, 0.8933824, 0.99264705]\n",
      "i_s=295 in 500;  max_index=547307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.3802 - accuracy: 0.8961 - top_k_categorical_accuracy: 0.9871 - val_loss: 2.2090 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=296 in 500;  max_index=548607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.7019 - accuracy: 0.8309 - top_k_categorical_accuracy: 0.9816 - val_loss: 3.7897 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=297 in 500;  max_index=549907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.3655 - accuracy: 0.6912 - top_k_categorical_accuracy: 0.8824 - val_loss: 3.7492 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=298 in 500;  max_index=551207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5330 - accuracy: 0.8833 - top_k_categorical_accuracy: 0.9697 - val_loss: 3.0539 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=299 in 500;  max_index=552507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.2069 - accuracy: 0.6305 - top_k_categorical_accuracy: 0.9706 - val_loss: 1.8138 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=300 in 500;  max_index=553807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.4872 - accuracy: 0.8502 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.7903 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=301 in 500;  max_index=555107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.2617 - accuracy: 0.6654 - top_k_categorical_accuracy: 0.9476 - val_loss: 0.7492 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=302 in 500;  max_index=556407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.1340 - accuracy: 0.9798 - top_k_categorical_accuracy: 0.9982 - val_loss: 2.9180 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=303 in 500;  max_index=557707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.5231 - accuracy: 0.8649 - top_k_categorical_accuracy: 0.9752 - val_loss: 2.9245 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=304 in 500;  max_index=559007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.3189 - accuracy: 0.6507 - top_k_categorical_accuracy: 0.9430 - val_loss: 3.0452 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=305 in 500;  max_index=560307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.4610 - accuracy: 0.8860 - top_k_categorical_accuracy: 0.9770 - val_loss: 2.7169 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=306 in 500;  max_index=561607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 3.3657 - accuracy: 0.5423 - top_k_categorical_accuracy: 0.6691 - val_loss: 6.1001 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=307 in 500;  max_index=562907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.3684 - accuracy: 0.6085 - top_k_categorical_accuracy: 0.9072 - val_loss: 2.0309 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=308 in 500;  max_index=564207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.6839 - accuracy: 0.8061 - top_k_categorical_accuracy: 0.9743 - val_loss: 0.3314 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=309 in 500;  max_index=565507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 1.3683 - accuracy: 0.6471 - top_k_categorical_accuracy: 0.9347 - val_loss: 3.0376 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=310 in 500;  max_index=566807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.2494 - accuracy: 0.9182 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.0144 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=311 in 500;  max_index=568107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4523 - accuracy: 0.8575 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.8012 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=312 in 500;  max_index=569407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 2.7036 - accuracy: 0.4816 - top_k_categorical_accuracy: 0.7371 - val_loss: 4.9580 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=313 in 500;  max_index=570707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.5474 - accuracy: 0.8557 - top_k_categorical_accuracy: 0.9706 - val_loss: 4.4876 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=314 in 500;  max_index=572007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.4626 - accuracy: 0.8493 - top_k_categorical_accuracy: 0.9982 - val_loss: 2.2812 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=315 in 500;  max_index=573307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 3.8414 - accuracy: 0.0524 - top_k_categorical_accuracy: 0.4384 - val_loss: 4.6552 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=316 in 500;  max_index=574607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.9580 - accuracy: 0.7353 - top_k_categorical_accuracy: 0.9559 - val_loss: 3.4143 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=317 in 500;  max_index=575907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.7975 - accuracy: 0.7950 - top_k_categorical_accuracy: 0.9246 - val_loss: 0.7085 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=318 in 500;  max_index=577207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.0841 - accuracy: 0.9881 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.0127 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=319 in 500;  max_index=578507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.1707 - accuracy: 0.7224 - top_k_categorical_accuracy: 0.9844 - val_loss: 2.3859 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=320 in 500;  max_index=579807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.4816 - accuracy: 0.8631 - top_k_categorical_accuracy: 0.9972 - val_loss: 3.7165 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=321 in 500;  max_index=581107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.3420 - accuracy: 0.9154 - top_k_categorical_accuracy: 0.9871 - val_loss: 2.8395 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=322 in 500;  max_index=582407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.0118 - accuracy: 0.7426 - top_k_categorical_accuracy: 0.9550 - val_loss: 2.7175 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=323 in 500;  max_index=583707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.3797 - accuracy: 0.8566 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.8808 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=324 in 500;  max_index=585007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.3718 - accuracy: 0.8805 - top_k_categorical_accuracy: 0.9972 - val_loss: 1.6123 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=325 in 500;  max_index=586307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.3189 - accuracy: 0.9127 - top_k_categorical_accuracy: 0.9954 - val_loss: 1.2951 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=326 in 500;  max_index=587607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.2415 - accuracy: 0.9200 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.1981 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=327 in 500;  max_index=588907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 2.2181 - accuracy: 0.5432 - top_k_categorical_accuracy: 0.7987 - val_loss: 3.2096 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=328 in 500;  max_index=590207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.4199 - accuracy: 0.8741 - top_k_categorical_accuracy: 0.9972 - val_loss: 3.5177 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=329 in 500;  max_index=591507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3823 - accuracy: 0.8824 - top_k_categorical_accuracy: 0.9991 - val_loss: 2.5120 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=330 in 500;  max_index=592807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.2432 - accuracy: 0.9320 - top_k_categorical_accuracy: 0.9954 - val_loss: 1.6366 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=331 in 500;  max_index=594107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.7809 - accuracy: 0.7638 - top_k_categorical_accuracy: 0.9871 - val_loss: 3.1546 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=332 in 500;  max_index=595407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.3223 - accuracy: 0.6875 - top_k_categorical_accuracy: 0.9761 - val_loss: 2.7496 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=333 in 500;  max_index=596707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.0974 - accuracy: 0.9733 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0743 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=334 in 500;  max_index=598007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.1340 - accuracy: 0.6939 - top_k_categorical_accuracy: 0.9761 - val_loss: 0.8057 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=335 in 500;  max_index=599307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3880 - accuracy: 0.8879 - top_k_categorical_accuracy: 0.9982 - val_loss: 2.6779 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=336 in 500;  max_index=600607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 5.1698 - accuracy: 0.0901 - top_k_categorical_accuracy: 0.2950 - val_loss: 3.9156 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=337 in 500;  max_index=601907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.7901 - accuracy: 0.8153 - top_k_categorical_accuracy: 0.9320 - val_loss: 3.4353 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=338 in 500;  max_index=603207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.7510 - accuracy: 0.8097 - top_k_categorical_accuracy: 0.9596 - val_loss: 0.3611 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=339 in 500;  max_index=604507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5155 - accuracy: 0.8640 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.0917 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=340 in 500;  max_index=605807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.6065 - accuracy: 0.8695 - top_k_categorical_accuracy: 0.9972 - val_loss: 3.0712 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=341 in 500;  max_index=607107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.2466 - accuracy: 0.9182 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.0939 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=342 in 500;  max_index=608407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5097 - accuracy: 0.8438 - top_k_categorical_accuracy: 0.9963 - val_loss: 2.1006 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=343 in 500;  max_index=609707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 1.0384 - accuracy: 0.7537 - top_k_categorical_accuracy: 0.9798 - val_loss: 1.9495 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824, 0.8860294, 0.8079044, 0.8878676, 0.8869485, 0.4007353, 0.63419116, 0.85202205, 0.8722426, 0.9981618, 0.9632353, 0.83547795, 0.3961397, 0.77205884, 0.7398897, 0.8079044, 0.95955884, 0.67830884, 0.8400735, 0.8446691, 0.3272059, 0.7693015, 0.59375, 0.8805147, 0.7610294, 0.7049632, 0.91268384, 0.99264705, 0.8704044, 0.80422795, 0.97702205, 0.9485294, 0.9512868, 0.8235294, 0.9963235, 0.9990809, 0.9834559, 0.71231616, 0.6884191, 0.3970588, 0.984375, 0.7674632, 0.1985294, 0.96875, 0.5321691, 0.6884191, 0.8878676, 0.8253676, 0.7049632, 0.9338235, 0.58731616, 0.7352941, 0.67830884, 0.90625, 0.984375, 0.9954044, 0.8318015, 0.5772059, 0.89889705, 0.38327205, 0.2757353, 0.7637868, 0.8887868, 0.6930147, 0.8713235, 0.5726103, 0.7352941, 0.94485295, 0.83639705, 0.5496324, 0.984375, 0.66360295, 0.6994485, 0.9227941, 0.78768384, 0.9071691, 0.80514705, 0.5284926, 0.8878676, 0.7481618, 0.32444853, 0.8639706, 0.7398897, 0.8648897, 0.7159926, 0.32536766, 0.8272059, 0.390625, 0.77481616, 0.9632353, 0.7325368, 0.57077205, 0.75643384, 0.9209559, 0.8161765, 0.8933824, 0.9485294, 0.18933824, 0.36213234, 0.6847426, 0.94485295, 0.77297795, 0.7637868, 0.9568015, 0.5137868, 0.89705884, 0.5183824, 0.8887868, 0.3740809, 0.8602941, 0.3033088, 0.9898897, 0.9788603, 0.3492647, 0.78768384, 0.52297795, 0.4007353, 0.7472426, 0.85110295, 0.61580884, 0.7922794, 0.9659926, 0.734375, 0.95955884, 0.5744485, 0.8584559, 0.47242647, 0.9393382, 0.83547795, 0.9733456, 0.8584559, 0.6139706, 0.5670956, 0.37959558, 0.8023897, 0.8474265, 0.49724266, 0.8759191, 0.8694853, 0.3658088, 0.6865809, 0.7518382, 0.8897059, 0.90625, 0.8566176, 0.60202205, 0.8648897, 0.5110294, 0.9696691, 0.7472426, 0.9044118, 0.4990809, 0.7867647, 0.9494485, 0.58547795, 0.7444853, 0.8722426, 1.0, 0.7784926, 0.9319853, 0.9356618, 0.8428309, 0.80514705, 0.7693015, 0.8602941, 0.9485294, 0.8492647, 0.7693015, 0.0625, 0.9540441, 0.7867647, 0.5459559, 0.38235295, 0.953125, 0.74264705, 0.83731616, 0.91452205, 0.9411765, 0.9264706, 0.3961397, 0.6443015, 0.6755515, 0.9586397, 0.92830884, 0.9034926, 0.99080884, 0.8961397, 0.7977941, 0.6277574, 0.7325368, 0.54044116, 0.9246324, 0.49172795, 0.94485295, 0.8483456, 0.6286765, 0.9209559, 0.8961397, 0.8308824, 0.6911765, 0.88327205, 0.6305147, 0.85018384, 0.66544116, 0.9797794, 0.8648897, 0.6507353, 0.8860294, 0.5422794, 0.6084559, 0.80606616, 0.64705884, 0.9181985, 0.8575368, 0.48161766, 0.8556985, 0.8492647, 0.052389707, 0.7352941, 0.7950368, 0.9880515, 0.7224265, 0.8630515, 0.91544116, 0.74264705, 0.8566176, 0.8805147, 0.91268384, 0.9200368, 0.5431985, 0.8740809, 0.88235295, 0.9319853, 0.7637868, 0.6875, 0.9733456, 0.69393384, 0.8878676, 0.090073526, 0.8152574, 0.8097426, 0.8639706, 0.8694853, 0.9181985, 0.84375, 0.7536765]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324, 0.9990809, 0.9797794, 1.0, 0.9990809, 0.8538603, 0.97610295, 0.94577205, 0.9963235, 1.0, 1.0, 0.99264705, 0.6213235, 0.9209559, 0.890625, 0.9944853, 1.0, 0.9108456, 0.9834559, 0.9852941, 0.5818015, 0.99264705, 0.8161765, 0.97610295, 0.9411765, 0.9825368, 0.96875, 1.0, 0.9944853, 0.9944853, 0.9990809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9227941, 0.9705882, 0.6672794, 1.0, 0.9705882, 0.4227941, 0.9990809, 0.8428309, 0.8612132, 0.9733456, 0.9981618, 0.8722426, 0.9990809, 0.7628676, 0.9963235, 0.8878676, 0.99356616, 0.9981618, 1.0, 0.9586397, 0.8290441, 0.9963235, 0.6461397, 0.7058824, 0.91452205, 0.9862132, 0.9044118, 0.9696691, 0.74264705, 0.9209559, 0.99264705, 0.97518384, 0.85110295, 0.9990809, 0.8805147, 0.9273897, 0.9963235, 0.9963235, 1.0, 1.0, 0.86856616, 0.9963235, 0.9724265, 0.6976103, 0.9724265, 0.89797795, 0.9862132, 0.9494485, 0.6985294, 0.9568015, 0.6773897, 0.8887868, 0.9954044, 0.9577206, 0.7858456, 0.9568015, 0.99172795, 0.94577205, 0.9788603, 0.9990809, 0.4944853, 0.83455884, 0.875, 0.9972426, 0.99172795, 0.9797794, 0.9990809, 0.7987132, 0.9871324, 0.7233456, 0.9356618, 0.7619485, 0.9889706, 0.5441176, 1.0, 1.0, 0.6672794, 0.91360295, 0.7380515, 0.6148897, 0.9181985, 0.96139705, 0.921875, 0.97794116, 1.0, 0.9696691, 1.0, 0.7830882, 0.9825368, 0.8391544, 0.9898897, 0.99172795, 1.0, 0.9981618, 0.8483456, 0.7628676, 0.7233456, 0.9007353, 0.9797794, 0.78768384, 0.9944853, 0.99080884, 0.7159926, 0.890625, 0.92922795, 1.0, 1.0, 1.0, 0.85294116, 0.9963235, 0.7169118, 0.9990809, 0.9200368, 0.9898897, 0.80422795, 0.94669116, 1.0, 0.7628676, 0.9586397, 0.9715074, 1.0, 0.90625, 0.9954044, 0.9981618, 0.97702205, 0.9659926, 0.984375, 0.984375, 1.0, 0.99264705, 0.9696691, 0.3805147, 0.9963235, 0.9889706, 0.8961397, 0.88419116, 0.9972426, 0.9090074, 0.953125, 0.9834559, 0.9990809, 0.9862132, 0.6424632, 0.8492647, 0.9200368, 0.9963235, 0.9889706, 0.99356616, 1.0, 1.0, 0.89981616, 0.8566176, 0.8694853, 0.8455882, 0.9898897, 0.94485295, 1.0, 0.9724265, 0.8933824, 0.99264705, 0.9871324, 0.9816176, 0.88235295, 0.9696691, 0.9705882, 0.9990809, 0.9476103, 0.9981618, 0.97518384, 0.9430147, 0.97702205, 0.6691176, 0.9071691, 0.9742647, 0.9347426, 1.0, 0.9972426, 0.7371324, 0.9705882, 0.9981618, 0.4384191, 0.9558824, 0.9246324, 1.0, 0.984375, 0.9972426, 0.9871324, 0.9549632, 1.0, 0.9972426, 0.9954044, 1.0, 0.7987132, 0.9972426, 0.9990809, 0.9954044, 0.9871324, 0.97610295, 1.0, 0.97610295, 0.9981618, 0.29503676, 0.9319853, 0.95955884, 1.0, 0.9972426, 1.0, 0.9963235, 0.9797794]\n",
      "i_s=344 in 500;  max_index=611007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3334 - accuracy: 0.8925 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.8557 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=345 in 500;  max_index=612307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.2521 - accuracy: 0.9265 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2415 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=346 in 500;  max_index=613607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 3.7062 - accuracy: 0.2583 - top_k_categorical_accuracy: 0.6232 - val_loss: 5.8428 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=347 in 500;  max_index=614907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.0211 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.8927 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=348 in 500;  max_index=616207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.8555 - accuracy: 0.8125 - top_k_categorical_accuracy: 0.9807 - val_loss: 2.0596 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=349 in 500;  max_index=617507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.1816 - accuracy: 0.9403 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.5428 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=350 in 500;  max_index=618807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 1.1113 - accuracy: 0.7004 - top_k_categorical_accuracy: 0.9844 - val_loss: 3.4665 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=351 in 500;  max_index=620107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.6290 - accuracy: 0.8088 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.8495 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=352 in 500;  max_index=621407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.6161 - accuracy: 0.8447 - top_k_categorical_accuracy: 0.9917 - val_loss: 2.4257 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=353 in 500;  max_index=622707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.6405 - accuracy: 0.5744 - top_k_categorical_accuracy: 0.9256 - val_loss: 1.5396 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=354 in 500;  max_index=624007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.5703 - accuracy: 0.6921 - top_k_categorical_accuracy: 0.9182 - val_loss: 4.7178 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=355 in 500;  max_index=625307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.7351 - accuracy: 0.8079 - top_k_categorical_accuracy: 0.9357 - val_loss: 2.2292 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=356 in 500;  max_index=626607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 3.8911 - accuracy: 0.0046 - top_k_categorical_accuracy: 0.4963 - val_loss: 4.3370 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=357 in 500;  max_index=627907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.4813 - accuracy: 0.8952 - top_k_categorical_accuracy: 0.9669 - val_loss: 0.2061 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=358 in 500;  max_index=629207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.0236 - accuracy: 0.9991 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.1770 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=359 in 500;  max_index=630507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 1.3650 - accuracy: 0.7537 - top_k_categorical_accuracy: 0.9357 - val_loss: 3.7934 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=360 in 500;  max_index=631807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.4123 - accuracy: 0.8888 - top_k_categorical_accuracy: 0.9926 - val_loss: 1.5850 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=361 in 500;  max_index=633107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.6681 - accuracy: 0.8300 - top_k_categorical_accuracy: 0.9844 - val_loss: 0.6736 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=362 in 500;  max_index=634407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.6694 - accuracy: 0.8162 - top_k_categorical_accuracy: 0.9954 - val_loss: 2.0084 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=363 in 500;  max_index=635707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 2.9934 - accuracy: 0.3061 - top_k_categorical_accuracy: 0.6498 - val_loss: 3.2948 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=364 in 500;  max_index=637007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.3808 - accuracy: 0.6581 - top_k_categorical_accuracy: 0.8244 - val_loss: 2.6115 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=365 in 500;  max_index=638307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.7797 - accuracy: 0.7978 - top_k_categorical_accuracy: 0.9844 - val_loss: 2.5605 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=366 in 500;  max_index=639607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 3.0732 - accuracy: 0.2619 - top_k_categorical_accuracy: 0.5726 - val_loss: 3.6605 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=367 in 500;  max_index=640907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4984 - accuracy: 0.8631 - top_k_categorical_accuracy: 0.9743 - val_loss: 2.1090 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=368 in 500;  max_index=642207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.7715 - accuracy: 0.5119 - top_k_categorical_accuracy: 0.8263 - val_loss: 2.9493 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=369 in 500;  max_index=643507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 2.6863 - accuracy: 0.3961 - top_k_categorical_accuracy: 0.6415 - val_loss: 3.4368 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=370 in 500;  max_index=644807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.5795 - accuracy: 0.8529 - top_k_categorical_accuracy: 0.9586 - val_loss: 2.5409 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=371 in 500;  max_index=646107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.7367 - accuracy: 0.8189 - top_k_categorical_accuracy: 0.9200 - val_loss: 1.9348 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=372 in 500;  max_index=647407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.6945 - accuracy: 0.6186 - top_k_categorical_accuracy: 0.8143 - val_loss: 3.9131 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=373 in 500;  max_index=648707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.4539 - accuracy: 0.9320 - top_k_categorical_accuracy: 0.9881 - val_loss: 1.9320 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=374 in 500;  max_index=650007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.2622 - accuracy: 0.6847 - top_k_categorical_accuracy: 0.9228 - val_loss: 2.9869 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=375 in 500;  max_index=651307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.7886 - accuracy: 0.8153 - top_k_categorical_accuracy: 0.9347 - val_loss: 2.1024 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=376 in 500;  max_index=652607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.2925 - accuracy: 0.6792 - top_k_categorical_accuracy: 0.9347 - val_loss: 0.4538 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=377 in 500;  max_index=653907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 3.7179 - accuracy: 0.2279 - top_k_categorical_accuracy: 0.5349 - val_loss: 3.5971 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=378 in 500;  max_index=655207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.3662 - accuracy: 0.9292 - top_k_categorical_accuracy: 0.9715 - val_loss: 1.3672 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=379 in 500;  max_index=656507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.5009 - accuracy: 0.8529 - top_k_categorical_accuracy: 0.9982 - val_loss: 4.6225 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=380 in 500;  max_index=657807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.0258 - accuracy: 0.9963 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=381 in 500;  max_index=659107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.1680 - accuracy: 0.7206 - top_k_categorical_accuracy: 0.9375 - val_loss: 3.2944 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=382 in 500;  max_index=660407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 2.1242 - accuracy: 0.4246 - top_k_categorical_accuracy: 0.8667 - val_loss: 4.2702 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=383 in 500;  max_index=661707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 1.7646 - accuracy: 0.5855 - top_k_categorical_accuracy: 0.7996 - val_loss: 0.9561 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=384 in 500;  max_index=663007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4412 - accuracy: 0.8814 - top_k_categorical_accuracy: 0.9789 - val_loss: 2.1771 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=385 in 500;  max_index=664307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.0917 - accuracy: 0.9789 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.7986 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=386 in 500;  max_index=665607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.0896 - accuracy: 0.6847 - top_k_categorical_accuracy: 0.9752 - val_loss: 0.6994 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=387 in 500;  max_index=666907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 1.0698 - accuracy: 0.6801 - top_k_categorical_accuracy: 0.9963 - val_loss: 1.6876 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=388 in 500;  max_index=668207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.8859 - accuracy: 0.7454 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.3175 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=389 in 500;  max_index=669507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 3.1804 - accuracy: 0.3015 - top_k_categorical_accuracy: 0.7142 - val_loss: 3.5481 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=390 in 500;  max_index=670807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 1.5702 - accuracy: 0.6397 - top_k_categorical_accuracy: 0.7923 - val_loss: 1.2976 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=391 in 500;  max_index=672107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4276 - accuracy: 0.8805 - top_k_categorical_accuracy: 0.9972 - val_loss: 2.7168 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=392 in 500;  max_index=673407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4030 - accuracy: 0.8842 - top_k_categorical_accuracy: 0.9991 - val_loss: 2.5140 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824, 0.8860294, 0.8079044, 0.8878676, 0.8869485, 0.4007353, 0.63419116, 0.85202205, 0.8722426, 0.9981618, 0.9632353, 0.83547795, 0.3961397, 0.77205884, 0.7398897, 0.8079044, 0.95955884, 0.67830884, 0.8400735, 0.8446691, 0.3272059, 0.7693015, 0.59375, 0.8805147, 0.7610294, 0.7049632, 0.91268384, 0.99264705, 0.8704044, 0.80422795, 0.97702205, 0.9485294, 0.9512868, 0.8235294, 0.9963235, 0.9990809, 0.9834559, 0.71231616, 0.6884191, 0.3970588, 0.984375, 0.7674632, 0.1985294, 0.96875, 0.5321691, 0.6884191, 0.8878676, 0.8253676, 0.7049632, 0.9338235, 0.58731616, 0.7352941, 0.67830884, 0.90625, 0.984375, 0.9954044, 0.8318015, 0.5772059, 0.89889705, 0.38327205, 0.2757353, 0.7637868, 0.8887868, 0.6930147, 0.8713235, 0.5726103, 0.7352941, 0.94485295, 0.83639705, 0.5496324, 0.984375, 0.66360295, 0.6994485, 0.9227941, 0.78768384, 0.9071691, 0.80514705, 0.5284926, 0.8878676, 0.7481618, 0.32444853, 0.8639706, 0.7398897, 0.8648897, 0.7159926, 0.32536766, 0.8272059, 0.390625, 0.77481616, 0.9632353, 0.7325368, 0.57077205, 0.75643384, 0.9209559, 0.8161765, 0.8933824, 0.9485294, 0.18933824, 0.36213234, 0.6847426, 0.94485295, 0.77297795, 0.7637868, 0.9568015, 0.5137868, 0.89705884, 0.5183824, 0.8887868, 0.3740809, 0.8602941, 0.3033088, 0.9898897, 0.9788603, 0.3492647, 0.78768384, 0.52297795, 0.4007353, 0.7472426, 0.85110295, 0.61580884, 0.7922794, 0.9659926, 0.734375, 0.95955884, 0.5744485, 0.8584559, 0.47242647, 0.9393382, 0.83547795, 0.9733456, 0.8584559, 0.6139706, 0.5670956, 0.37959558, 0.8023897, 0.8474265, 0.49724266, 0.8759191, 0.8694853, 0.3658088, 0.6865809, 0.7518382, 0.8897059, 0.90625, 0.8566176, 0.60202205, 0.8648897, 0.5110294, 0.9696691, 0.7472426, 0.9044118, 0.4990809, 0.7867647, 0.9494485, 0.58547795, 0.7444853, 0.8722426, 1.0, 0.7784926, 0.9319853, 0.9356618, 0.8428309, 0.80514705, 0.7693015, 0.8602941, 0.9485294, 0.8492647, 0.7693015, 0.0625, 0.9540441, 0.7867647, 0.5459559, 0.38235295, 0.953125, 0.74264705, 0.83731616, 0.91452205, 0.9411765, 0.9264706, 0.3961397, 0.6443015, 0.6755515, 0.9586397, 0.92830884, 0.9034926, 0.99080884, 0.8961397, 0.7977941, 0.6277574, 0.7325368, 0.54044116, 0.9246324, 0.49172795, 0.94485295, 0.8483456, 0.6286765, 0.9209559, 0.8961397, 0.8308824, 0.6911765, 0.88327205, 0.6305147, 0.85018384, 0.66544116, 0.9797794, 0.8648897, 0.6507353, 0.8860294, 0.5422794, 0.6084559, 0.80606616, 0.64705884, 0.9181985, 0.8575368, 0.48161766, 0.8556985, 0.8492647, 0.052389707, 0.7352941, 0.7950368, 0.9880515, 0.7224265, 0.8630515, 0.91544116, 0.74264705, 0.8566176, 0.8805147, 0.91268384, 0.9200368, 0.5431985, 0.8740809, 0.88235295, 0.9319853, 0.7637868, 0.6875, 0.9733456, 0.69393384, 0.8878676, 0.090073526, 0.8152574, 0.8097426, 0.8639706, 0.8694853, 0.9181985, 0.84375, 0.7536765, 0.8924632, 0.9264706, 0.25827205, 1.0, 0.8125, 0.9402574, 0.7003676, 0.8088235, 0.8446691, 0.5744485, 0.6920956, 0.8079044, 0.0045955884, 0.8952206, 0.9990809, 0.7536765, 0.8887868, 0.8299632, 0.8161765, 0.3060662, 0.6580882, 0.7977941, 0.26194853, 0.8630515, 0.5119485, 0.3961397, 0.85294116, 0.81893384, 0.61856616, 0.9319853, 0.6847426, 0.8152574, 0.67922795, 0.22794117, 0.92922795, 0.85294116, 0.9963235, 0.7205882, 0.42463234, 0.58547795, 0.88143384, 0.9788603, 0.6847426, 0.68014705, 0.7454044, 0.30147058, 0.6397059, 0.8805147, 0.88419116]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324, 0.9990809, 0.9797794, 1.0, 0.9990809, 0.8538603, 0.97610295, 0.94577205, 0.9963235, 1.0, 1.0, 0.99264705, 0.6213235, 0.9209559, 0.890625, 0.9944853, 1.0, 0.9108456, 0.9834559, 0.9852941, 0.5818015, 0.99264705, 0.8161765, 0.97610295, 0.9411765, 0.9825368, 0.96875, 1.0, 0.9944853, 0.9944853, 0.9990809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9227941, 0.9705882, 0.6672794, 1.0, 0.9705882, 0.4227941, 0.9990809, 0.8428309, 0.8612132, 0.9733456, 0.9981618, 0.8722426, 0.9990809, 0.7628676, 0.9963235, 0.8878676, 0.99356616, 0.9981618, 1.0, 0.9586397, 0.8290441, 0.9963235, 0.6461397, 0.7058824, 0.91452205, 0.9862132, 0.9044118, 0.9696691, 0.74264705, 0.9209559, 0.99264705, 0.97518384, 0.85110295, 0.9990809, 0.8805147, 0.9273897, 0.9963235, 0.9963235, 1.0, 1.0, 0.86856616, 0.9963235, 0.9724265, 0.6976103, 0.9724265, 0.89797795, 0.9862132, 0.9494485, 0.6985294, 0.9568015, 0.6773897, 0.8887868, 0.9954044, 0.9577206, 0.7858456, 0.9568015, 0.99172795, 0.94577205, 0.9788603, 0.9990809, 0.4944853, 0.83455884, 0.875, 0.9972426, 0.99172795, 0.9797794, 0.9990809, 0.7987132, 0.9871324, 0.7233456, 0.9356618, 0.7619485, 0.9889706, 0.5441176, 1.0, 1.0, 0.6672794, 0.91360295, 0.7380515, 0.6148897, 0.9181985, 0.96139705, 0.921875, 0.97794116, 1.0, 0.9696691, 1.0, 0.7830882, 0.9825368, 0.8391544, 0.9898897, 0.99172795, 1.0, 0.9981618, 0.8483456, 0.7628676, 0.7233456, 0.9007353, 0.9797794, 0.78768384, 0.9944853, 0.99080884, 0.7159926, 0.890625, 0.92922795, 1.0, 1.0, 1.0, 0.85294116, 0.9963235, 0.7169118, 0.9990809, 0.9200368, 0.9898897, 0.80422795, 0.94669116, 1.0, 0.7628676, 0.9586397, 0.9715074, 1.0, 0.90625, 0.9954044, 0.9981618, 0.97702205, 0.9659926, 0.984375, 0.984375, 1.0, 0.99264705, 0.9696691, 0.3805147, 0.9963235, 0.9889706, 0.8961397, 0.88419116, 0.9972426, 0.9090074, 0.953125, 0.9834559, 0.9990809, 0.9862132, 0.6424632, 0.8492647, 0.9200368, 0.9963235, 0.9889706, 0.99356616, 1.0, 1.0, 0.89981616, 0.8566176, 0.8694853, 0.8455882, 0.9898897, 0.94485295, 1.0, 0.9724265, 0.8933824, 0.99264705, 0.9871324, 0.9816176, 0.88235295, 0.9696691, 0.9705882, 0.9990809, 0.9476103, 0.9981618, 0.97518384, 0.9430147, 0.97702205, 0.6691176, 0.9071691, 0.9742647, 0.9347426, 1.0, 0.9972426, 0.7371324, 0.9705882, 0.9981618, 0.4384191, 0.9558824, 0.9246324, 1.0, 0.984375, 0.9972426, 0.9871324, 0.9549632, 1.0, 0.9972426, 0.9954044, 1.0, 0.7987132, 0.9972426, 0.9990809, 0.9954044, 0.9871324, 0.97610295, 1.0, 0.97610295, 0.9981618, 0.29503676, 0.9319853, 0.95955884, 1.0, 0.9972426, 1.0, 0.9963235, 0.9797794, 1.0, 1.0, 0.6231618, 1.0, 0.9806985, 1.0, 0.984375, 1.0, 0.99172795, 0.9255515, 0.9181985, 0.9356618, 0.49632353, 0.9669118, 1.0, 0.9356618, 0.99264705, 0.984375, 0.9954044, 0.64981616, 0.8244485, 0.984375, 0.5726103, 0.9742647, 0.8262868, 0.6415441, 0.9586397, 0.9200368, 0.8143382, 0.9880515, 0.9227941, 0.9347426, 0.9347426, 0.5349265, 0.9715074, 0.9981618, 1.0, 0.9375, 0.86672795, 0.7996324, 0.9788603, 1.0, 0.97518384, 0.9963235, 0.9990809, 0.7141544, 0.7922794, 0.9972426, 0.9990809]\n",
      "i_s=393 in 500;  max_index=674707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.8127 - accuracy: 0.7904 - top_k_categorical_accuracy: 0.9862 - val_loss: 2.1445 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=394 in 500;  max_index=676007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.8569 - accuracy: 0.6112 - top_k_categorical_accuracy: 0.8603 - val_loss: 3.0894 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=395 in 500;  max_index=677307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.7382 - accuracy: 0.7932 - top_k_categorical_accuracy: 0.9697 - val_loss: 1.5637 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=396 in 500;  max_index=678607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.2797 - accuracy: 0.9154 - top_k_categorical_accuracy: 0.9982 - val_loss: 1.8707 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=397 in 500;  max_index=679907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 3.8440 - accuracy: 0.1756 - top_k_categorical_accuracy: 0.4568 - val_loss: 4.9305 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=398 in 500;  max_index=681207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.0772 - accuracy: 0.9835 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0448 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=399 in 500;  max_index=682507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4447 - accuracy: 0.8869 - top_k_categorical_accuracy: 0.9954 - val_loss: 3.7469 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=400 in 500;  max_index=683807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.2051 - accuracy: 0.9403 - top_k_categorical_accuracy: 0.9982 - val_loss: 2.4967 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=401 in 500;  max_index=685107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 2.3276 - accuracy: 0.4807 - top_k_categorical_accuracy: 0.8557 - val_loss: 1.9793 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=402 in 500;  max_index=686407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.3148 - accuracy: 0.9274 - top_k_categorical_accuracy: 0.9954 - val_loss: 2.3651 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=403 in 500;  max_index=687707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.1006 - accuracy: 0.9770 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.1065 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=404 in 500;  max_index=689007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.4020 - accuracy: 0.8860 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.4266 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=405 in 500;  max_index=690307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.5203 - accuracy: 0.8199 - top_k_categorical_accuracy: 0.9972 - val_loss: 3.3702 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=406 in 500;  max_index=691607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 2.1466 - accuracy: 0.5515 - top_k_categorical_accuracy: 0.7601 - val_loss: 4.1494 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=407 in 500;  max_index=692907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.3470 - accuracy: 0.6756 - top_k_categorical_accuracy: 0.8612 - val_loss: 2.5687 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=408 in 500;  max_index=694207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.4856 - accuracy: 0.8713 - top_k_categorical_accuracy: 0.9715 - val_loss: 0.3867 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=409 in 500;  max_index=695507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.0389 - accuracy: 0.7188 - top_k_categorical_accuracy: 0.9623 - val_loss: 3.1839 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=410 in 500;  max_index=696807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.3301 - accuracy: 0.9210 - top_k_categorical_accuracy: 0.9954 - val_loss: 1.7415 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=411 in 500;  max_index=698107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.5899 - accuracy: 0.8336 - top_k_categorical_accuracy: 0.9761 - val_loss: 2.4033 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=412 in 500;  max_index=699407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.1699 - accuracy: 0.9504 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3110 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=413 in 500;  max_index=700707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.0470 - accuracy: 0.9899 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0420 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=414 in 500;  max_index=702007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.2746 - accuracy: 0.7325 - top_k_categorical_accuracy: 0.9072 - val_loss: 4.0400 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=415 in 500;  max_index=703307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.6530 - accuracy: 0.5754 - top_k_categorical_accuracy: 0.8732 - val_loss: 2.4111 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=416 in 500;  max_index=704607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.5094 - accuracy: 0.8346 - top_k_categorical_accuracy: 0.9936 - val_loss: 2.0064 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=417 in 500;  max_index=705907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 2.4904 - accuracy: 0.4256 - top_k_categorical_accuracy: 0.6673 - val_loss: 6.2317 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=418 in 500;  max_index=707207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.1668 - accuracy: 0.6498 - top_k_categorical_accuracy: 0.9145 - val_loss: 2.9539 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=419 in 500;  max_index=708507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 2.0601 - accuracy: 0.4752 - top_k_categorical_accuracy: 0.8199 - val_loss: 3.7885 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=420 in 500;  max_index=709807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.5356 - accuracy: 0.8667 - top_k_categorical_accuracy: 0.9761 - val_loss: 0.7380 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=421 in 500;  max_index=711107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.8613 - accuracy: 0.7500 - top_k_categorical_accuracy: 0.9449 - val_loss: 1.7461 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=422 in 500;  max_index=712407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.2851 - accuracy: 0.6526 - top_k_categorical_accuracy: 0.9357 - val_loss: 1.1802 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=423 in 500;  max_index=713707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 3.1103 - accuracy: 0.3051 - top_k_categorical_accuracy: 0.6213 - val_loss: 3.6168 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=424 in 500;  max_index=715007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.5172 - accuracy: 0.8566 - top_k_categorical_accuracy: 0.9963 - val_loss: 2.8109 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=425 in 500;  max_index=716307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.7795 - accuracy: 0.7950 - top_k_categorical_accuracy: 0.9596 - val_loss: 4.7656 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=426 in 500;  max_index=717607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.4882 - accuracy: 0.6388 - top_k_categorical_accuracy: 0.8364 - val_loss: 3.5003 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=427 in 500;  max_index=718907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4314 - accuracy: 0.8925 - top_k_categorical_accuracy: 0.9715 - val_loss: 1.3107 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=428 in 500;  max_index=720207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.6335 - accuracy: 0.8116 - top_k_categorical_accuracy: 0.9697 - val_loss: 2.1115 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=429 in 500;  max_index=721507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.4557 - accuracy: 0.6884 - top_k_categorical_accuracy: 0.8695 - val_loss: 3.8493 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=430 in 500;  max_index=722807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.2249 - accuracy: 0.9504 - top_k_categorical_accuracy: 0.9972 - val_loss: 1.9574 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=431 in 500;  max_index=724107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.8632 - accuracy: 0.7675 - top_k_categorical_accuracy: 0.9770 - val_loss: 2.3326 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=432 in 500;  max_index=725407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.0805 - accuracy: 0.9862 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.7467 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=433 in 500;  max_index=726707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5193 - accuracy: 0.8704 - top_k_categorical_accuracy: 0.9890 - val_loss: 1.0228 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=434 in 500;  max_index=728007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.1453 - accuracy: 0.9522 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.7388 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=435 in 500;  max_index=729307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.0090 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4396 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=436 in 500;  max_index=730607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.0523 - accuracy: 0.9825 - top_k_categorical_accuracy: 0.9991 - val_loss: 0.2579 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=437 in 500;  max_index=731907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.0279 - accuracy: 0.9926 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3383 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=438 in 500;  max_index=733207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.1604 - accuracy: 0.7868 - top_k_categorical_accuracy: 0.9982 - val_loss: 4.9404 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=439 in 500;  max_index=734507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.9535 - accuracy: 0.7445 - top_k_categorical_accuracy: 0.9972 - val_loss: 2.0034 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=440 in 500;  max_index=735807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.0914 - accuracy: 0.9715 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.4169 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=441 in 500;  max_index=737107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5031 - accuracy: 0.8364 - top_k_categorical_accuracy: 0.9991 - val_loss: 1.7494 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824, 0.8860294, 0.8079044, 0.8878676, 0.8869485, 0.4007353, 0.63419116, 0.85202205, 0.8722426, 0.9981618, 0.9632353, 0.83547795, 0.3961397, 0.77205884, 0.7398897, 0.8079044, 0.95955884, 0.67830884, 0.8400735, 0.8446691, 0.3272059, 0.7693015, 0.59375, 0.8805147, 0.7610294, 0.7049632, 0.91268384, 0.99264705, 0.8704044, 0.80422795, 0.97702205, 0.9485294, 0.9512868, 0.8235294, 0.9963235, 0.9990809, 0.9834559, 0.71231616, 0.6884191, 0.3970588, 0.984375, 0.7674632, 0.1985294, 0.96875, 0.5321691, 0.6884191, 0.8878676, 0.8253676, 0.7049632, 0.9338235, 0.58731616, 0.7352941, 0.67830884, 0.90625, 0.984375, 0.9954044, 0.8318015, 0.5772059, 0.89889705, 0.38327205, 0.2757353, 0.7637868, 0.8887868, 0.6930147, 0.8713235, 0.5726103, 0.7352941, 0.94485295, 0.83639705, 0.5496324, 0.984375, 0.66360295, 0.6994485, 0.9227941, 0.78768384, 0.9071691, 0.80514705, 0.5284926, 0.8878676, 0.7481618, 0.32444853, 0.8639706, 0.7398897, 0.8648897, 0.7159926, 0.32536766, 0.8272059, 0.390625, 0.77481616, 0.9632353, 0.7325368, 0.57077205, 0.75643384, 0.9209559, 0.8161765, 0.8933824, 0.9485294, 0.18933824, 0.36213234, 0.6847426, 0.94485295, 0.77297795, 0.7637868, 0.9568015, 0.5137868, 0.89705884, 0.5183824, 0.8887868, 0.3740809, 0.8602941, 0.3033088, 0.9898897, 0.9788603, 0.3492647, 0.78768384, 0.52297795, 0.4007353, 0.7472426, 0.85110295, 0.61580884, 0.7922794, 0.9659926, 0.734375, 0.95955884, 0.5744485, 0.8584559, 0.47242647, 0.9393382, 0.83547795, 0.9733456, 0.8584559, 0.6139706, 0.5670956, 0.37959558, 0.8023897, 0.8474265, 0.49724266, 0.8759191, 0.8694853, 0.3658088, 0.6865809, 0.7518382, 0.8897059, 0.90625, 0.8566176, 0.60202205, 0.8648897, 0.5110294, 0.9696691, 0.7472426, 0.9044118, 0.4990809, 0.7867647, 0.9494485, 0.58547795, 0.7444853, 0.8722426, 1.0, 0.7784926, 0.9319853, 0.9356618, 0.8428309, 0.80514705, 0.7693015, 0.8602941, 0.9485294, 0.8492647, 0.7693015, 0.0625, 0.9540441, 0.7867647, 0.5459559, 0.38235295, 0.953125, 0.74264705, 0.83731616, 0.91452205, 0.9411765, 0.9264706, 0.3961397, 0.6443015, 0.6755515, 0.9586397, 0.92830884, 0.9034926, 0.99080884, 0.8961397, 0.7977941, 0.6277574, 0.7325368, 0.54044116, 0.9246324, 0.49172795, 0.94485295, 0.8483456, 0.6286765, 0.9209559, 0.8961397, 0.8308824, 0.6911765, 0.88327205, 0.6305147, 0.85018384, 0.66544116, 0.9797794, 0.8648897, 0.6507353, 0.8860294, 0.5422794, 0.6084559, 0.80606616, 0.64705884, 0.9181985, 0.8575368, 0.48161766, 0.8556985, 0.8492647, 0.052389707, 0.7352941, 0.7950368, 0.9880515, 0.7224265, 0.8630515, 0.91544116, 0.74264705, 0.8566176, 0.8805147, 0.91268384, 0.9200368, 0.5431985, 0.8740809, 0.88235295, 0.9319853, 0.7637868, 0.6875, 0.9733456, 0.69393384, 0.8878676, 0.090073526, 0.8152574, 0.8097426, 0.8639706, 0.8694853, 0.9181985, 0.84375, 0.7536765, 0.8924632, 0.9264706, 0.25827205, 1.0, 0.8125, 0.9402574, 0.7003676, 0.8088235, 0.8446691, 0.5744485, 0.6920956, 0.8079044, 0.0045955884, 0.8952206, 0.9990809, 0.7536765, 0.8887868, 0.8299632, 0.8161765, 0.3060662, 0.6580882, 0.7977941, 0.26194853, 0.8630515, 0.5119485, 0.3961397, 0.85294116, 0.81893384, 0.61856616, 0.9319853, 0.6847426, 0.8152574, 0.67922795, 0.22794117, 0.92922795, 0.85294116, 0.9963235, 0.7205882, 0.42463234, 0.58547795, 0.88143384, 0.9788603, 0.6847426, 0.68014705, 0.7454044, 0.30147058, 0.6397059, 0.8805147, 0.88419116, 0.79044116, 0.6112132, 0.7931985, 0.91544116, 0.17555147, 0.9834559, 0.8869485, 0.9402574, 0.48069853, 0.9273897, 0.97702205, 0.8860294, 0.81985295, 0.5514706, 0.6755515, 0.8713235, 0.71875, 0.9209559, 0.8336397, 0.9503676, 0.9898897, 0.7325368, 0.5753676, 0.83455884, 0.42555147, 0.64981616, 0.4751838, 0.86672795, 0.75, 0.6525735, 0.30514705, 0.8566176, 0.7950368, 0.6387868, 0.8924632, 0.8115809, 0.6884191, 0.9503676, 0.7674632, 0.9862132, 0.8704044, 0.9522059, 1.0, 0.9825368, 0.99264705, 0.7867647, 0.7444853, 0.9715074, 0.83639705]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324, 0.9990809, 0.9797794, 1.0, 0.9990809, 0.8538603, 0.97610295, 0.94577205, 0.9963235, 1.0, 1.0, 0.99264705, 0.6213235, 0.9209559, 0.890625, 0.9944853, 1.0, 0.9108456, 0.9834559, 0.9852941, 0.5818015, 0.99264705, 0.8161765, 0.97610295, 0.9411765, 0.9825368, 0.96875, 1.0, 0.9944853, 0.9944853, 0.9990809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9227941, 0.9705882, 0.6672794, 1.0, 0.9705882, 0.4227941, 0.9990809, 0.8428309, 0.8612132, 0.9733456, 0.9981618, 0.8722426, 0.9990809, 0.7628676, 0.9963235, 0.8878676, 0.99356616, 0.9981618, 1.0, 0.9586397, 0.8290441, 0.9963235, 0.6461397, 0.7058824, 0.91452205, 0.9862132, 0.9044118, 0.9696691, 0.74264705, 0.9209559, 0.99264705, 0.97518384, 0.85110295, 0.9990809, 0.8805147, 0.9273897, 0.9963235, 0.9963235, 1.0, 1.0, 0.86856616, 0.9963235, 0.9724265, 0.6976103, 0.9724265, 0.89797795, 0.9862132, 0.9494485, 0.6985294, 0.9568015, 0.6773897, 0.8887868, 0.9954044, 0.9577206, 0.7858456, 0.9568015, 0.99172795, 0.94577205, 0.9788603, 0.9990809, 0.4944853, 0.83455884, 0.875, 0.9972426, 0.99172795, 0.9797794, 0.9990809, 0.7987132, 0.9871324, 0.7233456, 0.9356618, 0.7619485, 0.9889706, 0.5441176, 1.0, 1.0, 0.6672794, 0.91360295, 0.7380515, 0.6148897, 0.9181985, 0.96139705, 0.921875, 0.97794116, 1.0, 0.9696691, 1.0, 0.7830882, 0.9825368, 0.8391544, 0.9898897, 0.99172795, 1.0, 0.9981618, 0.8483456, 0.7628676, 0.7233456, 0.9007353, 0.9797794, 0.78768384, 0.9944853, 0.99080884, 0.7159926, 0.890625, 0.92922795, 1.0, 1.0, 1.0, 0.85294116, 0.9963235, 0.7169118, 0.9990809, 0.9200368, 0.9898897, 0.80422795, 0.94669116, 1.0, 0.7628676, 0.9586397, 0.9715074, 1.0, 0.90625, 0.9954044, 0.9981618, 0.97702205, 0.9659926, 0.984375, 0.984375, 1.0, 0.99264705, 0.9696691, 0.3805147, 0.9963235, 0.9889706, 0.8961397, 0.88419116, 0.9972426, 0.9090074, 0.953125, 0.9834559, 0.9990809, 0.9862132, 0.6424632, 0.8492647, 0.9200368, 0.9963235, 0.9889706, 0.99356616, 1.0, 1.0, 0.89981616, 0.8566176, 0.8694853, 0.8455882, 0.9898897, 0.94485295, 1.0, 0.9724265, 0.8933824, 0.99264705, 0.9871324, 0.9816176, 0.88235295, 0.9696691, 0.9705882, 0.9990809, 0.9476103, 0.9981618, 0.97518384, 0.9430147, 0.97702205, 0.6691176, 0.9071691, 0.9742647, 0.9347426, 1.0, 0.9972426, 0.7371324, 0.9705882, 0.9981618, 0.4384191, 0.9558824, 0.9246324, 1.0, 0.984375, 0.9972426, 0.9871324, 0.9549632, 1.0, 0.9972426, 0.9954044, 1.0, 0.7987132, 0.9972426, 0.9990809, 0.9954044, 0.9871324, 0.97610295, 1.0, 0.97610295, 0.9981618, 0.29503676, 0.9319853, 0.95955884, 1.0, 0.9972426, 1.0, 0.9963235, 0.9797794, 1.0, 1.0, 0.6231618, 1.0, 0.9806985, 1.0, 0.984375, 1.0, 0.99172795, 0.9255515, 0.9181985, 0.9356618, 0.49632353, 0.9669118, 1.0, 0.9356618, 0.99264705, 0.984375, 0.9954044, 0.64981616, 0.8244485, 0.984375, 0.5726103, 0.9742647, 0.8262868, 0.6415441, 0.9586397, 0.9200368, 0.8143382, 0.9880515, 0.9227941, 0.9347426, 0.9347426, 0.5349265, 0.9715074, 0.9981618, 1.0, 0.9375, 0.86672795, 0.7996324, 0.9788603, 1.0, 0.97518384, 0.9963235, 0.9990809, 0.7141544, 0.7922794, 0.9972426, 0.9990809, 0.9862132, 0.8602941, 0.9696691, 0.9981618, 0.45680147, 1.0, 0.9954044, 0.9981618, 0.8556985, 0.9954044, 1.0, 1.0, 0.9972426, 0.7601103, 0.8612132, 0.9715074, 0.96231616, 0.9954044, 0.97610295, 1.0, 1.0, 0.9071691, 0.8731618, 0.99356616, 0.6672794, 0.91452205, 0.81985295, 0.97610295, 0.94485295, 0.9356618, 0.6213235, 0.9963235, 0.95955884, 0.83639705, 0.9715074, 0.9696691, 0.8694853, 0.9972426, 0.97702205, 1.0, 0.9889706, 1.0, 1.0, 0.9990809, 1.0, 0.9981618, 0.9972426, 1.0, 0.9990809]\n",
      "i_s=442 in 500;  max_index=738407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.1354 - accuracy: 0.9531 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5740 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=443 in 500;  max_index=739707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.9652 - accuracy: 0.7307 - top_k_categorical_accuracy: 0.9936 - val_loss: 3.1995 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=444 in 500;  max_index=741007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.0405 - accuracy: 0.9890 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.8101 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=445 in 500;  max_index=742307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.1657 - accuracy: 0.9467 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.0440 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=446 in 500;  max_index=743607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 49s 3s/step - loss: 2.0705 - accuracy: 0.5487 - top_k_categorical_accuracy: 0.8428 - val_loss: 4.3543 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=447 in 500;  max_index=744907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5702 - accuracy: 0.8539 - top_k_categorical_accuracy: 0.9605 - val_loss: 0.4958 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=448 in 500;  max_index=746207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 2.1928 - accuracy: 0.4660 - top_k_categorical_accuracy: 0.8879 - val_loss: 3.7283 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=449 in 500;  max_index=747507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 3.6732 - accuracy: 0.1728 - top_k_categorical_accuracy: 0.4743 - val_loss: 3.8655 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=450 in 500;  max_index=748807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.6726 - accuracy: 0.8373 - top_k_categorical_accuracy: 0.9430 - val_loss: 1.4989 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=451 in 500;  max_index=750107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.0951 - accuracy: 0.9862 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3864 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=452 in 500;  max_index=751407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.2986 - accuracy: 0.6710 - top_k_categorical_accuracy: 0.9651 - val_loss: 1.0316 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=453 in 500;  max_index=752707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.9219 - accuracy: 0.5414 - top_k_categorical_accuracy: 0.8079 - val_loss: 3.2994 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=454 in 500;  max_index=754007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3604 - accuracy: 0.8961 - top_k_categorical_accuracy: 0.9862 - val_loss: 1.9179 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=455 in 500;  max_index=755307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 4.8689 - accuracy: 0.0092 - top_k_categorical_accuracy: 0.1994 - val_loss: 5.8493 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=456 in 500;  max_index=756607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 2.4734 - accuracy: 0.4256 - top_k_categorical_accuracy: 0.7206 - val_loss: 4.2666 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=457 in 500;  max_index=757907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.0942 - accuracy: 0.7426 - top_k_categorical_accuracy: 0.8915 - val_loss: 1.7977 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=458 in 500;  max_index=759207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.8792 - accuracy: 0.4697 - top_k_categorical_accuracy: 0.8529 - val_loss: 3.7887 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=459 in 500;  max_index=760507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.3181 - accuracy: 0.9779 - top_k_categorical_accuracy: 0.9991 - val_loss: 3.8503 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=460 in 500;  max_index=761807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.9059 - accuracy: 0.8070 - top_k_categorical_accuracy: 0.8989 - val_loss: 0.5900 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=461 in 500;  max_index=763107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.1514 - accuracy: 0.6480 - top_k_categorical_accuracy: 0.9789 - val_loss: 1.8411 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=462 in 500;  max_index=764407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.9229 - accuracy: 0.7335 - top_k_categorical_accuracy: 0.9476 - val_loss: 2.2086 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=463 in 500;  max_index=765707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 3.4222 - accuracy: 0.2426 - top_k_categorical_accuracy: 0.6425 - val_loss: 3.9935 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=464 in 500;  max_index=767007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 3.7289 - accuracy: 9.1912e-04 - top_k_categorical_accuracy: 0.4200 - val_loss: 4.6596 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=465 in 500;  max_index=768307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 1.9330 - accuracy: 0.5349 - top_k_categorical_accuracy: 0.7812 - val_loss: 3.2478 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=466 in 500;  max_index=769607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.6939 - accuracy: 0.8116 - top_k_categorical_accuracy: 0.9568 - val_loss: 1.3799 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=467 in 500;  max_index=770907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.0564 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.0962 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=468 in 500;  max_index=772207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.3341 - accuracy: 0.6259 - top_k_categorical_accuracy: 0.9596 - val_loss: 0.3301 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=469 in 500;  max_index=773507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 2.1850 - accuracy: 0.5046 - top_k_categorical_accuracy: 0.7344 - val_loss: 4.9574 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=470 in 500;  max_index=774807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 1.1556 - accuracy: 0.6949 - top_k_categorical_accuracy: 0.8925 - val_loss: 2.6644 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=471 in 500;  max_index=776107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.8442 - accuracy: 0.7528 - top_k_categorical_accuracy: 0.9439 - val_loss: 1.8445 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=472 in 500;  max_index=777407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 1.5863 - accuracy: 0.6167 - top_k_categorical_accuracy: 0.8456 - val_loss: 3.5447 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=473 in 500;  max_index=778707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.9634 - accuracy: 0.7215 - top_k_categorical_accuracy: 0.9439 - val_loss: 1.1431 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=474 in 500;  max_index=780007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.8610 - accuracy: 0.5689 - top_k_categorical_accuracy: 0.7583 - val_loss: 3.8878 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=475 in 500;  max_index=781307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.9408 - accuracy: 0.7426 - top_k_categorical_accuracy: 0.9219 - val_loss: 2.3829 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=476 in 500;  max_index=782607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4468 - accuracy: 0.8704 - top_k_categorical_accuracy: 0.9963 - val_loss: 2.4037 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=477 in 500;  max_index=783907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 1.8582 - accuracy: 0.6443 - top_k_categorical_accuracy: 0.8465 - val_loss: 3.9020 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=478 in 500;  max_index=785207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.4207 - accuracy: 0.9017 - top_k_categorical_accuracy: 0.9770 - val_loss: 2.4522 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=479 in 500;  max_index=786507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4297 - accuracy: 0.8759 - top_k_categorical_accuracy: 0.9926 - val_loss: 1.1133 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=480 in 500;  max_index=787807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.9246 - accuracy: 0.5414 - top_k_categorical_accuracy: 0.8566 - val_loss: 3.9272 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=481 in 500;  max_index=789107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.6629 - accuracy: 0.8107 - top_k_categorical_accuracy: 0.9752 - val_loss: 0.2937 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=482 in 500;  max_index=790407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 50s 3s/step - loss: 0.1160 - accuracy: 0.9733 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.5858 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=483 in 500;  max_index=791707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.0677 - accuracy: 0.7050 - top_k_categorical_accuracy: 0.9439 - val_loss: 2.8119 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=484 in 500;  max_index=793007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.6175 - accuracy: 0.8483 - top_k_categorical_accuracy: 0.9908 - val_loss: 4.0538 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=485 in 500;  max_index=794307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 47s 3s/step - loss: 2.6095 - accuracy: 0.4219 - top_k_categorical_accuracy: 0.6884 - val_loss: 4.3616 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=486 in 500;  max_index=795607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.3267 - accuracy: 0.9173 - top_k_categorical_accuracy: 0.9991 - val_loss: 2.4409 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=487 in 500;  max_index=796907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.3363 - accuracy: 0.9145 - top_k_categorical_accuracy: 0.9936 - val_loss: 1.8749 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=488 in 500;  max_index=798207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 54s 3s/step - loss: 1.0518 - accuracy: 0.7142 - top_k_categorical_accuracy: 0.9329 - val_loss: 2.1965 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=489 in 500;  max_index=799507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 2.5347 - accuracy: 0.3428 - top_k_categorical_accuracy: 0.7583 - val_loss: 5.3957 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=490 in 500;  max_index=800807\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5081 - accuracy: 0.8594 - top_k_categorical_accuracy: 0.9706 - val_loss: 2.3680 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.9393382, 0.99172795, 0.4632353, 0.15441176, 0.5303309, 0.9071691, 0.8943015, 0.42647058, 0.4540441, 0.89981616, 0.53768384, 0.8391544, 0.53768384, 0.9641544, 0.3125, 0.25, 0.6709559, 0.36397058, 0.5946691, 0.890625, 0.5450368, 0.890625, 0.45772058, 0.71047795, 0.7196691, 0.7077206, 0.5027574, 0.5615809, 0.7766544, 0.86580884, 0.74264705, 0.58455884, 0.8777574, 0.8152574, 0.8069853, 0.9954044, 0.5349265, 0.9007353, 0.17003676, 0.44117647, 0.8943015, 0.7803309, 0.77297795, 0.34007353, 0.88419116, 0.9972426, 0.72702205, 0.9430147, 0.6066176, 0.55330884, 0.8005515, 0.8878676, 0.8327206, 0.6599265, 0.91452205, 0.50735295, 0.6360294, 0.15716912, 0.6507353, 0.2821691, 0.7297794, 0.8694853, 0.31985295, 0.53768384, 0.78125, 0.5045956, 0.9090074, 0.52205884, 0.9246324, 0.9981618, 0.7830882, 0.7784926, 0.81985295, 0.9246324, 0.9356618, 0.8079044, 0.7913603, 0.9632353, 0.48713234, 0.56893384, 0.80606616, 0.6691176, 0.77481616, 0.68014705, 0.765625, 0.74356616, 0.7683824, 0.859375, 0.9650735, 0.7132353, 0.9696691, 0.6755515, 0.69577205, 0.6102941, 0.953125, 0.38235295, 0.41452205, 0.89705884, 0.7683824, 0.8860294, 0.8079044, 0.8878676, 0.8869485, 0.4007353, 0.63419116, 0.85202205, 0.8722426, 0.9981618, 0.9632353, 0.83547795, 0.3961397, 0.77205884, 0.7398897, 0.8079044, 0.95955884, 0.67830884, 0.8400735, 0.8446691, 0.3272059, 0.7693015, 0.59375, 0.8805147, 0.7610294, 0.7049632, 0.91268384, 0.99264705, 0.8704044, 0.80422795, 0.97702205, 0.9485294, 0.9512868, 0.8235294, 0.9963235, 0.9990809, 0.9834559, 0.71231616, 0.6884191, 0.3970588, 0.984375, 0.7674632, 0.1985294, 0.96875, 0.5321691, 0.6884191, 0.8878676, 0.8253676, 0.7049632, 0.9338235, 0.58731616, 0.7352941, 0.67830884, 0.90625, 0.984375, 0.9954044, 0.8318015, 0.5772059, 0.89889705, 0.38327205, 0.2757353, 0.7637868, 0.8887868, 0.6930147, 0.8713235, 0.5726103, 0.7352941, 0.94485295, 0.83639705, 0.5496324, 0.984375, 0.66360295, 0.6994485, 0.9227941, 0.78768384, 0.9071691, 0.80514705, 0.5284926, 0.8878676, 0.7481618, 0.32444853, 0.8639706, 0.7398897, 0.8648897, 0.7159926, 0.32536766, 0.8272059, 0.390625, 0.77481616, 0.9632353, 0.7325368, 0.57077205, 0.75643384, 0.9209559, 0.8161765, 0.8933824, 0.9485294, 0.18933824, 0.36213234, 0.6847426, 0.94485295, 0.77297795, 0.7637868, 0.9568015, 0.5137868, 0.89705884, 0.5183824, 0.8887868, 0.3740809, 0.8602941, 0.3033088, 0.9898897, 0.9788603, 0.3492647, 0.78768384, 0.52297795, 0.4007353, 0.7472426, 0.85110295, 0.61580884, 0.7922794, 0.9659926, 0.734375, 0.95955884, 0.5744485, 0.8584559, 0.47242647, 0.9393382, 0.83547795, 0.9733456, 0.8584559, 0.6139706, 0.5670956, 0.37959558, 0.8023897, 0.8474265, 0.49724266, 0.8759191, 0.8694853, 0.3658088, 0.6865809, 0.7518382, 0.8897059, 0.90625, 0.8566176, 0.60202205, 0.8648897, 0.5110294, 0.9696691, 0.7472426, 0.9044118, 0.4990809, 0.7867647, 0.9494485, 0.58547795, 0.7444853, 0.8722426, 1.0, 0.7784926, 0.9319853, 0.9356618, 0.8428309, 0.80514705, 0.7693015, 0.8602941, 0.9485294, 0.8492647, 0.7693015, 0.0625, 0.9540441, 0.7867647, 0.5459559, 0.38235295, 0.953125, 0.74264705, 0.83731616, 0.91452205, 0.9411765, 0.9264706, 0.3961397, 0.6443015, 0.6755515, 0.9586397, 0.92830884, 0.9034926, 0.99080884, 0.8961397, 0.7977941, 0.6277574, 0.7325368, 0.54044116, 0.9246324, 0.49172795, 0.94485295, 0.8483456, 0.6286765, 0.9209559, 0.8961397, 0.8308824, 0.6911765, 0.88327205, 0.6305147, 0.85018384, 0.66544116, 0.9797794, 0.8648897, 0.6507353, 0.8860294, 0.5422794, 0.6084559, 0.80606616, 0.64705884, 0.9181985, 0.8575368, 0.48161766, 0.8556985, 0.8492647, 0.052389707, 0.7352941, 0.7950368, 0.9880515, 0.7224265, 0.8630515, 0.91544116, 0.74264705, 0.8566176, 0.8805147, 0.91268384, 0.9200368, 0.5431985, 0.8740809, 0.88235295, 0.9319853, 0.7637868, 0.6875, 0.9733456, 0.69393384, 0.8878676, 0.090073526, 0.8152574, 0.8097426, 0.8639706, 0.8694853, 0.9181985, 0.84375, 0.7536765, 0.8924632, 0.9264706, 0.25827205, 1.0, 0.8125, 0.9402574, 0.7003676, 0.8088235, 0.8446691, 0.5744485, 0.6920956, 0.8079044, 0.0045955884, 0.8952206, 0.9990809, 0.7536765, 0.8887868, 0.8299632, 0.8161765, 0.3060662, 0.6580882, 0.7977941, 0.26194853, 0.8630515, 0.5119485, 0.3961397, 0.85294116, 0.81893384, 0.61856616, 0.9319853, 0.6847426, 0.8152574, 0.67922795, 0.22794117, 0.92922795, 0.85294116, 0.9963235, 0.7205882, 0.42463234, 0.58547795, 0.88143384, 0.9788603, 0.6847426, 0.68014705, 0.7454044, 0.30147058, 0.6397059, 0.8805147, 0.88419116, 0.79044116, 0.6112132, 0.7931985, 0.91544116, 0.17555147, 0.9834559, 0.8869485, 0.9402574, 0.48069853, 0.9273897, 0.97702205, 0.8860294, 0.81985295, 0.5514706, 0.6755515, 0.8713235, 0.71875, 0.9209559, 0.8336397, 0.9503676, 0.9898897, 0.7325368, 0.5753676, 0.83455884, 0.42555147, 0.64981616, 0.4751838, 0.86672795, 0.75, 0.6525735, 0.30514705, 0.8566176, 0.7950368, 0.6387868, 0.8924632, 0.8115809, 0.6884191, 0.9503676, 0.7674632, 0.9862132, 0.8704044, 0.9522059, 1.0, 0.9825368, 0.99264705, 0.7867647, 0.7444853, 0.9715074, 0.83639705, 0.953125, 0.7306985, 0.9889706, 0.94669116, 0.5487132, 0.8538603, 0.46599266, 0.17279412, 0.83731616, 0.9862132, 0.6709559, 0.5413603, 0.8961397, 0.009191177, 0.42555147, 0.74264705, 0.4696691, 0.97794116, 0.8069853, 0.64797795, 0.7334559, 0.24264705, 0.00091911765, 0.5349265, 0.8115809, 1.0, 0.6259191, 0.5045956, 0.69485295, 0.7527574, 0.61672795, 0.7215074, 0.56893384, 0.74264705, 0.8704044, 0.6443015, 0.9016544, 0.8759191, 0.5413603, 0.8106618, 0.9733456, 0.7049632, 0.8483456, 0.421875, 0.9172794, 0.91452205, 0.7141544, 0.3428309, 0.859375]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.0, 1.0, 0.7527574, 0.9503676, 0.7766544, 0.9990809, 0.9981618, 0.9806985, 0.7987132, 0.9724265, 0.8318015, 0.9659926, 0.8768382, 0.9990809, 0.6378676, 0.5413603, 0.8556985, 0.59375, 0.94393384, 0.9549632, 0.7803309, 0.9990809, 0.7132353, 0.8483456, 0.92922795, 0.94669116, 0.80514705, 0.7821691, 0.9393382, 0.9972426, 0.9586397, 0.8005515, 0.9862132, 0.9724265, 0.9494485, 1.0, 0.8851103, 0.9944853, 0.453125, 0.8143382, 0.9806985, 0.9549632, 0.9825368, 0.55422795, 0.97610295, 1.0, 0.97702205, 1.0, 0.8713235, 0.7509191, 0.99264705, 1.0, 0.9733456, 0.8272059, 0.9972426, 0.7628676, 0.92922795, 0.42922795, 0.8253676, 0.5900735, 0.8897059, 0.9476103, 0.63143384, 0.7950368, 0.9200368, 0.6746324, 0.9724265, 0.8759191, 0.9944853, 1.0, 0.9972426, 0.9356618, 0.9659926, 0.99264705, 0.9954044, 0.9733456, 0.9650735, 0.9972426, 0.7472426, 0.8556985, 0.9742647, 0.9347426, 0.8915441, 0.8428309, 0.8924632, 0.9705882, 0.921875, 0.97702205, 1.0, 0.8887868, 1.0, 0.9411765, 0.9402574, 0.8400735, 1.0, 0.6930147, 0.69577205, 0.9862132, 0.9871324, 0.9990809, 0.9797794, 1.0, 0.9990809, 0.8538603, 0.97610295, 0.94577205, 0.9963235, 1.0, 1.0, 0.99264705, 0.6213235, 0.9209559, 0.890625, 0.9944853, 1.0, 0.9108456, 0.9834559, 0.9852941, 0.5818015, 0.99264705, 0.8161765, 0.97610295, 0.9411765, 0.9825368, 0.96875, 1.0, 0.9944853, 0.9944853, 0.9990809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9227941, 0.9705882, 0.6672794, 1.0, 0.9705882, 0.4227941, 0.9990809, 0.8428309, 0.8612132, 0.9733456, 0.9981618, 0.8722426, 0.9990809, 0.7628676, 0.9963235, 0.8878676, 0.99356616, 0.9981618, 1.0, 0.9586397, 0.8290441, 0.9963235, 0.6461397, 0.7058824, 0.91452205, 0.9862132, 0.9044118, 0.9696691, 0.74264705, 0.9209559, 0.99264705, 0.97518384, 0.85110295, 0.9990809, 0.8805147, 0.9273897, 0.9963235, 0.9963235, 1.0, 1.0, 0.86856616, 0.9963235, 0.9724265, 0.6976103, 0.9724265, 0.89797795, 0.9862132, 0.9494485, 0.6985294, 0.9568015, 0.6773897, 0.8887868, 0.9954044, 0.9577206, 0.7858456, 0.9568015, 0.99172795, 0.94577205, 0.9788603, 0.9990809, 0.4944853, 0.83455884, 0.875, 0.9972426, 0.99172795, 0.9797794, 0.9990809, 0.7987132, 0.9871324, 0.7233456, 0.9356618, 0.7619485, 0.9889706, 0.5441176, 1.0, 1.0, 0.6672794, 0.91360295, 0.7380515, 0.6148897, 0.9181985, 0.96139705, 0.921875, 0.97794116, 1.0, 0.9696691, 1.0, 0.7830882, 0.9825368, 0.8391544, 0.9898897, 0.99172795, 1.0, 0.9981618, 0.8483456, 0.7628676, 0.7233456, 0.9007353, 0.9797794, 0.78768384, 0.9944853, 0.99080884, 0.7159926, 0.890625, 0.92922795, 1.0, 1.0, 1.0, 0.85294116, 0.9963235, 0.7169118, 0.9990809, 0.9200368, 0.9898897, 0.80422795, 0.94669116, 1.0, 0.7628676, 0.9586397, 0.9715074, 1.0, 0.90625, 0.9954044, 0.9981618, 0.97702205, 0.9659926, 0.984375, 0.984375, 1.0, 0.99264705, 0.9696691, 0.3805147, 0.9963235, 0.9889706, 0.8961397, 0.88419116, 0.9972426, 0.9090074, 0.953125, 0.9834559, 0.9990809, 0.9862132, 0.6424632, 0.8492647, 0.9200368, 0.9963235, 0.9889706, 0.99356616, 1.0, 1.0, 0.89981616, 0.8566176, 0.8694853, 0.8455882, 0.9898897, 0.94485295, 1.0, 0.9724265, 0.8933824, 0.99264705, 0.9871324, 0.9816176, 0.88235295, 0.9696691, 0.9705882, 0.9990809, 0.9476103, 0.9981618, 0.97518384, 0.9430147, 0.97702205, 0.6691176, 0.9071691, 0.9742647, 0.9347426, 1.0, 0.9972426, 0.7371324, 0.9705882, 0.9981618, 0.4384191, 0.9558824, 0.9246324, 1.0, 0.984375, 0.9972426, 0.9871324, 0.9549632, 1.0, 0.9972426, 0.9954044, 1.0, 0.7987132, 0.9972426, 0.9990809, 0.9954044, 0.9871324, 0.97610295, 1.0, 0.97610295, 0.9981618, 0.29503676, 0.9319853, 0.95955884, 1.0, 0.9972426, 1.0, 0.9963235, 0.9797794, 1.0, 1.0, 0.6231618, 1.0, 0.9806985, 1.0, 0.984375, 1.0, 0.99172795, 0.9255515, 0.9181985, 0.9356618, 0.49632353, 0.9669118, 1.0, 0.9356618, 0.99264705, 0.984375, 0.9954044, 0.64981616, 0.8244485, 0.984375, 0.5726103, 0.9742647, 0.8262868, 0.6415441, 0.9586397, 0.9200368, 0.8143382, 0.9880515, 0.9227941, 0.9347426, 0.9347426, 0.5349265, 0.9715074, 0.9981618, 1.0, 0.9375, 0.86672795, 0.7996324, 0.9788603, 1.0, 0.97518384, 0.9963235, 0.9990809, 0.7141544, 0.7922794, 0.9972426, 0.9990809, 0.9862132, 0.8602941, 0.9696691, 0.9981618, 0.45680147, 1.0, 0.9954044, 0.9981618, 0.8556985, 0.9954044, 1.0, 1.0, 0.9972426, 0.7601103, 0.8612132, 0.9715074, 0.96231616, 0.9954044, 0.97610295, 1.0, 1.0, 0.9071691, 0.8731618, 0.99356616, 0.6672794, 0.91452205, 0.81985295, 0.97610295, 0.94485295, 0.9356618, 0.6213235, 0.9963235, 0.95955884, 0.83639705, 0.9715074, 0.9696691, 0.8694853, 0.9972426, 0.97702205, 1.0, 0.9889706, 1.0, 1.0, 0.9990809, 1.0, 0.9981618, 0.9972426, 1.0, 0.9990809, 1.0, 0.99356616, 1.0, 1.0, 0.8428309, 0.96047795, 0.8878676, 0.4742647, 0.9430147, 1.0, 0.9650735, 0.8079044, 0.9862132, 0.19944853, 0.7205882, 0.8915441, 0.85294116, 0.9990809, 0.89889705, 0.9788603, 0.9476103, 0.6424632, 0.42003676, 0.78125, 0.9568015, 1.0, 0.95955884, 0.734375, 0.8924632, 0.94393384, 0.8455882, 0.94393384, 0.75827205, 0.921875, 0.9963235, 0.8465074, 0.97702205, 0.99264705, 0.8566176, 0.97518384, 1.0, 0.94393384, 0.99080884, 0.6884191, 0.9990809, 0.99356616, 0.9329044, 0.75827205, 0.9705882]\n",
      "i_s=491 in 500;  max_index=802107\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 1.4228 - accuracy: 0.6599 - top_k_categorical_accuracy: 0.9072 - val_loss: 3.6164 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=492 in 500;  max_index=803407\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 42s 2s/step - loss: 0.1573 - accuracy: 0.9642 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.6599 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=493 in 500;  max_index=804707\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.9514 - accuracy: 0.7371 - top_k_categorical_accuracy: 0.9182 - val_loss: 2.5129 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=494 in 500;  max_index=806007\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.1838 - accuracy: 0.9623 - top_k_categorical_accuracy: 0.9963 - val_loss: 0.8947 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=495 in 500;  max_index=807307\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.0177 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.9473 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=496 in 500;  max_index=808607\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.8422 - accuracy: 0.7923 - top_k_categorical_accuracy: 0.9706 - val_loss: 2.2992 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=497 in 500;  max_index=809907\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 2.7835 - accuracy: 0.3401 - top_k_categorical_accuracy: 0.7849 - val_loss: 3.9336 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "i_s=498 in 500;  max_index=811207\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.0854 - accuracy: 0.9825 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3635 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "i_s=499 in 500;  max_index=812507\n",
      "Epoch 1/1\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.0119 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.7744 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.7399099270357983\n",
      "0.244\n",
      "0.9107536762356758\n"
     ]
    }
   ],
   "source": [
    "print('-----------------Start the loop training!!---------------------')\n",
    "# Scores_all=np.zeros([N_Split,4])\n",
    "# ACC=[0.0,0.0]\n",
    "# val_ACC=[0.0,0.0]\n",
    "# val_TopACC=[0.0,0.0]\n",
    "# for i_s in range(N_Split):\n",
    "#     train_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=N_Train_OverSampler+i_s*BlockSize, max_index=N_Train_OverSampler+(i_s+1)*BlockSize+1, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "#     print('i_s='+str(i_s)+ ' in '+ str(N_Split) +';  max_index='+str(N_Train_OverSampler+(i_s+1)*BlockSize+1))\n",
    "#     val_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=N_Train_OverSampler+(i_s+1)*BlockSize-lookback, max_index=N_Train_OverSampler+(i_s+1)*BlockSize+1,shuffle=False, batch_size=1, step=BlockSize)\n",
    "#     #history = RNNmodel.fit_generator(train_generator,steps_per_epoch=1,epochs=N_EPOCHS,verbose=1,validation_data=val_generator,validation_steps=(N_Val_OverSampler - lookback) // N_BATCH)\n",
    "#     history = RNNmodel.fit_generator(train_generator,steps_per_epoch=17,epochs=1,verbose=2,validation_data=val_generator,validation_steps=1)\n",
    "\n",
    "#     ACC.extend(history.history['accuracy'])\n",
    "#     val_ACC.extend(history.history['val_accuracy'])\n",
    "#     val_TopACC.extend(history.history['top_k_categorical_accuracy'])\n",
    "#     if i_s%100==0:\n",
    "#         print(ACC)\n",
    "#         print(val_ACC)\n",
    "#         print(val_TopACC)\n",
    "\n",
    "ACC=[0.0,0.0]\n",
    "val_ACC=[0.0,0.0]\n",
    "val_TopACC=[0.0,0.0]\n",
    "for i_s in range(N_Split):\n",
    "    train_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=N_Train_OverSampler+i_s*BlockSize, max_index=N_Train_OverSampler+(i_s+1)*BlockSize+1, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "    print('i_s='+str(i_s)+ ' in '+ str(N_Split) +';  max_index='+str(N_Train_OverSampler+(i_s+1)*BlockSize+1))\n",
    "    val_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=N_Train_OverSampler+(i_s+1)*BlockSize-lookback, max_index=N_Train_OverSampler+(i_s+1)*BlockSize+1,shuffle=False, batch_size=1, step=BlockSize)\n",
    "    #history = RNNmodel.fit_generator(train_generator,steps_per_epoch=1,epochs=N_EPOCHS,verbose=1,validation_data=val_generator,validation_steps=(N_Val_OverSampler - lookback) // N_BATCH)\n",
    "    history = RNNmodel.fit_generator(train_generator,steps_per_epoch=17,epochs=1,verbose=1,validation_data=val_generator,validation_steps=1)\n",
    "    ACC.extend(history.history['accuracy'])\n",
    "    val_ACC.extend(history.history['val_accuracy'])\n",
    "    val_TopACC.extend(history.history['top_k_categorical_accuracy'])#\n",
    "    if i_s%49==0:\n",
    "        print(ACC)\n",
    "        print(val_ACC)\n",
    "        print(val_TopACC)\n",
    "RNNmodel.save('jjs_model_0203LSTMV2.h5')        \n",
    "print(val_ACC)\n",
    "NP_ACC=np.array(ACC)\n",
    "print(np.sum(NP_ACC)/500)\n",
    "NP_val_ACC=np.array(val_ACC)\n",
    "print(np.sum(NP_val_ACC)/500)\n",
    "NP_val_ACCTop5=np.array(val_TopACC)\n",
    "print(np.sum(NP_val_ACCTop5)/500)\n",
    "#     loss_i, acc_i, top5acc_i=RNNmodel.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "# print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "#     Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])# np.mean(Scores_all, axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.9478823560476303\n",
      "0.184\n",
      "0.9808235305547714\n"
     ]
    }
   ],
   "source": [
    "print(val_ACC)\n",
    "\n",
    "NP_ACC=np.array(ACC)\n",
    "print(np.sum(NP_ACC)/500)\n",
    "\n",
    "NP_val_ACC=np.array(val_ACC)\n",
    "print(np.sum(NP_val_ACC)/500)\n",
    "\n",
    "NP_val_ACCTop5=np.array(val_TopACC)\n",
    "print(np.sum(NP_val_ACCTop5)/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------2222  Start the loop training!!---------------------\n",
      "i_s=0 in 500;  max_index=163807\n",
      "[0, 0, 1.0]\n",
      "[0, 0, 0.0]\n",
      "[0, 0, 1.0]\n",
      "i_s=1 in 500;  max_index=165107\n",
      "i_s=2 in 500;  max_index=166407\n",
      "i_s=3 in 500;  max_index=167707\n",
      "i_s=4 in 500;  max_index=169007\n",
      "i_s=5 in 500;  max_index=170307\n",
      "i_s=6 in 500;  max_index=171607\n",
      "i_s=7 in 500;  max_index=172907\n",
      "i_s=8 in 500;  max_index=174207\n",
      "i_s=9 in 500;  max_index=175507\n",
      "i_s=10 in 500;  max_index=176807\n",
      "i_s=11 in 500;  max_index=178107\n",
      "i_s=12 in 500;  max_index=179407\n",
      "i_s=13 in 500;  max_index=180707\n",
      "i_s=14 in 500;  max_index=182007\n",
      "i_s=15 in 500;  max_index=183307\n",
      "i_s=16 in 500;  max_index=184607\n",
      "i_s=17 in 500;  max_index=185907\n",
      "i_s=18 in 500;  max_index=187207\n",
      "i_s=19 in 500;  max_index=188507\n",
      "i_s=20 in 500;  max_index=189807\n",
      "i_s=21 in 500;  max_index=191107\n",
      "i_s=22 in 500;  max_index=192407\n",
      "i_s=23 in 500;  max_index=193707\n",
      "i_s=24 in 500;  max_index=195007\n",
      "i_s=25 in 500;  max_index=196307\n",
      "i_s=26 in 500;  max_index=197607\n",
      "i_s=27 in 500;  max_index=198907\n"
     ]
    }
   ],
   "source": [
    "print('-----------------2222  Start the loop training!!---------------------')\n",
    "# Scores_all=np.zeros([N_Split,4])\n",
    "ACC2=[0,0]\n",
    "val_ACC2=[0,0]\n",
    "val_TopACC2=[0,0]\n",
    "for i_s in range(N_Split):\n",
    "    train_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=N_Train_OverSampler+i_s*BlockSize, max_index=N_Train_OverSampler+(i_s+1)*BlockSize+1, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "    print('i_s='+str(i_s)+ ' in '+ str(N_Split) +';  max_index='+str(N_Train_OverSampler+(i_s+1)*BlockSize+1))\n",
    "    val_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=N_Train_OverSampler+(i_s+1)*BlockSize-lookback, max_index=N_Train_OverSampler+(i_s+1)*BlockSize+1,shuffle=False, batch_size=1, step=BlockSize)\n",
    "    #history = RNNmodel.fit_generator(train_generator,steps_per_epoch=1,epochs=N_EPOCHS,verbose=1,validation_data=val_generator,validation_steps=(N_Val_OverSampler - lookback) // N_BATCH)\n",
    "    history = RNNmodel.fit_generator(train_generator,steps_per_epoch=50,epochs=1,verbose=0,validation_data=val_generator,validation_steps=1)\n",
    "\n",
    "    ACC2.extend(history.history['accuracy'])\n",
    "    val_ACC2.extend(history.history['val_accuracy'])\n",
    "    val_TopACC2.extend(history.history['top_k_categorical_accuracy'])\n",
    "    if i_s%99==0:\n",
    "        print(ACC2)\n",
    "        print(val_ACC2)\n",
    "        print(val_TopACC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=5.0\n",
    "b=3.0\n",
    "div = a // b \n",
    "div\n",
    "i_s=10000\n",
    "i_s % 500==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0., 444.,   0.,   0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[1, 2, 3]])\n",
    "print(a[0,1])\n",
    "n=np.zeros([1,4])\n",
    "n[0,1]=444\n",
    "n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
