{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from copy import deepcopy\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "trainDF=pd.read_csv(\"./zj_train.csv\")\n",
    "xy_scaler=preprocessing.StandardScaler()\n",
    "xy_scaler.fit(trainDF[[\"X\",\"Y\"]])\n",
    "trainDF[[\"X\",\"Y\"]]=xy_scaler.transform(trainDF[[\"X\",\"Y\"]])\n",
    "#################Now proceed as before#################\n",
    "def parse_time(x):\n",
    "    DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")\n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "def Dates2TDMY(x):\n",
    "    DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")\n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    T_D_M_Y=str(time)+str(day)+str(month)+str(year)\n",
    "    return T_D_M_Y\n",
    "#################\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring\n",
    "####################################################\n",
    "def parse_data(df,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print(\"Creating address features\")###Creating address features###\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds_A[x])\n",
    "    address_features.columns=[\"logodds_A\"+str(x) for x in range(len(address_features.columns))]\n",
    "    print(\"Creating time T_D_M_Y features\")###Creating time T_D_M_Y features###\n",
    "    T_D_M_Y_features=cleanData[\"T_D_M_Y\"].apply(lambda xx: logodds_T[xx])\n",
    "    T_D_M_Y_features.columns=[\"logodds_T\"+str(xx) for xx in range(len(T_D_M_Y_features.columns))]\n",
    "\n",
    "    print(\"Parsing dates\")            ###Creating address features###\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "    #     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    #     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print(\"Creating one-hot variables\")\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPF_A\"]=cleanData[\"Address\"].apply(lambda x: logoddsPF_A[x])\n",
    "    cleanData[\"logoddsPF_T\"]=cleanData[\"T_D_M_Y\"].apply(lambda x: logoddsPF_T[x])\n",
    "    print(\"droping processed columns\")\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)\n",
    "    cleanData=cleanData.drop(\"T_D_M_Y\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    features = cleanData[feature_list].join(dummy_ranks_PD.ix[:,:]).join(dummy_ranks_DAY.ix[:,:]).join(address_features.ix[:,:]).join(T_D_M_Y_features.ix[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(keep='last')).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "    #label_names=labels.unique()\n",
    "    #labels=labels.cat.rename_categories(range(len(label_names)))\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels\n",
    "    #This part is slower than it needs to be.\n",
    "def field2Vec(trainDF,fieldStr):\n",
    "    fields=sorted(trainDF[fieldStr].unique())\n",
    "    categories=sorted(trainDF[\"Category\"].unique())\n",
    "    C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "    F_C_counts=trainDF.groupby([fieldStr,\"Category\"]).size()\n",
    "    F_counts=trainDF.groupby([fieldStr]).size()\n",
    "    logodds={}\n",
    "    logoddsPF={}\n",
    "    MIN_CAT_COUNTS=2\n",
    "    default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "    for f in fields:\n",
    "        PA=F_counts[f]/float(len(trainDF))\n",
    "        logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "        logodds[f]=deepcopy(default_logodds)\n",
    "        for cat in F_C_counts[f].keys():\n",
    "            if (F_C_counts[f][cat]>MIN_CAT_COUNTS) and F_C_counts[f][cat]<F_counts[f]:\n",
    "                PA=F_C_counts[f][cat]/float(F_counts[f])\n",
    "                logodds[f][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "        logodds[f]=pd.Series(logodds[f])\n",
    "        logodds[f].index=range(len(categories))\n",
    "    return logodds,logoddsPF\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Addresss---\n",
      "-----T_D_M_Y---\n",
      "-----parse_data---\n",
      "Creating address features\n",
      "Creating time T_D_M_Y features\n",
      "Parsing dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n",
      "['X', 'Y', 'Time', 'Day', 'Month', 'Year', 'IsInterection', 'logoddsPF_A', 'logoddsPF_T', 'PD_星期一', 'PD_星期三', 'PD_星期二', 'PD_星期五', 'PD_星期六', 'PD_星期四', 'PD_星期日', 'DAY_星期一', 'DAY_星期三', 'DAY_星期二', 'DAY_星期五', 'DAY_星期六', 'DAY_星期四', 'DAY_星期日', 'logodds_A0', 'logodds_A1', 'logodds_A2', 'logodds_A3', 'logodds_A4', 'logodds_A5', 'logodds_A6', 'logodds_A7', 'logodds_A8', 'logodds_A9', 'logodds_A10', 'logodds_A11', 'logodds_A12', 'logodds_A13', 'logodds_A14', 'logodds_A15', 'logodds_A16', 'logodds_A17', 'logodds_A18', 'logodds_A19', 'logodds_A20', 'logodds_A21', 'logodds_A22', 'logodds_A23', 'logodds_A24', 'logodds_A25', 'logodds_A26', 'logodds_A27', 'logodds_A28', 'logodds_A29', 'logodds_A30', 'logodds_A31', 'logodds_A32', 'logodds_A33', 'logodds_A34', 'logodds_A35', 'logodds_A36', 'logodds_A37', 'logodds_A38', 'logodds_A39', 'logodds_A40', 'logodds_A41', 'logodds_A42', 'logodds_A43', 'logodds_A44', 'logodds_A45', 'logodds_A46', 'logodds_A47', 'logodds_A48', 'logodds_A49', 'logodds_T0', 'logodds_T1', 'logodds_T2', 'logodds_T3', 'logodds_T4', 'logodds_T5', 'logodds_T6', 'logodds_T7', 'logodds_T8', 'logodds_T9', 'logodds_T10', 'logodds_T11', 'logodds_T12', 'logodds_T13', 'logodds_T14', 'logodds_T15', 'logodds_T16', 'logodds_T17', 'logodds_T18', 'logodds_T19', 'logodds_T20', 'logodds_T21', 'logodds_T22', 'logodds_T23', 'logodds_T24', 'logodds_T25', 'logodds_T26', 'logodds_T27', 'logodds_T28', 'logodds_T29', 'logodds_T30', 'logodds_T31', 'logodds_T32', 'logodds_T33', 'logodds_T34', 'logodds_T35', 'logodds_T36', 'logodds_T37', 'logodds_T38', 'logodds_T39', 'logodds_T40', 'logodds_T41', 'logodds_T42', 'logodds_T43', 'logodds_T44', 'logodds_T45', 'logodds_T46', 'logodds_T47', 'logodds_T48', 'logodds_T49', 'IsDup', 'Awake', 'Summer', 'Fall', 'Winter', 'Spring']\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "trainDF[\"T_D_M_Y\"]=trainDF[\"Dates\"].apply(Dates2TDMY)\n",
    "trainDF[\"T_D_M_Y\"]=trainDF[\"T_D_M_Y\"]+trainDF[\"DayOfWeek\"]\n",
    "print(\"-----Addresss---\")\n",
    "logodds_A,logoddsPF_A=field2Vec(trainDF,\"Address\")\n",
    "print(\"-----T_D_M_Y---\")\n",
    "logodds_T,logoddsPF_T=field2Vec(trainDF,\"T_D_M_Y\")\n",
    "print(\"-----parse_data---\")\n",
    "features, labels=parse_data(trainDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T)    \n",
    "##########################################################################\n",
    "print(features.columns.tolist())\n",
    "print(len(features.columns))\n",
    "##########################################################################\n",
    "# num_feature_list=[\"Time\",\"Day\",\"Month\",\"Year\",\"DayOfWeek\"]\n",
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, input_dim=129, kernel_initializer=\"glorot_uniform\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_initializer=\"glorot_uniform\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, kernel_initializer=\"glorot_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Go! Go! Go!!!!-----------\n",
      "Train on 3144600 samples, validate on 786150 samples\n",
      "Epoch 1/21\n",
      " - 701s - loss: 0.7510 - accuracy: 0.7962 - top_k_categorical_accuracy: 0.9245 - val_loss: 0.4356 - val_accuracy: 0.8772 - val_top_k_categorical_accuracy: 0.9682\n",
      "Epoch 2/21\n",
      " - 693s - loss: 0.4606 - accuracy: 0.8665 - top_k_categorical_accuracy: 0.9682 - val_loss: 0.3631 - val_accuracy: 0.8918 - val_top_k_categorical_accuracy: 0.9763\n",
      "Epoch 3/21\n",
      " - 654s - loss: 0.4090 - accuracy: 0.8788 - top_k_categorical_accuracy: 0.9751 - val_loss: 0.3389 - val_accuracy: 0.8970 - val_top_k_categorical_accuracy: 0.9809\n",
      "Epoch 4/21\n",
      " - 654s - loss: 0.3802 - accuracy: 0.8858 - top_k_categorical_accuracy: 0.9790 - val_loss: 0.3174 - val_accuracy: 0.9023 - val_top_k_categorical_accuracy: 0.9835\n",
      "Epoch 5/21\n",
      " - 665s - loss: 0.3614 - accuracy: 0.8905 - top_k_categorical_accuracy: 0.9814 - val_loss: 0.3004 - val_accuracy: 0.9058 - val_top_k_categorical_accuracy: 0.9858\n",
      "Epoch 6/21\n",
      " - 702s - loss: 0.3478 - accuracy: 0.8939 - top_k_categorical_accuracy: 0.9833 - val_loss: 0.2950 - val_accuracy: 0.9077 - val_top_k_categorical_accuracy: 0.9866\n",
      "Epoch 7/21\n",
      " - 700s - loss: 0.3367 - accuracy: 0.8969 - top_k_categorical_accuracy: 0.9848 - val_loss: 0.2809 - val_accuracy: 0.9118 - val_top_k_categorical_accuracy: 0.9884\n",
      "Epoch 8/21\n",
      " - 700s - loss: 0.3279 - accuracy: 0.8992 - top_k_categorical_accuracy: 0.9859 - val_loss: 0.2762 - val_accuracy: 0.9135 - val_top_k_categorical_accuracy: 0.9890\n",
      "Epoch 9/21\n",
      " - 698s - loss: 0.3203 - accuracy: 0.9012 - top_k_categorical_accuracy: 0.9867 - val_loss: 0.2701 - val_accuracy: 0.9168 - val_top_k_categorical_accuracy: 0.9898\n",
      "Epoch 10/21\n",
      " - 698s - loss: 0.3141 - accuracy: 0.9030 - top_k_categorical_accuracy: 0.9876 - val_loss: 0.2589 - val_accuracy: 0.9177 - val_top_k_categorical_accuracy: 0.9908\n",
      "Epoch 11/21\n",
      " - 699s - loss: 0.3086 - accuracy: 0.9045 - top_k_categorical_accuracy: 0.9881 - val_loss: 0.2653 - val_accuracy: 0.9181 - val_top_k_categorical_accuracy: 0.9909\n",
      "Epoch 12/21\n",
      " - 699s - loss: 0.3037 - accuracy: 0.9060 - top_k_categorical_accuracy: 0.9886 - val_loss: 0.2509 - val_accuracy: 0.9223 - val_top_k_categorical_accuracy: 0.9919\n",
      "Epoch 13/21\n",
      " - 699s - loss: 0.2993 - accuracy: 0.9072 - top_k_categorical_accuracy: 0.9892 - val_loss: 0.2507 - val_accuracy: 0.9219 - val_top_k_categorical_accuracy: 0.9919\n",
      "Epoch 14/21\n",
      " - 699s - loss: 0.2959 - accuracy: 0.9081 - top_k_categorical_accuracy: 0.9895 - val_loss: 0.2474 - val_accuracy: 0.9231 - val_top_k_categorical_accuracy: 0.9924\n",
      "Epoch 15/21\n",
      " - 701s - loss: 0.2920 - accuracy: 0.9093 - top_k_categorical_accuracy: 0.9899 - val_loss: 0.2407 - val_accuracy: 0.9244 - val_top_k_categorical_accuracy: 0.9934\n",
      "Epoch 16/21\n",
      " - 677s - loss: 0.2890 - accuracy: 0.9101 - top_k_categorical_accuracy: 0.9903 - val_loss: 0.2382 - val_accuracy: 0.9265 - val_top_k_categorical_accuracy: 0.9932\n",
      "Epoch 17/21\n",
      " - 689s - loss: 0.2861 - accuracy: 0.9110 - top_k_categorical_accuracy: 0.9904 - val_loss: 0.2428 - val_accuracy: 0.9263 - val_top_k_categorical_accuracy: 0.9930\n",
      "Epoch 18/21\n",
      " - 667s - loss: 0.2836 - accuracy: 0.9117 - top_k_categorical_accuracy: 0.9907 - val_loss: 0.2331 - val_accuracy: 0.9275 - val_top_k_categorical_accuracy: 0.9936\n",
      "Epoch 19/21\n",
      " - 674s - loss: 0.2810 - accuracy: 0.9126 - top_k_categorical_accuracy: 0.9910 - val_loss: 0.2315 - val_accuracy: 0.9281 - val_top_k_categorical_accuracy: 0.9939\n",
      "Epoch 20/21\n",
      " - 678s - loss: 0.2788 - accuracy: 0.9131 - top_k_categorical_accuracy: 0.9912 - val_loss: 0.2327 - val_accuracy: 0.9282 - val_top_k_categorical_accuracy: 0.9938\n",
      "Epoch 21/21\n",
      " - 676s - loss: 0.2767 - accuracy: 0.9138 - top_k_categorical_accuracy: 0.9914 - val_loss: 0.2315 - val_accuracy: 0.9288 - val_top_k_categorical_accuracy: 0.9939\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "786150/786150 [==============================] - 47s 60us/step\n",
      "[0.2314566976904816, 0.928817629814148, 0.9938892126083374]\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS=21\n",
    "N_HN=256\n",
    "N_LAYERS=2\n",
    "N_BATCH=64\n",
    "DP=0.5\n",
    "N_CLASS=len(labels.unique())\n",
    "#########################################################################################\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "featuresArray, labelsArray = ros.fit_resample(features.values,labels.values)\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "time_temp=featuresArray[:,2]+np.dot(featuresArray[:,3],100)+np.dot(featuresArray[:,4],10000)+np.dot(featuresArray[:,5],1000000)\n",
    "features_label_time=np.column_stack((featuresArray,labelsArray))\n",
    "features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "labelsArray=features_label_time[:,-2]\n",
    "featuresArray=features_label_time[:,0:featuresArray.shape[1]]\n",
    "del features_label_time\n",
    "    ###排序结束#############\n",
    "SORTbyTime=False\n",
    "Test_size=100000\n",
    "split_count=1\n",
    "split_size=int(Test_size/split_count)\n",
    "N_hight=featuresArray.shape[0]\n",
    "for t_i in range(split_count):\n",
    "    if SORTbyTime:\n",
    "        # t_i=t0_i+1\n",
    "        print('--------NNN_spllit_NNN_spllit_NNN_spllit_NNN_spllit_---------')\n",
    "        print(t_i)\n",
    "        x_train=featuresArray[0:N_hight-Test_size+t_i*split_size,:]\n",
    "        y_train=labelsArray[0:N_hight-Test_size+t_i*split_size]\n",
    "\n",
    "        x_test=featuresArray[N_hight-Test_size+t_i*split_size:N_hight-Test_size+(t_i+1)*split_size,:]\n",
    "        y_test=labelsArray[N_hight-Test_size+t_i*split_size:N_hight-Test_size+(t_i+1)*split_size]\n",
    "    else:\n",
    "        x_train,x_test,y_train,y_test = train_test_split(featuresArray,labelsArray,test_size=0.2,shuffle=True)\n",
    "\n",
    "    y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "    y_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_test)), num_classes=N_CLASS)\n",
    "    \n",
    "    ##########################################################################\n",
    "    input_dim=x_train.shape[1]\n",
    "    output_dim=N_CLASS\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512,input_dim=input_dim,init='glorot_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    # model.add(Dropout(dp))\n",
    "    for i in range(N_LAYERS):\n",
    "        model.add(Dense(N_HN, init='glorot_uniform'))\n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(PReLU())    \n",
    "    #   model.add(Dropout(dp))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(output_dim, init='glorot_uniform'))\n",
    "    model.add(Activation('softmax'))\n",
    "    # model = multi_gpu_model(model, 2)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "    print('------------Go! Go! Go!!!!-----------')\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=2,validation_data=(x_test,y_test))\n",
    "    # acc_test, test_score,fitting, model = build_and_fit_model(features_train.values,labels_train,X_test=features_test.values,y_test=labels_test,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)\n",
    "    # model.save('jjs_model_0112.h5')\n",
    "    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "    acc_test = model.evaluate(x_test,y_test, batch_size=N_BATCH)\n",
    "    print(acc_test)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
                                                                                                                                            